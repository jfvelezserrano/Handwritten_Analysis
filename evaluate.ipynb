{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.10\n",
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "# Importing the recquired libraries:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Ignore harmless warnings:\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from platform import python_version\n",
    "print(python_version())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ['urjc', 'gavab'] * 400\n",
    "len_train = len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Function to convert letters (and therefore words) into one-hot PyTorch tensors:\n",
    "\n",
    "letters = ['SOS_token', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "          'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'EOS_token']\n",
    "def letter_to_vector(letter):\n",
    "    vector = torch.zeros(1, 1, len(letters))\n",
    "    for i in range(len(letters)):\n",
    "        if letters[i] == letter:\n",
    "            vector[0, 0, i] = 1.\n",
    "    return(vector)\n",
    "print(letter_to_vector('SOS_token').cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVOLUTIONAL NEURAL NETWORK:\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1, 2) # padding???\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1, 2)\n",
    "        self.fc1 = nn.Linear(12*2*50, 1024)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, 12*2*50) # -1 para no tener que determinar aquí el tamaño del batch (se ajusta, podemos variarlo)\n",
    "        X = F.relu(self.fc1(X))\n",
    "\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER:\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = input.view(1,1,-1)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECODER:\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(output_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = input.view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, define loss and optimization functions:\n",
    "\n",
    "torch.manual_seed(101)\n",
    "\n",
    "input_size = 1024\n",
    "hidden_size = 256\n",
    "output_size = 28\n",
    "\n",
    "CNN_model = ConvolutionalNetwork().cuda()\n",
    "CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr = 0.01)\n",
    "\n",
    "Encoder_model = EncoderRNN(input_size = input_size, hidden_size = hidden_size).cuda()\n",
    "Encoder_optimizer = optim.SGD(Encoder_model.parameters(), lr = 0.01)\n",
    "\n",
    "Decoder_model = DecoderRNN(hidden_size = hidden_size, output_size = output_size).cuda()\n",
    "Decoder_optimizer = optim.SGD(Decoder_model.parameters(), lr = 0.01)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4f2aff4100f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mCNN_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mEncoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_estoril/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_estoril/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING THE MODEL:\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Setting the image parameters:\n",
    "\n",
    "height = 48\n",
    "width = 192\n",
    "\n",
    "# Setting the sliding window parameters:\n",
    "\n",
    "patch_height = 48\n",
    "patch_width = 10\n",
    "stepsize = 2\n",
    "n_patches = int((width - patch_width)/2 + 1)\n",
    "# images_tensor = torch.empty(size = (len_train + len_test, 92, 1, 48, 10)) # (image, patch, color channel, height, width)\n",
    "torch.manual_seed(101)\n",
    "epochs = 10\n",
    "train_losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for j in range(len_train): # we chose individual words/images as batches:\n",
    "        \n",
    "        CNN_optimizer.zero_grad()\n",
    "        Encoder_optimizer.zero_grad()\n",
    "        Decoder_optimizer.zero_grad()\n",
    "        \n",
    "        image = 255 * np.ones(shape = [height, width], dtype = np.uint8)\n",
    "        image = cv2.putText(image, text = train_set[j], org = (5, 30),\n",
    "        fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.7, color = (0, 0, 0),\n",
    "        thickness = 1, lineType = cv2.LINE_AA)\n",
    "        \n",
    "        im = transforms.ToPILImage()(image) # np.ndarray to PIL.Image.Image\n",
    "        # encoder_input_tensor = torch.empty(n_patches, input_size)\n",
    "        \n",
    "        encoder_hidden = Encoder_model.initHidden()\n",
    "        \n",
    "        for p in range(n_patches):\n",
    "            \n",
    "            im_patch = transforms.functional.crop(im, 0, 0 + p*stepsize, patch_height, patch_width) # cropping of the image into patches\n",
    "            im_patch_tensor = transforms.ToTensor()(im_patch) # torch.Tensor of the patch (normalized)\n",
    "            im_patch_tesnsor = 1. - im_patch_tensor\n",
    "            # images_tensor[j, p, 0, :, :] = im_patch_tensor # assign position within the global tensor\n",
    "            im_patch_tensor = im_patch_tensor.view(1, 1, patch_height, patch_width) # CNN_model expects a 4-dimensional tensor (1 dimension for batch)\n",
    "            im_patch_tensor = im_patch_tensor.cuda()\n",
    "            \n",
    "            \n",
    "            encoder_input = CNN_model(im_patch_tensor) # 1024-length vector associated to patch p (i.e. CNN output, Encoder input)\n",
    "            # encoder_input_tensor[p, :] = encoder_input # we store each output in a CNN output tensor (92x1024)\n",
    "            encoder_output, encoder_hidden = Encoder_model(encoder_input, encoder_hidden)\n",
    "            \n",
    "        decoder_input = letter_to_vector('SOS_token').cuda()\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for d in range(len(list(train_set[j]))):\n",
    "            \n",
    "            decoder_output, decoder_hidden = Decoder_model(decoder_input, decoder_hidden)\n",
    "            \n",
    "            #decoder_input = decoder_output.cuda()\n",
    "            #decoder_input = letter_to_vector(list(train_set[j])[d]).cuda()\n",
    "\n",
    "            categorical_decoder_output = torch.zeros(1, 1, output_size).cuda()\n",
    "            categorical_decoder_output[0][0][torch.argmax(decoder_output)] = 1.\n",
    "            decoder_input = categorical_decoder_output\n",
    "            \n",
    "            \n",
    "            if torch.equal(categorical_decoder_output, letter_to_vector('EOS_token').cuda()) == True:\n",
    "                break\n",
    "            \n",
    "            if d == 0:\n",
    "                \n",
    "                output_word = decoder_output # torch vector associated to the first output letter\n",
    "                #categorical_decoder_output = torch.zeros(1,1,output_size).cuda()\n",
    "                #categorical_decoder_output[0][0][torch.argmax(decoder_output)] = 1.\n",
    "                #output_word = categorical_decoder_output\n",
    "                \n",
    "                real_word = letter_to_vector(list(train_set[j])[d]).type(torch.LongTensor).cuda()\n",
    "                \n",
    "                index = torch.argmax(real_word.view(output_size))\n",
    "                ground_word = torch.tensor([index], dtype = torch.long).cuda() \n",
    "                # torch vector associated to the first real letter\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                #categorical_decoder_output = torch.zeros(1,1,output_size).cuda()\n",
    "                #categorical_decoder_output[0][0][torch.argmax(decoder_output)] = 1.\n",
    "                \n",
    "                output_word = torch.cat((output_word, decoder_output), dim = 0) # we concatenate the remaining output letters\n",
    "                real_word = torch.cat((real_word, \n",
    "                                       letter_to_vector(list(train_set[j])[d]).type(torch.LongTensor).cuda()), dim = 0)\n",
    "                ground_word = torch.cat((ground_word,\n",
    "                                        torch.tensor([torch.argmax(letter_to_vector(list(train_set[j])[d]).type(torch.LongTensor))]).cuda()), \n",
    "                                        dim = 0)\n",
    "\n",
    "        loss = criterion(output_word, ground_word) \n",
    "        \n",
    "        loss.backward()\n",
    "        CNN_optimizer.step()\n",
    "        Encoder_optimizer.step()\n",
    "        Decoder_optimizer.step()\n",
    "        \n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    \n",
    "    print(i)    \n",
    "print(train_losses) \n",
    "print(f'\\nDuration: {(time.time() - start_time)/60} minutes') # print the time elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.yscale('log')\n",
    "plt.plot(train_losses, label='training loss')\n",
    "plt.title('Loss at the end of each epoch')\n",
    "plt.legend();\n",
    "plt.savefig('eval_losses_2_palabras.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1377e+01, -1.3229e+01, -3.1854e+01, -2.5503e+01, -2.1495e+01,\n",
      "         -2.1070e+01, -2.1964e+01, -6.1174e+00, -2.1113e+01, -2.1587e+01,\n",
      "         -1.7754e+01, -2.1434e+01, -2.1393e+01, -2.1661e+01, -2.1507e+01,\n",
      "         -2.1649e+01, -2.0999e+01, -2.1768e+01, -5.7797e+00, -2.1456e+01,\n",
      "         -2.1479e+01, -5.3101e-03, -2.4700e+01, -2.1744e+01, -2.1573e+01,\n",
      "         -2.1654e+01, -2.1771e+01, -2.1719e+01],\n",
      "        [-1.5872e+01, -8.2379e+00, -2.5109e+01, -1.4734e+01, -1.6143e+01,\n",
      "         -1.5690e+01, -1.6523e+01, -1.0360e+01, -1.5529e+01, -1.6572e+01,\n",
      "         -5.5352e+00, -1.5830e+01, -1.6086e+01, -1.6113e+01, -1.6164e+01,\n",
      "         -1.5897e+01, -1.5663e+01, -1.6265e+01, -7.1354e-03, -1.5943e+01,\n",
      "         -1.6298e+01, -5.8559e+00, -1.2989e+01, -1.6581e+01, -1.6328e+01,\n",
      "         -1.5934e+01, -1.6181e+01, -1.6153e+01],\n",
      "        [-1.5357e+01, -9.7732e+00, -1.8727e+01, -6.2725e+00, -1.5596e+01,\n",
      "         -1.5431e+01, -1.5848e+01, -2.0714e+01, -1.5123e+01, -1.6343e+01,\n",
      "         -7.0210e-03, -1.5344e+01, -1.5745e+01, -1.5465e+01, -1.5832e+01,\n",
      "         -1.5323e+01, -1.5488e+01, -1.5553e+01, -6.0546e+00, -1.5430e+01,\n",
      "         -1.5900e+01, -1.8398e+01, -5.9139e+00, -1.6029e+01, -1.6002e+01,\n",
      "         -1.5343e+01, -1.5346e+01, -1.5619e+01],\n",
      "        [-1.4511e+01, -8.1789e+00, -7.0620e+00, -3.8691e-03, -1.4656e+01,\n",
      "         -1.4930e+01, -1.4818e+01, -2.4351e+01, -1.4625e+01, -1.5088e+01,\n",
      "         -5.9140e+00, -1.4695e+01, -1.4708e+01, -1.4674e+01, -1.4925e+01,\n",
      "         -1.4807e+01, -1.4743e+01, -1.4376e+01, -1.5449e+01, -1.4507e+01,\n",
      "         -1.4634e+01, -2.3338e+01, -1.1150e+01, -1.4761e+01, -1.5079e+01,\n",
      "         -1.4606e+01, -1.4604e+01, -1.4408e+01]], device='cuda:0')\n",
      "tensor([21, 18, 10,  3], device='cuda:0')\n",
      "urjc\n"
     ]
    }
   ],
   "source": [
    "word = 'urjc'\n",
    "torch.manual_seed(101)\n",
    "image_test = 255 * np.ones(shape=[height, width], dtype = np.uint8)\n",
    "image_test = cv2.putText(image_test, text = word, org = (5, 30),\n",
    "fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.7, color = (0, 0, 0),\n",
    "thickness = 1, lineType = cv2.LINE_AA)\n",
    "\n",
    "im_test = transforms.ToPILImage()(image_test) # np.ndarray to PIL.Image.Image\n",
    "# encoder_input_tensor_test = torch.empty(n_patches, input_size) \n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    encoder_hidden_test = Encoder_model.initHidden()\n",
    "\n",
    "    for p in range(0, n_patches):\n",
    "\n",
    "        im_patch_test = transforms.functional.crop(im_test, 0, 0 + p*stepsize, patch_height, patch_width) # cropping of the image into patches\n",
    "        im_patch_tensor_test = transforms.ToTensor()(im_patch_test) # torch.Tensor of the patch (normalized)\n",
    "        im_patch_tensor_test = 1. - im_patch_tensor_test\n",
    "        # images_tensor[k + len(train_set), p, 0, :, :] = im_patch_tensor_test # assign position within the global tensor\n",
    "        im_patch_tensor_test = im_patch_tensor_test.cuda()\n",
    "        im_patch_tensor_test = im_patch_tensor_test.view(1, 1, patch_height, patch_width) # CNN_model expects a 4-dimensional tensor (1 dimension for batch)\n",
    "\n",
    "\n",
    "        encoder_input_test = CNN_model(im_patch_tensor_test) # 1024-length vector associated to patch p (i.e. CNN output, Encoder input)\n",
    "        #encoder_input_tensor_test[p,:] = encoder_input_test # we store each output in a CNN output tensor (92x1024)\n",
    "\n",
    "        encoder_output_test, encoder_hidden_test = Encoder_model(encoder_input_test, encoder_hidden_test)\n",
    "\n",
    "    decoder_input_test = letter_to_vector('SOS_token').cuda()\n",
    "    decoder_hidden_test = encoder_hidden_test\n",
    "\n",
    "    for d in range(len(list(word))):\n",
    "\n",
    "        decoder_output_test, decoder_hidden_test = Decoder_model(decoder_input_test, decoder_hidden_test)\n",
    "        \n",
    "        #decoder_input_test = decoder_output_test.cuda()\n",
    "        #decoder_input_test = letter_to_vector(list(word)[d]).cuda()\n",
    "\n",
    "        categorical_decoder_output_test = torch.zeros(1, 1, output_size).cuda()\n",
    "        categorical_decoder_output_test[0][0][torch.argmax(decoder_output_test)] = 1.\n",
    "        decoder_input_test = categorical_decoder_output_test\n",
    "        \n",
    "        if torch.equal(categorical_decoder_output_test, letter_to_vector('EOS_token').cuda()) == True:\n",
    "            break\n",
    "\n",
    "        if d == 0:\n",
    "\n",
    "            output_word_test = decoder_output_test # torch vector associated to the first output letter                \n",
    "            real_word_test = letter_to_vector(list(word)[d]).type(torch.LongTensor).cuda() \n",
    "            ground_word_test = torch.tensor([torch.argmax(real_word_test)]).type(torch.LongTensor).cuda() \n",
    "            # torch vector associated to the first real letter\n",
    "\n",
    "        else:\n",
    "\n",
    "            output_word_test = torch.cat((output_word_test, decoder_output_test), dim = 0) # we concatenate the remaining output letters\n",
    "            real_word_test = torch.cat((real_word_test, \n",
    "                                   letter_to_vector(list(word)[d]).type(torch.LongTensor).cuda()), dim = 0)\n",
    "            ground_word_test = torch.cat((ground_word_test,\n",
    "                                    torch.tensor([torch.argmax(letter_to_vector(list(word)[d]).type(torch.LongTensor))]).cuda()), \n",
    "                                    dim = 0)\n",
    "            \n",
    "print(output_word_test)\n",
    "model_word = []\n",
    "indices = torch.argmax(output_word_test, dim = 1)\n",
    "print(indices)\n",
    "\n",
    "for i in range(indices.numel()):\n",
    "    model_word.append(letters[indices[i]])\n",
    "      \n",
    "model_word = ''.join(model_word)\n",
    "print(model_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_estoril",
   "language": "python",
   "name": "pytorch_estoril"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
