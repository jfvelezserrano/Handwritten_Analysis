{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n",
      "3.6.10\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "import cv2\n",
    "import skimage\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Ignore harmless warnings:\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import platform\n",
    "print(torch.__version__)\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TITAN X (Pascal)'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ['hola', 'urjc', 'gavab', 'estoril', 'alvaro', 'victoria', 'jose', 'fuengirola'] * 512\n",
    "val_set = ['hola', 'urjc', 'gavab', 'estoril', 'alvaro', 'victoria', 'jose', 'fuengirola'] * 64\n",
    "test_set = ['estoril', 'fuengirola']\n",
    "\n",
    "MAX_LENGTH = max(len(list(word)) for word in train_set)\n",
    "\n",
    "len_train = len(train_set)\n",
    "len_val = len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0.]]])\n",
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# Function to convert letters (and therefore words) into PyTorch tensors:\n",
    "\n",
    "letters = ['SOS_token', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "          'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'EOS_token']\n",
    "\n",
    "PAD_token = torch.zeros(1, 1, 28)\n",
    "\n",
    "def letter_to_vector(letter):\n",
    "    vector = torch.zeros(1, 1, len(letters))\n",
    "    for i in range(len(letters)):\n",
    "        if letters[i] == letter:\n",
    "            vector[0, 0, i] = 1.\n",
    "    return(vector)\n",
    "\n",
    "print(PAD_token)\n",
    "print(letter_to_vector('SOS_token'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_gen(word, n_patches, patch_height, patch_width, stepsize):\n",
    "    \n",
    "    image = 255 * np.ones(shape = [height, width], dtype = np.uint8)\n",
    "    image = cv2.putText(image, text = word, org = (5, 30),\n",
    "    fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.7, color = (0, 0, 0),\n",
    "    thickness = 2, lineType = cv2.LINE_AA)\n",
    "    image = transforms.ToPILImage()(image) # np.ndarray to PIL.Image.Image\n",
    "    patches_tensor = torch.empty(n_patches, 1, patch_height, patch_width)\n",
    "    \n",
    "    for p in range(n_patches):\n",
    "        \n",
    "        patch = transforms.functional.crop(image, 0, 0 + p * stepsize, patch_height, patch_width) # cropping of the image into patches\n",
    "        patch = transforms.ToTensor()(patch) # torch.Tensor of the patch (normalized)\n",
    "        #patch = skimage.util.random_noise(patch, mode='gaussian') # we set some random noise to the image\n",
    "        #patch = torch.from_numpy(patch) # conversion to pytorch tensor again\n",
    "        patch = 1. - patch # it will work better if we have white text over black background\n",
    "        patch = patch.view(1, 1, patch_height, patch_width) # CNN_model expects a 4-dimensional tensor (1 dimension for batch)\n",
    "        patch = patch.type(torch.FloatTensor) # conversion to float\n",
    "        patch = patch.cuda() # set to cuda\n",
    "        patches_tensor[p, 0, :, :] = patch\n",
    "        patches_tensor = patches_tensor.cuda()\n",
    "        \n",
    "    return patches_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting image and sliding window parameters:\n",
    "\n",
    "height = 48\n",
    "width = 192\n",
    "patch_height = 48\n",
    "patch_width = 10\n",
    "stepsize = 2\n",
    "color_channels = 1\n",
    "n_patches = int((width - patch_width)/stepsize + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to get a tuple for each image with (its 92 tensor patches, label):\n",
    "\n",
    "def complete_set(set):\n",
    "    complete_set = []\n",
    "    for word in set:\n",
    "        complete_set.append((patch_gen(word, n_patches, patch_height, patch_width, stepsize), word))\n",
    "        \n",
    "    return complete_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_train_set = complete_set(set = train_set)\n",
    "comp_val_set = complete_set(set = val_set)\n",
    "comp_test_set = complete_set(set = test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 92, 1, 48, 10])\n",
      "torch.Size([184, 1, 48, 10])\n",
      "('jose', 'victoria')\n"
     ]
    }
   ],
   "source": [
    "# Loading data in image batches:\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(comp_train_set, batch_size = batch_size, shuffle=True)\n",
    "val_loader = DataLoader(comp_val_set, batch_size = batch_size, shuffle=False)\n",
    "test_loader = DataLoader(comp_test_set, batch_size = batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "for image, label in train_loader:\n",
    "    break\n",
    "    \n",
    "image_cnn = image.view(-1, color_channels, patch_height, patch_width)   \n",
    "print(image.shape)\n",
    "print(image_cnn.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_target(label):\n",
    "    \n",
    "    one_hot_target = torch.zeros(batch_size, MAX_LENGTH + 1, output_size) # one_hot_tensor of the input batch of words for Decoder\n",
    "    \n",
    "    for j in range(batch_size): # for each word of the batch\n",
    "        \n",
    "        length = len(list(label[j])) # compute the number of letters\n",
    "        \n",
    "        one_hot_target[j, 0, :] = letter_to_vector('SOS_token') # the first letter of every word is always the SOS_token\n",
    "        \n",
    "        for k in range(length): # for each letter\n",
    "            \n",
    "            one_hot_target[j, k + 1, :] = letter_to_vector(list(label[j])[k]) # add the one_hot vector of that letter\n",
    "                                                                              # to the global tensor\n",
    "    return one_hot_target        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model and architecture:\n",
    "\n",
    "# CONVOLUTIONAL NEURAL NETWORK:\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1, 2) # padding???\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1, 2)\n",
    "        self.fc1 = nn.Linear(12*2*50, 1024)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, 12*2*50) # -1 para no tener que determinar aquí el tamaño del batch (se ajusta, podemos variarlo)\n",
    "        X = F.relu(self.fc1(X))\n",
    "\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER:\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        output = input.view(batch_size, n_patches, input_size)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECODER:\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, batch_first = True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 2) \n",
    "        # dim = 2 porque esta última dimensión es la correspondiente a output_size, que es sobre\n",
    "        # la que queremos hacer el softmax\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #output = input.view(batch_size, MAX_LENGTH + 1, output_size)\n",
    "        output = input\n",
    "        #output = F.relu(output) # la relu se metía aquí porque en el\n",
    "        #caso NLP del ejemplo de PyTorch previamente había una capa de embedding\n",
    "        #No nos hace falta porque nuestro tensor de inputs ya es one-hot\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "               torch.zeros(1, batch_size, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_size = 1024\n",
    "hidden_size = 256\n",
    "output_size = 28\n",
    "\n",
    "CNN_model = ConvolutionalNetwork().cuda()\n",
    "CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr = 0.001)\n",
    "\n",
    "Encoder_model = EncoderRNN(input_size = input_size, hidden_size = hidden_size).cuda()\n",
    "Encoder_optimizer = optim.SGD(Encoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "Decoder_model = DecoderRNN(hidden_size = hidden_size, output_size = output_size).cuda()\n",
    "Decoder_optimizer = optim.SGD(Decoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "Duration: 10.741639296213785 minutes\n",
      "[tensor(1.9803, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.8501, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.2566, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.3551, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.8163, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8292, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7231, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0175, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9671, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9338, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8569, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1085, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8065, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.5819, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.5589, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.5567, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.5665, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.3135, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.4199, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.2602, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.4135, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.2881, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1833, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1846, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.2114, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1652, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.2438, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.2088, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.2019, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0707, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(2.4067, device='cuda:0'), tensor(2.0848, device='cuda:0'), tensor(1.8554, device='cuda:0'), tensor(1.6800, device='cuda:0'), tensor(1.5089, device='cuda:0'), tensor(1.3577, device='cuda:0'), tensor(1.2234, device='cuda:0'), tensor(1.1558, device='cuda:0'), tensor(1.0348, device='cuda:0'), tensor(0.9320, device='cuda:0'), tensor(0.8345, device='cuda:0'), tensor(0.7719, device='cuda:0'), tensor(0.7055, device='cuda:0'), tensor(0.6432, device='cuda:0'), tensor(0.5869, device='cuda:0'), tensor(0.5338, device='cuda:0'), tensor(0.4887, device='cuda:0'), tensor(0.4487, device='cuda:0'), tensor(0.4131, device='cuda:0'), tensor(0.3812, device='cuda:0'), tensor(0.3513, device='cuda:0'), tensor(0.3237, device='cuda:0'), tensor(0.2988, device='cuda:0'), tensor(0.2765, device='cuda:0'), tensor(0.2568, device='cuda:0'), tensor(0.2427, device='cuda:0'), tensor(0.2230, device='cuda:0'), tensor(0.2083, device='cuda:0'), tensor(0.1944, device='cuda:0'), tensor(0.2027, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE MODEL:\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for b, (image, label) in enumerate(train_loader):\n",
    "        \n",
    "        b += 1\n",
    "        \n",
    "        encoder_hidden = Encoder_model.initHidden()\n",
    "        \n",
    "        image_cnn = image.view(-1, color_channels, patch_height, patch_width).cuda()\n",
    "        encoder_input = CNN_model(image_cnn)\n",
    "        _, encoder_hidden = Encoder_model(encoder_input, encoder_hidden)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = get_one_hot_target(label=label).cuda()\n",
    "        decoder_output, decoder_hidden = Decoder_model(decoder_input, decoder_hidden)\n",
    "        \n",
    "        output_indices = torch.tensor(list(range(0, MAX_LENGTH))).cuda() # remove EOS_token from the output\n",
    "        decoder_output = torch.index_select(decoder_output, dim = 1, index = output_indices)\n",
    "        \n",
    "        ground_truth = torch.argmax(decoder_input, 2)\n",
    "        target_indices = torch.tensor(list(range(1, MAX_LENGTH + 1))).cuda() # remove SOS_token from the input\n",
    "        ground_truth = torch.index_select(ground_truth, dim = 1, index = target_indices)\n",
    "        #decoder_output = decoder_output.view(batch_size, output_size, MAX_LENGTH)\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            \n",
    "            loss += criterion(decoder_output[j], ground_truth[j])   \n",
    "        #loss = criterion(decoder_output[j], ground_truth[j]) + criterion(decoder_output[1], ground_truth[1])\n",
    "        \n",
    "        loss = loss/batch_size\n",
    "        \n",
    "        CNN_optimizer.zero_grad()\n",
    "        Encoder_optimizer.zero_grad()\n",
    "        Decoder_optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        CNN_optimizer.step()\n",
    "        Encoder_optimizer.step()\n",
    "        Decoder_optimizer.step()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for v, (image_val, label_val) in enumerate(val_loader):\n",
    "        \n",
    "            v += 1\n",
    "\n",
    "            encoder_hidden_val = Encoder_model.initHidden()\n",
    "\n",
    "            image_cnn_val = image_val.view(-1, color_channels, patch_height, patch_width).cuda()\n",
    "            encoder_input_val = CNN_model(image_cnn_val)\n",
    "            _, encoder_hidden_val = Encoder_model(encoder_input_val, encoder_hidden_val)\n",
    "\n",
    "            decoder_hidden_val = encoder_hidden_val\n",
    "            decoder_input_val = get_one_hot_target(label=label_val).cuda()\n",
    "            decoder_output_val, decoder_hidden_val = Decoder_model(decoder_input_val, decoder_hidden_val)\n",
    "\n",
    "            output_indices_val = torch.tensor(list(range(0, MAX_LENGTH))).cuda() # remove EOS_token from the output\n",
    "            decoder_output_val = torch.index_select(decoder_output_val, dim = 1, index = output_indices_val)\n",
    "\n",
    "            ground_truth_val = torch.argmax(decoder_input_val, 2)\n",
    "            target_indices_val = torch.tensor(list(range(1, MAX_LENGTH + 1))).cuda() # remove SOS_token from the input\n",
    "            ground_truth_val = torch.index_select(ground_truth_val, dim = 1, index = target_indices_val)\n",
    "            #decoder_output_val = decoder_output_val.view(batch_size, output_size, MAX_LENGTH)\n",
    "            \n",
    "            loss_val = 0\n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                \n",
    "                loss_val += criterion(decoder_output_val[j], ground_truth_val[j])\n",
    "            \n",
    "            #loss_val = criterion(decoder_output_val[0], ground_truth_val[0]) + criterion(decoder_output_val[1], ground_truth_val[1])\n",
    "            loss_val = loss_val/batch_size\n",
    "        \n",
    "        \n",
    "    train_losses.append(loss)\n",
    "    val_losses.append(loss_val)\n",
    "    print(i)\n",
    "    \n",
    "print(f'Duration: {(time.time() - start_time)/60} minutes')    \n",
    "print(train_losses)\n",
    "print(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting results\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(train_losses, label='training loss')\n",
    "plt.plot(val_losses, label='validation loss')\n",
    "plt.title('Losses per epoch CNN-S2S 8 words')\n",
    "plt.legend();\n",
    "#plt.savefig('8 words training losses lr=0,001, bs=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(CNN_model.state_dict(), 'CNN_model_8_words_batch.pt')\n",
    "#torch.save(Encoder_model.state_dict(), 'Encoder_model_8_words_batch.pt')\n",
    "#torch.save(Decoder_model.state_dict(), 'Decoder_model_8_words_batch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 28])\n",
      "torch.Size([1, 10, 28])\n",
      "tensor([[ 5, 19, 20, 15, 18,  9, 12,  0,  0,  0],\n",
      "        [ 6, 21,  5, 14,  7,  9, 18, 15, 12,  1]], device='cuda:0')\n",
      "['e', 's', 't', 'o', 'r', 'i', 'l', 'SOS_token', 'SOS_token', 'SOS_token']\n",
      "['f', 'u', 'e', 'n', 'g', 'i', 'r', 'o', 'l', 'a']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        \n",
    "    for t, (image_test, label_test) in enumerate(test_loader):\n",
    "\n",
    "        t += 1\n",
    "\n",
    "        encoder_hidden_test = Encoder_model.initHidden()\n",
    "        image_cnn_test = image_test.view(-1, color_channels, patch_height, patch_width).cuda()\n",
    "        encoder_input_test = CNN_model(image_cnn_test)\n",
    "        encoder_outputs_test, encoder_hidden_test = Encoder_model(encoder_input_test, encoder_hidden_test)\n",
    "    \n",
    "        for j in range(batch_size):\n",
    "           \n",
    "            decoder_input_test = letter_to_vector('SOS_token').cuda() # We initialize the first Decoder input as the SOS token\n",
    "            \n",
    "            decoder_hidden_test = (encoder_hidden_test[0][0, j, :].view(1, 1, hidden_size), # We take the last hidden state of the Encoder \n",
    "                                   encoder_hidden_test[1][0, j, :].view(1, 1, hidden_size)) # for each image/word (j) within the patch \n",
    "            # This would be the first hidden state of the Decoder for image/word (j)\n",
    "\n",
    "            for d in range(MAX_LENGTH):\n",
    "                \n",
    "                decoder_output_test, decoder_hidden_test = Decoder_model(decoder_input_test, decoder_hidden_test)\n",
    "                \n",
    "                if d == 0:\n",
    "                    \n",
    "                    output_word = decoder_output_test\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    output_word = torch.cat((output_word, decoder_output_test), dim = 1).cuda()\n",
    "                    \n",
    "            output_word = torch.argmax(output_word, dim = 2)\n",
    "            \n",
    "            \n",
    "            if j == 0:\n",
    "                \n",
    "                total_output_word = output_word\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                total_output_word = torch.cat((total_output_word, output_word), dim = 0)\n",
    "           \n",
    "    print(total_output_word)\n",
    "\n",
    "\n",
    "for j in range(batch_size):\n",
    "    \n",
    "    model_word = []\n",
    "    \n",
    "    for i in range(total_output_word[j].numel()):\n",
    "        \n",
    "        \n",
    "        model_word.append(letters[total_output_word[j][i]])\n",
    "        \n",
    "\n",
    "    #model_word = ''.join(model_word)    \n",
    "    print(model_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.2407, -0.3303, -1.2581],\n",
      "         [-0.1125, -1.2684, -0.3344]]])\n",
      "tensor([[[-1.3905, -0.8152, -0.3204],\n",
      "         [ 0.7377, -1.7534,  0.6033]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "c = nn.LogSoftmax(dim=1)\n",
    "a = torch.randn(1,2,3)\n",
    "print(c(a))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7392961570497802"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log((np.exp(-1.3905))/(np.exp(-1.3905) + np.exp(-0.8152) + np.exp(-0.3204)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.240681356437162"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log((np.exp(-1.3905))/(np.exp(-1.3905) + np.exp(0.7377)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor(27)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(28)\n",
    "c = torch.argmax(a, dim = 0)\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_estoril",
   "language": "python",
   "name": "pytorch_estoril"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
