{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "3.6.10\n"
     ]
    }
   ],
   "source": [
    "# IMPORTING THE REQUIRED LIBRARIES:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "import cv2\n",
    "import skimage\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Ignore harmless warnings:\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import platform\n",
    "print(torch.__version__)\n",
    "print(platform.python_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TITAN X (Pascal)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA: 30000 MOST COMMON ENGLISH WORDS\n",
    "\n",
    "top_30000_words = pd.read_csv('30k.txt', delimiter = '\\t', header = None)\n",
    "\n",
    "words = []\n",
    "sample_length = len(top_30000_words)\n",
    "\n",
    "for i in range(sample_length):\n",
    "    words.append(top_30000_words.loc[i][0])\n",
    "\n",
    "total_length = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2577\n",
      "12819\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "# FINDING PROBLEMATIC WORDS\n",
    "\n",
    "huge_words = 0\n",
    "\n",
    "for i in range(30000):\n",
    "    if type(words[i]) == float:\n",
    "        print(i)\n",
    "        \n",
    "    elif len(words[i]) > 15:\n",
    "        words[i] = 'gavab'\n",
    "        huge_words += 1\n",
    "        #print(i)\n",
    "\n",
    "words[2577] = 'null'\n",
    "words[12819] = 'nan'\n",
    "print(huge_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# DEFINING TRAINING, VALIDATION AND TEST SETS\n",
    "\n",
    "#train_set = words[0:28000]\n",
    "#val_set = words[28000:29000] \n",
    "test_set = words[29000:30000] \n",
    "test2 = words[20000:21000]\n",
    "\n",
    "MAX_LENGTH = max(len(list(word)) for word in words) # length of the longest word within our sample\n",
    "\n",
    "print(MAX_LENGTH)\n",
    "#print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# ONE HOT ENCODING OF THE ALPHABET (+ START, END & PAD)\n",
    "\n",
    "letters = ['START', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "          'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'END', 'PAD']\n",
    "\n",
    "\n",
    "def letter_to_vector(letter):\n",
    "    vector = torch.zeros(1, 1, len(letters))\n",
    "    for i in range(len(letters)):\n",
    "        if letters[i] == letter:\n",
    "            vector[0, 0, i] = 1.\n",
    "    return(vector)\n",
    "\n",
    "\n",
    "print(letter_to_vector('PAD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING PATCH TENSOR FOR EACH WORD (PATCH, COLOR CHANNEL, HEIGHT, WIDTH)\n",
    "\n",
    "def patch_gen(word, n_patches, patch_height, patch_width, stepsize):\n",
    "    \n",
    "    image = 255 * np.ones(shape = [height, width], dtype = np.uint8)\n",
    "    image = cv2.putText(image, text = word, org = (5, 30),\n",
    "        fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.62, color = (0, 0, 0),\n",
    "        thickness = 1, lineType = cv2.LINE_AA)\n",
    "    image = transforms.ToPILImage()(image) # np.ndarray to PIL.Image.Image\n",
    "    patches_tensor = torch.empty(n_patches, color_channels, patch_height, patch_width)\n",
    "    \n",
    "    for p in range(n_patches):\n",
    "        \n",
    "        patch = transforms.functional.crop(image, 0, 0 + p * stepsize, patch_height, patch_width) # cropping of the image into patches\n",
    "        patch = transforms.ToTensor()(patch) # torch.Tensor of the patch (normalized)\n",
    "        #patch = skimage.util.random_noise(patch, mode='s&p') # we set some random noise to the image\n",
    "        #patch = torch.from_numpy(patch) # conversion to pytorch tensor again\n",
    "        patch = 1. - patch # it will work better if we have white text over black background\n",
    "        patch = patch.view(1, 1, patch_height, patch_width) # CNN_model expects a 4-dimensional tensor (1 dimension for batch)\n",
    "        patch = patch.type(torch.FloatTensor) # conversion to float\n",
    "        patch = patch.cuda(1) # set to cuda\n",
    "        patches_tensor[p, 0, :, :] = patch\n",
    "        patches_tensor = patches_tensor.cuda(1)\n",
    "        \n",
    "    return patches_tensor,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING IMAGE AND SLIDING WINDOW (PATCH) PARAMETERS\n",
    "\n",
    "height = 48\n",
    "width = 192\n",
    "patch_height = 48\n",
    "patch_width = 10\n",
    "stepsize = 2\n",
    "color_channels = 1\n",
    "n_patches = int((width - patch_width)/stepsize + 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING TUPLE (TENSOR WITH PATCHES, LABEL) FOR EACH WORD OF A GIVEN SET:\n",
    "\n",
    "def complete_set(which_set):\n",
    "    \n",
    "    complete_set = []\n",
    "    \n",
    "    for word in which_set:\n",
    "        \n",
    "        complete_set.append((patch_gen(word, n_patches, patch_height, patch_width, stepsize), word))\n",
    "        \n",
    "    return complete_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUPLE OF (RANK-4 TENSOR [PATCH, CHANNEL, HEIGHT, WIDTH], LABEL) FOR EVERY WORD IN EVERY SET\n",
    "\n",
    "#comp_train_set = complete_set(which_set = train_set)\n",
    "#comp_val_set = complete_set(which_set = val_set)\n",
    "comp_test_set = complete_set(which_set = test_set)\n",
    "comp_test2 = complete_set(which_set = test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA AND SETS IN BATCHES\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "#train_loader = DataLoader(comp_train_set, batch_size = batch_size, shuffle = True)\n",
    "#val_loader = DataLoader(comp_val_set, batch_size = batch_size, shuffle = False)\n",
    "test_loader = DataLoader(comp_test_set, batch_size = batch_size, shuffle = False)\n",
    "test2_loader = DataLoader(comp_test2, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING ONE HOT TENSOR OF THE TARGET WORD\n",
    "\n",
    "    # one hot tensor of the input batch of words for the Decoder:\n",
    "    # 1 dim for position within batch, 1 dim for position of the letter in the fixed sequence (MAX_LENGTH + START + END),\n",
    "    # 1 final dim specifying letter (output_size = 26 + START and END tokens)\n",
    "    # torch.zeros() ensures that we always have PAD token vectors in case our target word is not MAX_LENGTH long\n",
    "    # WARNING: only works if label contains 2 words or more (it is OK as long as we are doing batching)\n",
    "    \n",
    "def get_one_hot_target(label, batch, seq_len, output_size):\n",
    "    \n",
    "    one_hot_target = torch.zeros(batch, seq_len, output_size) \n",
    "\n",
    "    for j in range(batch): # for each word of the batch\n",
    "\n",
    "        length = len(list(label[j])) # we compute the number of letters\n",
    "\n",
    "        one_hot_target[j, 0, :] = letter_to_vector('START') # the first letter of every word will always be the START\n",
    "\n",
    "        for k in range(0, length): # now for each letter of the target word\n",
    "\n",
    "            target_letter = list(label[j])[k] # picks the 'k' target letter \n",
    "            one_hot_target[j, k + 1, :] = letter_to_vector(target_letter) # adds that one hot target letter to our global tensor\n",
    "\n",
    "        one_hot_target[j, length + 1, :] = letter_to_vector('END') # we put END after the last letter of the word\n",
    "        \n",
    "        for m in range(length + 2, seq_len):\n",
    "            \n",
    "            one_hot_target[j, m, :] = letter_to_vector('PAD') # padding until the end of the sequence\n",
    "        \n",
    "    return one_hot_target        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERTS THE LOGSOFTMAX DECODER OUTPUT DURING EVALUATION TO THE CORRESPONDING ONE-HOT LETTER\n",
    "\n",
    "def one_hot_conversion(decoder_output, output_size):\n",
    "    \n",
    "    one_hot_output_letter = torch.zeros(1, 1, output_size).cuda(1)\n",
    "    index = torch.argmax(decoder_output, dim = 2).item()\n",
    "    one_hot_output_letter[0, 0, index] = 1.\n",
    "    \n",
    "    return one_hot_output_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING MODEL AND ARCHITECTURE\n",
    "\n",
    "# CONVOLUTIONAL NEURAL NETWORK:\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1, 2) # padding???\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1, 2)\n",
    "        self.fc1 = nn.Linear(12*2*50, 1024)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, 12*2*50) # -1 para no tener que determinar aquí el tamaño del batch (se ajusta, podemos variarlo)\n",
    "        X = F.relu(self.fc1(X))\n",
    "\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER:\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, input, hidden, batch, seq_len):\n",
    "        \n",
    "        output = input.view(batch, seq_len, input_size)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch):\n",
    "        return (torch.zeros(1, batch, self.hidden_size, device=device),\n",
    "                torch.zeros(1, batch, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECODER:\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, batch_first = True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 2) \n",
    "        # dim = 2 porque esta última dimensión es la correspondiente a output_size, que es sobre\n",
    "        # la que queremos hacer el softmax\n",
    "\n",
    "    def forward(self, input, hidden, batch, seq_len):\n",
    "        \n",
    "        output = input.view(batch, seq_len, output_size)\n",
    "        #output = F.relu(output) # la relu se metía aquí porque en el\n",
    "        #caso NLP del ejemplo de PyTorch previamente había una capa de embedding\n",
    "        #No nos hace falta porque nuestro tensor de inputs ya es one-hot\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch):\n",
    "        return (torch.zeros(1, batch, self.hidden_size, device=device),\n",
    "               torch.zeros(1, batch, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_size = 1024\n",
    "hidden_size = 256\n",
    "output_size = 29\n",
    "\n",
    "CNN_model = ConvolutionalNetwork().cuda(1)\n",
    "CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr = 0.001)\n",
    "\n",
    "Encoder_model = EncoderRNN(input_size = input_size, hidden_size = hidden_size).cuda(1)\n",
    "Encoder_optimizer = optim.SGD(Encoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "Decoder_model = DecoderRNN(hidden_size = hidden_size, output_size = output_size).cuda(1)\n",
    "Decoder_optimizer = optim.SGD(Decoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (lstm): LSTM(29, 256, batch_first=True)\n",
       "  (out): Linear(in_features=256, out_features=29, bias=True)\n",
       "  (softmax): LogSoftmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_model.load_state_dict(torch.load('CNN_model_30000_words_TF_PAD.pt'))\n",
    "CNN_model.eval()\n",
    "\n",
    "Encoder_model.load_state_dict(torch.load('Encoder_model_30000_words_TF_PAD.pt'))\n",
    "Encoder_model.eval()\n",
    "\n",
    "Decoder_model.load_state_dict(torch.load('Decoder_model_30000_words_TF_PAD.pt'))\n",
    "Decoder_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liberalisation', 'ornate', 'utilise', 'midwife', 'arturo', 'appellee', 'granules', 'uniformed', 'gavab', 'rollout', 'snug', 'homegrown', 'datamonitor', 'reinforces', 'coveted', 'dirham', 'leahy', 'myc', 'prohibitions', 'esophageal', 'moulded', 'deceived', 'kira', 'convict', 'approximations', 'forzieri', 'intermediates', 'kgs', 'albumin', 'grantees', 'nai', 'tossing', 'loveland', 'regularity', 'maloney', 'criticised', 'sativa', 'lawfully', 'paramedic', 'trademarked', 'edgewood', 'goethe', 'stressing', 'slade', 'potable', 'limpopo', 'intensities', 'oncogene', 'dumas', 'antidepressant', 'jester', 'notifies', 'recount', 'ballpark', 'powys', 'orca', 'mascara', 'proline', 'dearest', 'molina', 'nema', 'nook', 'wipers', 'snoopy', 'informationen', 'commensurate', 'esf', 'riverdale', 'schiller', 'bowler', 'unleash', 'juelz', 'bls', 'noarch', 'koss', 'captioned', 'paq', 'wiser', 'gallant', 'summarizing', 'ucsd', 'disbelief', 'gleason', 'gon', 'baritone', 'unqualified', 'cautioned', 'recollection', 'independant', 'chlamydia', 'relativistic', 'rotors', 'driscoll', 'andalucia', 'mulher', 'bagels', 'locomotives', 'condemns', 'fastening', 'jeweler', 'subliminal', 'insecticide', 'nuremberg', 'segal', 'ostrich', 'maud', 'spline', 'undisclosed', 'flirting', 'noni', 'letterman', 'almeria', 'bryson', 'misplaced', 'prosecutions', 'wtb', 'dido', 'towson', 'poisoned', 'researches', 'htaccess', 'malayalam', 'chou', 'discriminating', 'crue', 'loo', 'pinoy', 'pallets', 'uplink', 'sheboygan', 'exclamation', 'collingwood', 'terrence', 'intercepted', 'ghc', 'ascendant', 'flung', 'gateshead', 'probationary', 'abducted', 'warlock', 'breakup', 'clovis', 'fiche', 'juror', 'eam', 'bowden', 'goggle', 'railing', 'metabolites', 'cremation', 'brainstorm', 'banter', 'balconies', 'smu', 'awaken', 'ahl', 'bateman', 'egcs', 'chirac', 'museo', 'pigeons', 'coffeehouse', 'singularity', 'scitech', 'signify', 'granddaughter', 'gcn', 'trolling', 'elmore', 'subdirectory', 'bancroft', 'progeny', 'grads', 'alters', 'lz', 'andi', 'localpref', 'kayla', 'ccl', 'gratefully', 'divergent', 'fleets', 'smeg', 'dorian', 'donut', 'libido', 'juli', 'fuselage', 'diabetics', 'tackled', 'ballerina', 'crp', 'shoals', 'morgantown', 'paseo', 'tributary', 'clique', 'rosy', 'ptsd', 'redheads', 'curran', 'diam', 'satanic', 'ragnarok', 'stubbs', 'hkd', 'summarised', 'durch', 'torment', 'jx', 'mussels', 'caitlin', 'emigration', 'conscientious', 'howl', 'bandai', 'hobs', 'wel', 'iglesias', 'eft', 'endometriosis', 'cushioning', 'hir', 'mcneil', 'ecclesiastical', 'crippled', 'belvedere', 'hilltop', 'tabor', 'peut', 'nar', 'tenet', 'acetyl', 'boomer', 'fifteenth', 'chute', 'perinatal', 'idm', 'automake', 'multichannel', 'petr', 'bohemia', 'daredevil', 'corcoran', 'mountainous', 'mrp', 'holliday', 'daimlerchrysler', 'fonds', 'bowes', 'mcgowan', 'agfa', 'ogre', 'unforeseen', 'pickles', 'submissive', 'mep', 'curses', 'goss', 'mulch', 'stampede', 'jvm', 'utilised', 'harwood', 'trieste', 'ranma', 'marinas', 'whine', 'mobipocket', 'streptococcus', 'nus', 'murcia', 'landfills', 'mcknight', 'fatality', 'tierra', 'edd', 'baud', 'mcfarland', 'designline', 'looming', 'undies', 'zo', 'prepay', 'sped', 'kodiak', 'printout', 'nonresident', 'marysville', 'curso', 'palmos', 'dorsey', 'ankles', 'roo', 'soulful', 'mosques', 'websearch', 'infotrac', 'mpgs', 'fouls', 'openssh', 'bravenet', 'fuchs', 'guerilla', 'etsi', 'squeezing', 'fisk', 'canes', 'serendipity', 'tq', 'follower', 'euler', 'sequentially', 'yogi', 'landslide', 'howtos', 'skool', 'alumina', 'degenerate', 'spiked', 'evolves', 'cru', 'gavab', 'iberia', 'anakin', 'duffel', 'goodrich', 'strung', 'subfamily', 'chanting', 'wrestler', 'perennials', 'officiating', 'hermit', 'behaving', 'ary', 'colbert', 'matchmaker', 'sagittarius', 'locates', 'dysfunctional', 'maastricht', 'bulletproof', 'josiah', 'deepen', 'mcr', 'uga', 'stenosis', 'chg', 'acadia', 'eso', 'recentchanges', 'remy', 'pats', 'abrasion', 'valentin', 'eindhoven', 'mora', 'cri', 'enrico', 'reciprocity', 'opportunistic', 'pcl', 'analogs', 'bba', 'crease', 'hillcrest', 'cantor', 'wis', 'econometric', 'ook', 'trafford', 'opie', 'cro', 'bartholomew', 'elkhart', 'ringers', 'diced', 'fairgrounds', 'cuyahoga', 'perseverance', 'plt', 'cartons', 'mustangs', 'enc', 'addons', 'wstrict', 'catalonia', 'gow', 'pharmacological', 'headwear', 'paediatric', 'genitals', 'hendricks', 'ivr', 'telemedicine', 'judi', 'yorktown', 'impede', 'icom', 'academically', 'chilton', 'cbo', 'amaya', 'flickrblog', 'clasps', 'tilted', 'vicar', 'confines', 'fulbright', 'foaf', 'cllr', 'prank', 'xh', 'dass', 'repent', 'fulltext', 'dio', 'agreeable', 'centrum', 'tecra', 'kinks', 'riddles', 'unisys', 'preschools', 'bennington', 'mcallen', 'pulpit', 'appreciates', 'contoured', 'aberdeenshire', 'icm', 'schenectady', 'marshes', 'schematics', 'bellies', 'dojo', 'eserver', 'corrosive', 'ambush', 'nin', 'interfacing', 'borrowings', 'hrt', 'palazzo', 'franciscan', 'heparin', 'universiteit', 'figurative', 'gait', 'hardcopy', 'emphasised', 'connective', 'bonfire', 'aversion', 'nihon', 'oso', 'adkins', 'dunlap', 'nsc', 'irr', 'clonazepam', 'wikiname', 'gaithersburg', 'vicente', 'biophysics', 'chromatin', 'mathis', 'bulova', 'roxanne', 'fca', 'drg', 'stiles', 'stewards', 'refurb', 'chauffeur', 'wasteland', 'elicit', 'plotter', 'findlay', 'henrietta', 'slapped', 'bitten', 'cymraeg', 'alc', 'meek', 'lind', 'phonebook', 'doodle', 'arb', 'wabash', 'salamanca', 'martyn', 'dynamo', 'hobson', 'chronologically', 'wms', 'whitfield', 'stow', 'mchenry', 'eide', 'assy', 'dusseldorf', 'summon', 'skeletons', 'mmol', 'shabbat', 'nclb', 'parchment', 'accommodates', 'lingua', 'cmi', 'stacker', 'distractions', 'forfeit', 'pepe', 'paddles', 'unpopular', 'msf', 'republics', 'touchdowns', 'plasmas', 'inspecting', 'retainer', 'hardening', 'barbell', 'loosen', 'awk', 'bibs', 'beowulf', 'sneaky', 'undiscovered', 'einem', 'smarts', 'lankan', 'synthetase', 'imputed', 'lightwave', 'alignments', 'cabs', 'coached', 'cheated', 'jac', 'framingham', 'opensource', 'restroom', 'videography', 'lcr', 'spatially', 'doanh', 'willows', 'preprocessor', 'hump', 'cohn', 'delft', 'aon', 'marginally', 'ocs', 'bak', 'communicative', 'cavalli', 'grieving', 'ddc', 'grunge', 'chastity', 'invoicing', 'bigtits', 'carney', 'braintree', 'southside', 'vca', 'flipped', 'cabrera', 'faust', 'fright', 'harbors', 'adorned', 'obnoxious', 'mindy', 'diligently', 'surfaced', 'decays', 'glam', 'cowgirl', 'mortimer', 'marvellous', 'nouvelle', 'easing', 'loginlogin', 'mtr', 'nakamura', 'mathieu', 'layoffs', 'picket', 'matures', 'thrones', 'cty', 'emilia', 'eyre', 'apm', 'iggy', 'maturing', 'margarine', 'seu', 'illogical', 'awakened', 'beet', 'suing', 'brine', 'lorna', 'sneaker', 'waning', 'cartwright', 'glycoprotein', 'armoire', 'gcs', 'queued', 'sab', 'hydroxide', 'piled', 'hanley', 'cellulite', 'hwang', 'mtd', 'twinkle', 'mcqueen', 'lodgings', 'passat', 'fluff', 'shifter', 'maitland', 'cartography', 'supple', 'firstprevious', 'vito', 'geld', 'soi', 'fabio', 'predicates', 'bcl', 'unfit', 'uttered', 'douay', 'rumanian', 'zeitgeist', 'nickelodeon', 'dru', 'apar', 'tending', 'shaggy', 'elongated', 'ordeal', 'pegs', 'astronomer', 'hernia', 'preisvergleich', 'incompetence', 'britton', 'stabilizing', 'socom', 'wsis', 'anil', 'flicker', 'ramsay', 'midsize', 'relieving', 'pullover', 'towering', 'operas', 'slaughtered', 'lpn', 'hoodwinked', 'photoes', 'assaulted', 'beastie', 'mena', 'rouse', 'appel', 'yucca', 'armand', 'harvester', 'emmett', 'spiel', 'shay', 'impurities', 'stemming', 'inscriptions', 'obstructive', 'hos', 'pacman', 'tentatively', 'tragedies', 'interlude', 'oates', 'retroactive', 'briefed', 'bebe', 'dialects', 'krusell', 'vas', 'ovid', 'clickz', 'carcass', 'kermit', 'gizmo', 'atherosclerosis', 'casually', 'scamp', 'demography', 'freedman', 'migraines', 'wallingford', 'newborns', 'ljubljana', 'restarted', 'rnc', 'reprise', 'meow', 'thayer', 'kilograms', 'zig', 'packager', 'populate', 'lash', 'pembrokeshire', 'ills', 'arcane', 'impractical', 'simms', 'danes', 'tcg', 'gavab', 'honeymoons', 'authoritarian', 'alu', 'judaica', 'tropicana', 'tyan', 'cardholder', 'peavey', 'gothenburg', 'pebbles', 'geocaching', 'ident', 'fluoxetine', 'tipton', 'quicksilver', 'sacked', 'teva', 'lsa', 'omen', 'effortlessly', 'failover', 'forfeited', 'cysts', 'primetime', 'kenosha', 'kokomo', 'penney', 'stipend', 'conceptions', 'snorkel', 'amin', 'lii', 'iridium', 'dwyer', 'conserving', 'toppers', 'amulet', 'cfg', 'informally', 'tvc', 'alternator', 'nysgrc', 'underwriter', 'springhill', 'panhandle', 'sarcastic', 'joann', 'isoform', 'indemnification', 'hawke', 'borden', 'bombed', 'complexion', 'daisies', 'informant', 'elt', 'sorrows', 'halton', 'ite', 'guaranteeing', 'aegean', 'fasta', 'gonzaga', 'boobies', 'nadine', 'andere', 'breitling', 'nutr', 'ingersoll', 'sandia', 'pacs', 'azur', 'sluggish', 'helms', 'brig', 'beos', 'srcdir', 'tiempo', 'sherpa', 'tuff', 'marsden', 'coy', 'ligands', 'smalltalk', 'sorghum', 'grouse', 'nucleotides', 'mmv', 'ebi', 'reginald', 'wierd', 'sbd', 'pasted', 'moths', 'lmao', 'enhancers', 'collaborated', 'produ', 'lila', 'batavia', 'evoke', 'slotted', 'nnw', 'fila', 'decking', 'dispositions', 'haywood', 'staunton', 'boz', 'accelerators', 'howstuffworks', 'nit', 'amorphous', 'neighbourhoods', 'michal', 'tributaries', 'townships', 'rab', 'hideaway', 'dwayne', 'coda', 'nantes', 'cyanide', 'kostenlose', 'grotesk', 'assam', 'marek', 'interlibrary', 'mousse', 'provenance', 'sra', 'sog', 'zinkle', 'shameful', 'chiffon', 'fanfare', 'mapper', 'boyce', 'mlk', 'dystrophy', 'infomation', 'archaic', 'elevate', 'deafness', 'footballs', 'emailemail', 'bathurst', 'bec', 'sala', 'fof', 'duracell', 'laureate', 'feinstein', 'contemporaries', 'syphilis', 'vigilance', 'magnavox', 'appalling', 'palmyra', 'foxes', 'davie', 'evra', 'affixed', 'servlets', 'tss', 'neill', 'ticking', 'pantheon', 'gully', 'epithelium', 'bitterness', 'thc', 'brill', 'defy', 'stor', 'webbing', 'bef', 'jaya', 'consumes', 'lovingly', 'mame', 'agua', 'ppe', 'thrush', 'bribery', 'emusic', 'smokes', 'tso', 'epp', 'glencoe', 'untested', 'ventilated', 'overviews', 'affleck', 'kettles', 'ascend', 'flinders', 'informationhide', 'hearst', 'verifies', 'reverb', 'kays', 'commuters', 'rcp', 'nutmeg', 'welivetogether', 'crit', 'sdm', 'durbin', 'chained', 'riken', 'canceling', 'brookhaven', 'magnify', 'gauss', 'precautionary', 'artistry', 'travail', 'phpnuke', 'livres', 'fiddler', 'falkirk', 'wholesome', 'pitts', 'wrists', 'severed', 'dtp', 'mites', 'kwon', 'rubric', 'headlamp', 'operand', 'puddle', 'azores', 'kristi', 'yasmin', 'gnl', 'vegetative', 'acdbvertex', 'agora', 'illini', 'macho', 'sob', 'ningbo', 'elaborated', 'reeve', 'embellishments', 'willful', 'grandeur', 'plough', 'staphylococcus', 'pritchard', 'mansions', 'busting', 'foss', 'gfp', 'macpherson', 'overheard', 'yhoo', 'sloane', 'wooster', 'delong', 'persisted', 'mdi', 'nilsson', 'whereabouts', 'substring']\n",
      "['lieralisationi', 'ornate', 'utilise', 'midwife', 'arturo', 'appellee', 'granules', 'uniformed', 'gavab', 'rollout', 'snug', 'homegronn', 'dataniotorn', 'reinforces', 'coveetd', 'dirmam', 'leahy', 'myc', 'prohibitions', 'essopaegal', 'moulded', 'deceived', 'kira', 'convict', 'approximantios', 'forzaeie', 'intermedaties', 'kgs', 'allbinn', 'grantees', 'nai', 'tossing', 'loveland', 'regularity', 'maloney', 'criticiesd', 'sativa', 'lawfully', 'paramecic', 'trademarede', 'edgowiod', 'goethe', 'stressing', 'slade', 'potable', 'limpopo', 'intensities', 'oncongen', 'dumas', 'antipersensatr', 'jester', 'notifies', 'recount', 'ballpark', 'powws', 'orca', 'mascara', 'proline', 'dearest', 'molina', 'nema', 'nook', 'wipers', 'snoopy', 'informematine', 'commenuruate', 'esf', 'riverdale', 'schiller', 'bowler', 'unleash', 'julez', 'bls', 'noarch', 'koss', 'captioned', 'paq', 'wiser', 'gallant', 'summarizing', 'uscd', 'disbleeil', 'gleason', 'gon', 'baritone', 'uuqufialied', 'cautioned', 'recolection', 'indepenantn', 'chlhamiam', 'relattinitis', 'rotors', 'driscoll', 'andalicia', 'mulher', 'bagels', 'locomotives', 'condenss', 'fastening', 'jeweler', 'subliminal', 'insecticide', 'nuerbberg', 'segal', 'osticrh', 'maud', 'spline', 'undiclssoed', 'flitring', 'noni', 'letterman', 'almerra', 'bryson', 'misplaced', 'prosecutions', 'wtb', 'dido', 'towwon', 'poisoned', 'researcers', 'hatccess', 'mallaatam', 'chou', 'discriminating', 'crue', 'loo', 'pinoy', 'pallets', 'uplink', 'shegegara', 'exclamation', 'colligwoowd', 'terrence', 'intercepted', 'ghh', 'ascendant', 'flung', 'gateseade', 'probanitaroy', 'abducted', 'warlock', 'breakup', 'clovis', 'fiche', 'juror', 'eam', 'bowden', 'goggle', 'railing', 'metabolites', 'cremation', 'braorssorm', 'banter', 'balconies', 'smu', 'awaken', 'ahl', 'bateman', 'egcs', 'chirac', 'museo', 'pigeons', 'coofeffhoes', 'singurlatiy', 'scicech', 'signify', 'granouutglers', 'ggn', 'trolling', 'elmore', 'subbidetoroy', 'bancortt', 'progeny', 'grads', 'alters', 'lz', 'andi', 'lolaplere', 'kayla', 'ccl', 'gratefully', 'divergent', 'fleets', 'smeg', 'dorian', 'donut', 'libido', 'juli', 'fuselage', 'diabetics', 'tackled', 'ballerina', 'crp', 'shoals', 'mogtranwon', 'paseo', 'tributary', 'clique', 'rosy', 'ptsd', 'redheads', 'curran', 'diam', 'satanic', 'raghonak', 'stubbs', 'hkd', 'summarised', 'durch', 'torment', 'jx', 'mussess', 'caitiln', 'emgirtaion', 'consciteuious', 'howl', 'bandai', 'hobs', 'wel', 'iggersas', 'eft', 'enncatiorisoe', 'cushioning', 'hir', 'mcheil', 'ecclestinsiala', 'crippled', 'belvervee', 'hilltop', 'tabor', 'peut', 'nar', 'tenet', 'accelt', 'boomer', 'fiftetent', 'chute', 'perinatal', 'idm', 'automake', 'multhinallen', 'petr', 'bhheeri', 'dareviele', 'corcoran', 'mountionaus', 'mrp', 'holdilay', 'daimilesarsler', 'fonds', 'bowes', 'mccowan', 'affa', 'ogre', 'unfofeseen', 'pickles', 'submissive', 'mep', 'curses', 'goss', 'mulch', 'stampdee', 'jvm', 'utilised', 'harwood', 'trieste', 'ramma', 'marinas', 'whine', 'mocibbbcte', 'strepcoutcucs', 'nus', 'murcia', 'landfills', 'mckkintt', 'fatality', 'tierra', 'edd', 'baud', 'mccfrandd', 'designinel', 'looming', 'undies', 'zo', 'prepay', 'sped', 'kodiak', 'printout', 'norendisent', 'armvysille', 'curso', 'palmos', 'dorsey', 'ankles', 'roo', 'soulull', 'mosques', 'wewshharh', 'inforacc', 'mggs', 'fouls', 'openssh', 'bravenet', 'fuchs', 'guerilla', 'etsi', 'squezzeng', 'fisk', 'canes', 'seertindiny', 'tq', 'follower', 'euler', 'sequentially', 'yogi', 'landdlide', 'howtos', 'skool', 'alumina', 'degenerate', 'spiked', 'evolves', 'cru', 'gavab', 'iberia', 'anakin', 'duffel', 'goodrcih', 'strung', 'subbmamiy', 'chanting', 'wrestler', 'perennials', 'officatiing', 'hermit', 'beavivng', 'ary', 'colbert', 'matchrakek', 'sagtitritas', 'locates', 'dysfontincall', 'mascarchtt', 'buulerpoofl', 'josiai', 'deepen', 'mcr', 'uga', 'stenisos', 'chh', 'acadia', 'eso', 'recershencans', 'remy', 'pats', 'abrasion', 'valentin', 'eidionven', 'mora', 'cri', 'enrioc', 'recicpricot', 'opportunsitic', 'pcl', 'analogs', 'bab', 'crease', 'hillrsets', 'cantor', 'wis', 'econorestic', 'ook', 'trafford', 'opie', 'cro', 'barthlemwow', 'ekklrat', 'ringers', 'diced', 'fairiganogs', 'cughoaga', 'perserenacen', 'plt', 'cartons', 'mustangs', 'enc', 'addons', 'wwrstic', 'cataloina', 'gow', 'pharamologicanl', 'heawwear', 'paeditaric', 'genitals', 'hendricks', 'ivr', 'teleedinisen', 'judi', 'yortowwo', 'impede', 'icom', 'acadescially', 'chilton', 'cob', 'amaya', 'fllbicolrg', 'classs', 'tilted', 'vicar', 'confines', 'fulbright', 'foaf', 'cllr', 'prank', 'xx', 'dass', 'repent', 'fulllett', 'dio', 'agreabele', 'centrum', 'tecra', 'kinks', 'riddles', 'unisys', 'presholoss', 'bennintonn', 'mocalen', 'pulpit', 'appreciates', 'contoured', 'aberdeorhiere', 'icm', 'shhecencany', 'marshes', 'schematics', 'bellies', 'dojo', 'eserver', 'corrosive', 'ambush', 'nin', 'interfacing', 'borrowings', 'hrt', 'paazloz', 'franciscan', 'heparin', 'univeresiite', 'figurative', 'gait', 'hardcopy', 'emphasised', 'connective', 'bonfire', 'aversion', 'nihon', 'oso', 'adkins', 'dulapp', 'nsc', 'irr', 'clopanemam', 'wikimena', 'gagigtubbbrg', 'vicente', 'biophisess', 'chromaitn', 'matihs', 'bulvoa', 'roxanne', 'fca', 'drg', 'stiles', 'stewards', 'refurb', 'cauffefur', 'wasteland', 'eliict', 'plotter', 'findiay', 'henrertia', 'slapped', 'bitten', 'cymraen', 'alc', 'meek', 'lind', 'phoneobok', 'doodle', 'arb', 'wabsah', 'salamcana', 'martyn', 'dynamm', 'hobson', 'chronologically', 'wms', 'whitfield', 'stow', 'mhcenry', 'eide', 'assy', 'dusseolrdo', 'summon', 'skeletons', 'moml', 'shabbat', 'ncbl', 'parchment', 'accommodates', 'lingua', 'cmi', 'stacker', 'distractions', 'fofreit', 'pepe', 'paddles', 'unpoluapr', 'msf', 'repuliccs', 'touchdowns', 'plasmas', 'inspecting', 'retainer', 'hardening', 'barbell', 'loosen', 'akk', 'bibs', 'beofull', 'sneaky', 'undicsoveved', 'eimem', 'smarts', 'lankan', 'syntheatse', 'imputed', 'lightavev', 'alignments', 'cabs', 'coached', 'cheated', 'jac', 'framimamim', 'openauroce', 'restorom', 'viderohaphy', 'lcr', 'spatially', 'doanh', 'wlliows', 'prerceoseors', 'hump', 'cohn', 'defft', 'aon', 'marginally', 'ocs', 'bak', 'communcivaviv', 'cavalil', 'grieving', 'ddc', 'grunge', 'chastity', 'invociing', 'bigtits', 'carney', 'brarntiee', 'soustides', 'vca', 'flipped', 'cabrera', 'faust', 'fright', 'harbors', 'adorned', 'oxoniuoss', 'mindy', 'diligently', 'surfaced', 'decays', 'glam', 'cowwirl', 'mortioer', 'marvelolus', 'nouvelle', 'easing', 'loggiongin', 'mtr', 'nakkarau', 'matheiu', 'laloffs', 'picket', 'matures', 'thrones', 'cty', 'emiila', 'eyre', 'apm', 'iggy', 'maturing', 'margarine', 'seu', 'iloglical', 'awakened', 'beet', 'suing', 'brine', 'lorna', 'sneaker', 'waning', 'carthrigtt', 'glophroteici', 'armiore', 'gcs', 'queued', 'sab', 'hyddordie', 'piled', 'hanley', 'celluitle', 'hangg', 'mtd', 'twwline', 'mcqueen', 'lodgings', 'passat', 'fluff', 'shiffer', 'maitland', 'carthhraphy', 'supple', 'firtesorisuus', 'vito', 'geld', 'soi', 'fabio', 'predicates', 'bcl', 'unitt', 'uttered', 'douay', 'rumanian', 'feginseit', 'nicheledoon', 'dru', 'apar', 'tending', 'shaggy', 'elongated', 'ordeal', 'pegs', 'astrononer', 'hernia', 'preigelticeilh', 'incompectenc', 'britton', 'stablizingg', 'soocm', 'wiss', 'anil', 'flicker', 'ramasy', 'midsise', 'relieving', 'pullover', 'towering', 'operas', 'slaugheterd', 'lpn', 'hoodwikedd', 'photoes', 'assaluted', 'beastie', 'mena', 'rouse', 'appel', 'yucca', 'armand', 'harvester', 'emmett', 'spiel', 'shay', 'impritiens', 'stemming', 'inscriptions', 'obstrctitie', 'hos', 'pamama', 'tententatiy', 'trageides', 'interlude', 'oates', 'retrocative', 'briefed', 'bebe', 'dialects', 'krusell', 'vas', 'ovid', 'cllikz', 'carcass', 'kerimt', 'gimmm', 'atecroessorioss', 'casually', 'scamp', 'demography', 'freemann', 'migraines', 'walfilordid', 'newworns', 'jubuhana', 'restarted', 'rnc', 'reprise', 'meow', 'thayer', 'kiorumams', 'zig', 'packager', 'populate', 'lash', 'pemmenchrhhre', 'ills', 'arcane', 'imparctical', 'simms', 'danes', 'tcg', 'gavab', 'honemomons', 'authoriatiran', 'alu', 'judaica', 'tropicana', 'tana', 'cardidoler', 'peavey', 'gotheburug', 'pebbles', 'geochacing', 'ident', 'fluuvntine', 'tipton', 'quicissterr', 'sacked', 'teva', 'lsa', 'omen', 'efforsesllyy', 'failover', 'forfeited', 'cysts', 'primetime', 'kensoha', 'kkomom', 'penney', 'stineed', 'conceptions', 'snokkel', 'amin', 'lii', 'iidrium', 'dweyr', 'conserving', 'toppers', 'amulet', 'cfg', 'informally', 'ttc', 'alternator', 'nysgrc', 'underwirter', 'sprinirhll', 'pandandle', 'sarcatics', 'joann', 'isoform', 'indenisacitoion', 'hawke', 'borden', 'bombed', 'complexion', 'daisies', 'informant', 'elt', 'sororss', 'halton', 'ite', 'guarentineng', 'aeegan', 'fasta', 'gongaga', 'boobies', 'nadine', 'andere', 'breitling', 'nutr', 'ingesorll', 'sandia', 'pacs', 'azur', 'sluggish', 'helms', 'brig', 'beos', 'sricre', 'timepp', 'sherpa', 'tuff', 'marsden', 'coy', 'ligands', 'smaltallk', 'sorghum', 'grouse', 'nucleditoes', 'mmm', 'ebi', 'reginald', 'wired', 'sbd', 'pasted', 'moths', 'lamo', 'enhancers', 'collaborated', 'produ', 'lila', 'batavia', 'evoke', 'slotted', 'nnw', 'fila', 'decking', 'dispositions', 'haywood', 'stuanton', 'boz', 'accelerators', 'howwwsftowrks', 'nit', 'amporouhs', 'neighhubrouods', 'michal', 'tribburites', 'townshiss', 'rab', 'hideaway', 'dwanye', 'coda', 'nantes', 'cyyanie', 'kostensole', 'grotesk', 'assam', 'marek', 'interebirary', 'mousse', 'provenance', 'sra', 'sog', 'zinkle', 'shameful', 'coiffon', 'fanafer', 'mapper', 'boyce', 'mlk', 'dystropyy', 'infomation', 'archaic', 'elevate', 'deanfess', 'footlalls', 'emaiimeall', 'bathurst', 'bec', 'sala', 'fof', 'duraecll', 'lauratee', 'feiseinin', 'conteporaseris', 'syyhiils', 'vigilance', 'magnvavo', 'appalling', 'palrmya', 'foxes', 'davie', 'evra', 'affifed', 'servlets', 'tss', 'neill', 'ticking', 'pantelno', 'gully', 'epithielum', 'bittensers', 'thc', 'brill', 'defy', 'stor', 'webbing', 'bef', 'jaya', 'consumes', 'lovingil', 'mame', 'agua', 'ppe', 'thrush', 'bribery', 'emusic', 'smokes', 'tso', 'epp', 'glencoe', 'untested', 'ventilated', 'overveiss', 'affleck', 'kettles', 'ascend', 'flinders', 'informantidoned', 'hearst', 'verifies', 'reverb', 'kays', 'commuters', 'rcp', 'nutten', 'welivetogeoter', 'crit', 'sdm', 'durbin', 'chained', 'riken', 'canceling', 'borovhevan', 'maniffy', 'gauss', 'precautoniary', 'artistry', 'travial', 'phnhuen', 'livres', 'fiddler', 'falkirk', 'wholeomme', 'pitts', 'wrists', 'severed', 'dtp', 'mites', 'kwon', 'rubicc', 'heacumam', 'operand', 'puddle', 'azores', 'kristi', 'yasmin', 'gnl', 'vegetative', 'accuedertx', 'agora', 'illini', 'macho', 'sob', 'ninggo', 'elaborated', 'reeve', 'emebromhmentss', 'willfll', 'granduer', 'ploggh', 'staphlodlccucs', 'prithchar', 'mansions', 'busting', 'foss', 'guf', 'macphesorn', 'overhared', 'yyoo', 'sloane', 'wooster', 'delong', 'persisted', 'mdi', 'nilsson', 'whereouuats', 'substring']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_model_word = []\n",
    "    for t, (image_test, label_test) in enumerate(test_loader):\n",
    "\n",
    "        t += 1\n",
    "        test_batch = len(label_test)\n",
    "        \n",
    "        encoder_hidden_test = Encoder_model.initHidden(batch = test_batch)\n",
    "        \n",
    "        image_cnn_test = image_test.view(-1, color_channels, patch_height, patch_width).cuda(1)\n",
    "        encoder_input_test = CNN_model(image_cnn_test)\n",
    "        _, encoder_hidden_test = Encoder_model(encoder_input_test, encoder_hidden_test, batch = test_batch, seq_len = n_patches)\n",
    "        \n",
    "        for j in range(test_batch):\n",
    "           \n",
    "            decoder_input_test = letter_to_vector('START').cuda(1) # We initialize the first Decoder input as the START token\n",
    "\n",
    "            decoder_hidden_test = (encoder_hidden_test[0][0, j, :].view(1, 1, hidden_size), # We take the last hidden state of the Encoder \n",
    "                                   encoder_hidden_test[1][0, j, :].view(1, 1, hidden_size)) # for each image/word (j) within the batch \n",
    "            # This would be the first hidden state of the Decoder for image/word (j)\n",
    "\n",
    "            for d in range(MAX_LENGTH + 2):\n",
    "                \n",
    "                decoder_output_test, decoder_hidden_test = Decoder_model(decoder_input_test, decoder_hidden_test, \n",
    "                                                                         batch = 1, seq_len = 1)\n",
    "                \n",
    "                output_letter = one_hot_conversion(decoder_output_test, output_size = output_size)\n",
    "                decoder_input_test = output_letter\n",
    "                \n",
    "                if d == 0:\n",
    "                    \n",
    "                    output_word = output_letter\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    output_word = torch.cat((output_word, output_letter), dim = 1).cuda(1)\n",
    "                \n",
    "                \n",
    "                END_condition = torch.equal(output_letter, letter_to_vector('END').cuda(1))\n",
    "                \n",
    "                if  END_condition == True:\n",
    "                    \n",
    "                    break\n",
    "            \n",
    "            output_word = torch.argmax(output_word, dim=2)\n",
    "            output_word = output_word.view(output_word.numel()) # view as a rank-1 tensor\n",
    "            \n",
    "            model_word = []\n",
    "\n",
    "\n",
    "            for item in output_word:\n",
    "                model_word.append(letters[item])\n",
    "\n",
    "            model_word = ''.join(model_word[:-1])\n",
    "            #print(model_word)\n",
    "            total_model_word.append(model_word)\n",
    "      \n",
    "    print(test_set) \n",
    "print(total_model_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "\n",
    "test_length = len(test_set)\n",
    "for i in range(test_length):\n",
    "        \n",
    "    if total_model_word[i] == test_set[i]:\n",
    "\n",
    "        acc += 1\n",
    "            \n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "acc_vector = []\n",
    "\n",
    "for i in range(test_length):\n",
    "        \n",
    "    if total_model_word[i] == test_set[i]:\n",
    "\n",
    "        acc_vector.append(1)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        acc_vector.append(0)\n",
    "            \n",
    "print(acc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 6, 7, 7, 6, 8, 8, 9, 5, 7, 4, 9, 11, 10, 7, 6, 5, 3, 12, 10, 7, 8, 4, 7, 14, 8, 13, 3, 7, 8, 3, 7, 8, 10, 7, 10, 6, 8, 9, 11, 8, 6, 9, 5, 7, 7, 11, 8, 5, 14, 6, 8, 7, 8, 5, 4, 7, 7, 7, 6, 4, 4, 6, 6, 13, 12, 3, 9, 8, 6, 7, 5, 3, 6, 4, 9, 3, 5, 7, 11, 4, 9, 7, 3, 8, 11, 9, 12, 11, 9, 12, 6, 8, 9, 6, 6, 11, 8, 9, 7, 10, 11, 9, 5, 7, 4, 6, 11, 8, 4, 9, 7, 6, 9, 12, 3, 4, 6, 8, 10, 8, 9, 4, 14, 4, 3, 5, 7, 6, 9, 11, 11, 8, 11, 3, 9, 5, 9, 12, 8, 7, 7, 6, 5, 5, 3, 6, 6, 7, 11, 9, 10, 6, 9, 3, 6, 3, 7, 4, 6, 5, 7, 11, 11, 7, 7, 13, 3, 8, 6, 12, 8, 7, 5, 6, 2, 4, 9, 5, 3, 10, 9, 6, 4, 6, 5, 6, 4, 8, 9, 7, 9, 3, 6, 10, 5, 9, 6, 4, 4, 8, 6, 4, 7, 8, 6, 3, 10, 5, 7, 2, 7, 7, 10, 13, 4, 6, 4, 3, 8, 3, 13, 10, 3, 6, 14, 8, 9, 7, 5, 4, 3, 5, 6, 6, 9, 5, 9, 3, 8, 12, 4, 7, 9, 8, 11, 3, 8, 15, 5, 5, 7, 4, 4, 10, 7, 10, 3, 6, 4, 5, 8, 3, 8, 7, 7, 5, 7, 5, 10, 13, 3, 6, 9, 8, 8, 6, 3, 4, 9, 10, 7, 6, 2, 6, 4, 6, 8, 11, 10, 5, 6, 6, 6, 3, 7, 7, 9, 8, 4, 5, 7, 8, 5, 8, 4, 9, 4, 5, 11, 2, 8, 5, 12, 4, 9, 6, 5, 7, 10, 6, 7, 3, 5, 6, 6, 6, 8, 6, 9, 8, 8, 10, 11, 6, 8, 3, 7, 10, 11, 7, 13, 10, 11, 6, 6, 3, 3, 8, 3, 6, 3, 13, 4, 4, 8, 8, 9, 4, 3, 6, 11, 13, 3, 7, 3, 6, 9, 6, 3, 11, 3, 8, 4, 3, 11, 7, 7, 5, 11, 8, 12, 3, 7, 8, 3, 6, 7, 9, 3, 15, 8, 10, 8, 9, 3, 12, 4, 8, 6, 4, 12, 7, 3, 5, 10, 6, 6, 5, 8, 9, 4, 4, 5, 2, 4, 6, 8, 3, 9, 7, 5, 5, 7, 6, 10, 10, 7, 6, 11, 9, 13, 3, 11, 7, 10, 7, 4, 7, 9, 6, 3, 11, 10, 3, 7, 10, 7, 12, 10, 4, 8, 10, 10, 7, 8, 5, 3, 6, 6, 3, 3, 10, 8, 12, 7, 10, 9, 6, 6, 7, 3, 3, 6, 8, 6, 9, 9, 6, 7, 7, 9, 7, 6, 7, 3, 4, 4, 9, 6, 3, 6, 9, 6, 6, 6, 15, 3, 9, 4, 7, 4, 4, 10, 6, 9, 4, 7, 4, 9, 12, 6, 3, 7, 12, 7, 4, 7, 9, 3, 9, 10, 7, 10, 8, 9, 7, 6, 3, 4, 7, 6, 12, 5, 6, 6, 10, 7, 9, 10, 4, 7, 7, 3, 10, 10, 8, 11, 3, 9, 5, 7, 12, 4, 4, 5, 3, 10, 3, 3, 13, 7, 8, 3, 6, 8, 9, 7, 6, 9, 9, 3, 7, 7, 5, 6, 7, 7, 9, 5, 10, 8, 6, 4, 7, 8, 10, 8, 6, 10, 3, 8, 7, 7, 6, 7, 7, 3, 6, 4, 3, 4, 8, 9, 3, 9, 8, 4, 5, 5, 5, 7, 6, 10, 12, 7, 3, 6, 3, 9, 5, 6, 9, 5, 3, 7, 7, 8, 6, 5, 7, 8, 11, 6, 13, 4, 4, 3, 5, 10, 3, 5, 7, 5, 8, 9, 11, 3, 4, 7, 6, 9, 6, 4, 10, 6, 14, 12, 7, 11, 5, 4, 4, 7, 6, 7, 9, 8, 8, 6, 11, 3, 10, 7, 9, 7, 4, 5, 5, 5, 6, 9, 6, 5, 4, 10, 8, 12, 11, 3, 6, 11, 9, 9, 5, 11, 7, 4, 8, 7, 3, 4, 6, 7, 6, 5, 15, 8, 5, 10, 8, 9, 11, 8, 9, 9, 3, 7, 4, 6, 9, 3, 8, 8, 4, 13, 4, 6, 11, 5, 5, 3, 5, 10, 13, 3, 7, 9, 4, 10, 6, 10, 7, 10, 5, 10, 6, 11, 6, 4, 3, 4, 12, 8, 9, 5, 9, 7, 6, 6, 7, 11, 7, 4, 3, 7, 5, 10, 7, 6, 3, 10, 3, 10, 6, 11, 10, 9, 9, 5, 7, 15, 5, 6, 6, 10, 7, 9, 3, 7, 6, 3, 12, 6, 5, 7, 7, 6, 6, 9, 4, 9, 6, 4, 4, 8, 5, 4, 4, 6, 6, 6, 4, 7, 3, 7, 9, 7, 6, 11, 3, 3, 8, 5, 3, 6, 5, 4, 9, 12, 5, 4, 7, 5, 7, 3, 4, 7, 12, 7, 8, 3, 12, 13, 3, 9, 14, 6, 11, 9, 3, 8, 6, 4, 6, 7, 10, 7, 5, 5, 12, 6, 10, 3, 3, 6, 8, 7, 7, 6, 5, 3, 9, 10, 7, 7, 8, 9, 10, 8, 3, 4, 3, 8, 8, 9, 14, 8, 9, 8, 9, 7, 5, 5, 4, 7, 8, 3, 5, 7, 8, 5, 10, 10, 3, 5, 4, 4, 7, 3, 4, 8, 8, 4, 4, 3, 6, 7, 6, 6, 3, 3, 7, 8, 10, 9, 7, 7, 6, 8, 15, 6, 8, 6, 4, 9, 3, 6, 14, 4, 3, 6, 7, 5, 9, 10, 7, 5, 13, 8, 7, 7, 6, 7, 7, 9, 5, 6, 7, 3, 5, 4, 6, 8, 7, 6, 6, 6, 6, 3, 10, 10, 5, 6, 5, 3, 6, 10, 5, 14, 7, 8, 6, 14, 9, 8, 7, 4, 3, 10, 9, 4, 6, 7, 6, 9, 3, 7, 11, 9]\n"
     ]
    }
   ],
   "source": [
    "len_vector = []\n",
    "\n",
    "for i in range(test_length):\n",
    "    \n",
    "    len_vector.append(len(test_set[i]))\n",
    "    \n",
    "print(len_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 0, 2, 1, 0, 0, 0, 5, 0, 0, 0, 0, 4, 3, 3, 0, 4, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 3, 2, 0, 0, 0, 0, 0, 0, 4, 0, 6, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 5, 0, 0, 0, 5, 0, 7, 4, 6, 5, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 3, 0, 3, 0, 0, 5, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 2, 3, 0, 0, 0, 0, 0, 0, 0, 6, 0, 4, 0, 0, 1, 0, 0, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 5, 1, 0, 8, 1, 0, 0, 6, 3, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 2, 4, 3, 0, 0, 0, 0, 3, 0, 10, 0, 0, 1, 7, 0, 3, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 6, 0, 4, 5, 0, 3, 0, 2, 10, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 7, 5, 0, 0, 0, 3, 0, 0, 0, 0, 5, 4, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 2, 0, 5, 3, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 7, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, 0, 3, 0, 4, 0, 0, 2, 6, 0, 7, 6, 5, 1, 0, 0, 0, 2, 1, 0, 0, 8, 0, 0, 0, 0, 4, 0, 0, 2, 7, 2, 0, 0, 2, 0, 4, 0, 0, 3, 0, 0, 0, 0, 5, 4, 0, 0, 6, 4, 5, 0, 0, 0, 0, 0, 6, 2, 0, 10, 1, 2, 0, 0, 0, 8, 0, 4, 0, 0, 3, 0, 2, 0, 6, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 4, 3, 3, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 4, 7, 0, 3, 2, 2, 2, 0, 0, 0, 0, 0, 0, 5, 0, 2, 0, 1, 3, 0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 5, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 3, 1, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 4, 4, 2, 3, 0, 0, 0, 2, 9, 0, 0, 1, 0, 0, 0, 0, 7, 2, 0, 0, 0, 0, 2, 0, 0, 2, 6, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 4, 0, 5, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 10, 2, 0, 0, 0, 4, 0, 0, 3, 3, 0, 4, 0, 0, 0, 0, 1, 0, 2, 0, 8, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 7, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 7, 5, 0, 6, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 4, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 5, 0, 4, 6, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 9, 0, 0, 0, 3, 0, 6, 1, 9, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 7, 0, 0, 2, 0, 0, 0, 0, 3, 4, 0, 0, 0, 3, 4, 0, 4, 0, 3, 0, 3, 0, 5, 0, 0, 0, 0, 6, 0, 0, 0, 0, 2, 5, 0, 2, 0, 1, 0, 0, 3, 2, 0, 0, 0, 0, 0, 1, 0, 0, 2, 3, 1, 4, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 4, 2, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 4, 3, 0, 0, 0, 0, 0, 3, 0, 0, 4, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 6, 0, 6, 5, 0, 5, 1, 0, 0, 2, 0, 0, 4, 2, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 1, 0, 0, 0, 2, 1, 4, 0, 0, 0, 0, 2, 3, 4, 8, 3, 0, 4, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 5, 3, 0, 3, 0, 2, 4, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 5, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 1, 0, 0, 11, 1, 2, 1, 7, 5, 0, 0, 0, 2, 3, 3, 1, 0, 0, 0, 0, 0, 0, 4, 0]\n",
      "685\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARRUlEQVR4nO3db4xVeX3H8fenoOv/uNsdCAUsmBCVNXXXTqh2E9OKdvFPZJ9sMqaaSbsJfUCtNiYW2gdNH9Bs08Zo0q4N8d8kbt3Q1c0STa0ENaaJdZ3VrcoihboKUxBGjfVfspb12wdzNr0LM8wZZi4XfrxfyeSc87u/c+/nkuEzhzP3HFJVSJLa8iujDiBJWnmWuyQ1yHKXpAZZ7pLUIMtdkhq0etQBAG6++ebatGnTqGNI0jXlkUce+X5Vjc332FVR7ps2bWJ6enrUMSTpmpLkuws95mkZSWrQouWe5CVJHh34+nGSdyW5KcmhJMe75Y0D++xNciLJsSR3DPctSJIutGi5V9Wxqrq1qm4FfhP4OfAgsAc4XFVbgMPdNkm2AhPALcAO4N4kq4aUX5I0j6WeltkO/FdVfRfYCUx141PAnd36TuD+qnqiqh4HTgDbViKsJKmfpZb7BPDxbn1tVZ0B6JZruvH1wKmBfWa6sadJsivJdJLp2dnZJcaQJF1K73JP8kzgLcA/LzZ1nrGL7k5WVfuraryqxsfG5v0kjyTpMi3lyP0NwFer6my3fTbJOoBuea4bnwE2Duy3ATi93KCSpP6WUu5v5f9PyQAcBCa79UngoYHxiSQ3JNkMbAEeXm5QSVJ/vS5iSvIc4PXAHw0M3wMcSHI3cBK4C6CqjiQ5ADwGnAd2V9WTK5paknRJvcq9qn4O/OoFYz9g7tMz883fB+xbdrqeNu359NCe+zv3vGlozy1Jw+IVqpLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkN6lXuSV6Y5IEk30pyNMmrk9yU5FCS493yxoH5e5OcSHIsyR3Diy9Jmk/fI/f3A5+pqpcCrwCOAnuAw1W1BTjcbZNkKzAB3ALsAO5Nsmqlg0uSFrZouSd5AfAa4EMAVfWLqvoRsBOY6qZNAXd26zuB+6vqiap6HDgBbFvp4JKkhfU5cn8xMAt8JMnXknwwyXOBtVV1BqBbrunmrwdODew/0409TZJdSaaTTM/Ozi7rTUiSnq5Pua8GXgl8oKpuA35GdwpmAZlnrC4aqNpfVeNVNT42NtYrrCSpnz7lPgPMVNWXu+0HmCv7s0nWAXTLcwPzNw7svwE4vTJxJUl9LFruVfU94FSSl3RD24HHgIPAZDc2CTzUrR8EJpLckGQzsAV4eEVTS5IuaXXPee8A7kvyTODbwB8w94PhQJK7gZPAXQBVdSTJAeZ+AJwHdlfVkyueXJK0oF7lXlWPAuPzPLR9gfn7gH3LyCVJWgavUJWkBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqUK9yT/KdJN9I8miS6W7spiSHkhzvljcOzN+b5ESSY0nuGFZ4SdL8lnLk/rtVdWtVjXfbe4DDVbUFONxtk2QrMAHcAuwA7k2yagUzS5IWsZzTMjuBqW59CrhzYPz+qnqiqh4HTgDblvE6kqQl6lvuBXw2ySNJdnVja6vqDEC3XNONrwdODew7041Jkq6Q1T3n3V5Vp5OsAQ4l+dYl5maesbpo0twPiV0AL3rRi3rGkCT10evIvapOd8tzwIPMnWY5m2QdQLc8102fATYO7L4BOD3Pc+6vqvGqGh8bG7v8dyBJusii5Z7kuUme/9Q68HvAN4GDwGQ3bRJ4qFs/CEwkuSHJZmAL8PBKB5ckLazPaZm1wINJnpr/T1X1mSRfAQ4kuRs4CdwFUFVHkhwAHgPOA7ur6smhpJckzWvRcq+qbwOvmGf8B8D2BfbZB+xbdjpJ0mXxClVJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktSg3uWeZFWSryX5VLd9U5JDSY53yxsH5u5NciLJsSR3DCO4JGlhSzlyfydwdGB7D3C4qrYAh7ttkmwFJoBbgB3AvUlWrUxcSVIfvco9yQbgTcAHB4Z3AlPd+hRw58D4/VX1RFU9DpwAtq1MXElSH32P3N8HvAf45cDY2qo6A9At13Tj64FTA/NmurGnSbIryXSS6dnZ2SUHlyQtbNFyT/Jm4FxVPdLzOTPPWF00ULW/qsaranxsbKznU0uS+ljdY87twFuSvBF4FvCCJB8DziZZV1VnkqwDznXzZ4CNA/tvAE6vZGhJ0qUteuReVXurakNVbWLuF6Wfq6q3AQeByW7aJPBQt34QmEhyQ5LNwBbg4RVPLklaUJ8j94XcAxxIcjdwErgLoKqOJDkAPAacB3ZX1ZPLTipJ6m1J5V5VXwC+0K3/ANi+wLx9wL5lZpMkXSavUJWkBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lq0KLlnuRZSR5O8h9JjiT5q278piSHkhzvljcO7LM3yYkkx5LcMcw3IEm6WJ8j9yeA11bVK4BbgR1JXgXsAQ5X1RbgcLdNkq3ABHALsAO4N8mqYYSXJM1v0XKvOT/tNp/RfRWwE5jqxqeAO7v1ncD9VfVEVT0OnAC2rWhqSdIl9TrnnmRVkkeBc8ChqvoysLaqzgB0yzXd9PXAqYHdZ7qxC59zV5LpJNOzs7PLeQ+SpAv0KveqerKqbgU2ANuSvPwS0zPfU8zznPuraryqxsfGxvqllST1sqRPy1TVj4AvMHcu/WySdQDd8lw3bQbYOLDbBuD0spNKknrr82mZsSQv7NafDbwO+BZwEJjspk0CD3XrB4GJJDck2QxsAR5e6eCSpIWt7jFnHTDVfeLlV4ADVfWpJF8CDiS5GzgJ3AVQVUeSHAAeA84Du6vqyeHElyTNZ9Fyr6qvA7fNM/4DYPsC++wD9i07nSTpsniFqiQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDFi33JBuTfD7J0SRHkryzG78pyaEkx7vljQP77E1yIsmxJHcM8w1Iki7W58j9PPDuqnoZ8Cpgd5KtwB7gcFVtAQ5323SPTQC3ADuAe5OsGkZ4SdL8Fi33qjpTVV/t1n8CHAXWAzuBqW7aFHBnt74TuL+qnqiqx4ETwLaVDi5JWtiSzrkn2QTcBnwZWFtVZ2DuBwCwppu2Hjg1sNtMN3bhc+1KMp1kenZ2dunJJUkL6l3uSZ4HfAJ4V1X9+FJT5xmriwaq9lfVeFWNj42N9Y0hSeqhV7kneQZzxX5fVX2yGz6bZF33+DrgXDc+A2wc2H0DcHpl4kqS+ujzaZkAHwKOVtV7Bx46CEx265PAQwPjE0luSLIZ2AI8vHKRJUmLWd1jzu3A24FvJHm0G/tz4B7gQJK7gZPAXQBVdSTJAeAx5j5ps7uqnlzx5JKkBS1a7lX1b8x/Hh1g+wL77AP2LSOXJGkZvEJVkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGLlnuSDyc5l+SbA2M3JTmU5Hi3vHHgsb1JTiQ5luSOYQWXJC2sz5H7R4EdF4ztAQ5X1RbgcLdNkq3ABHBLt8+9SVatWFpJUi+LlntVfRH44QXDO4Gpbn0KuHNg/P6qeqKqHgdOANtWKKskqafLPee+tqrOAHTLNd34euDUwLyZbkySdAWt9C9UM89YzTsx2ZVkOsn07OzsCseQpOvb5Zb72STrALrluW58Btg4MG8DcHq+J6iq/VU1XlXjY2NjlxlDkjSfyy33g8Bktz4JPDQwPpHkhiSbgS3Aw8uLKElaqtWLTUjyceB3gJuTzAB/CdwDHEhyN3ASuAugqo4kOQA8BpwHdlfVk0PKLklawKLlXlVvXeCh7QvM3wfsW04oSdLyeIWqJDVo0SN3Dc+mPZ8e6vN/5543DfX5JV29PHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDfKjkA0b5kct/ZildHXzyF2SGuSR+yKGfaGRJA2D5a7L4tW10tXN0zKS1CDLXZIaZLlLUoM8566rkh/jlJbHI3dJapDlLkkN8rSMrjue8tH1wCN3SWqQR+7SCvLiLl0tPHKXpAYNrdyT7EhyLMmJJHuG9TqSpIsN5bRMklXAPwCvB2aAryQ5WFWPDeP1pOuFvwxWX8M6574NOFFV3wZIcj+wE7DcpauUd0AdjWH9UB1Wua8HTg1szwC/NTghyS5gV7f50yTHhpTlQjcD379Cr7XSzD4aZr/yrtXcsMTs+ZtlvdavL/TAsMo984zV0zaq9gP7h/T6C0oyXVXjV/p1V4LZR8PsV961mhuunuzD+oXqDLBxYHsDcHpIryVJusCwyv0rwJYkm5M8E5gADg7ptSRJFxjKaZmqOp/kj4F/BVYBH66qI8N4rctwxU8FrSCzj4bZr7xrNTdcJdlTVYvPkiRdU7xCVZIaZLlLUoOuq3K/Vm+JkGRjks8nOZrkSJJ3jjrTUiRZleRrST416ixLkeSFSR5I8q3uz/7Vo87UV5I/7b5Xvpnk40meNepMC0ny4STnknxzYOymJIeSHO+WN44y40IWyP633ffM15M8mOSFo8h23ZT7wC0R3gBsBd6aZOtoU/V2Hnh3Vb0MeBWw+xrKDvBO4OioQ1yG9wOfqaqXAq/gGnkPSdYDfwKMV9XLmftQw8RoU13SR4EdF4ztAQ5X1RbgcLd9NfooF2c/BLy8qn4D+E9g75UOBddRuTNwS4Sq+gXw1C0RrnpVdaaqvtqt/4S5klk/2lT9JNkAvAn44KizLEWSFwCvAT4EUFW/qKofjTbVkqwGnp1kNfAcruLrTKrqi8APLxjeCUx161PAnVc0VE/zZa+qz1bV+W7z35m7zueKu57Kfb5bIlwTBTkoySbgNuDLo03S2/uA9wC/HHWQJXoxMAt8pDul9MEkzx11qD6q6r+BvwNOAmeA/6mqz4421ZKtraozMHdwA6wZcZ7L9YfAv4ziha+ncl/0lghXuyTPAz4BvKuqfjzqPItJ8mbgXFU9Muosl2E18ErgA1V1G/Azrt5TA0/TnZ/eCWwGfg14bpK3jTbV9SfJXzB3SvW+Ubz+9VTu1/QtEZI8g7liv6+qPjnqPD3dDrwlyXeYOw322iQfG22k3maAmap66l9IDzBX9teC1wGPV9VsVf0v8Engt0ecaanOJlkH0C3PjTjPkiSZBN4M/H6N6GKi66ncr9lbIiQJc+d+j1bVe0edp6+q2ltVG6pqE3N/3p+rqmviCLKqvgecSvKSbmg7184tq08Cr0rynO57ZzvXyC+DBxwEJrv1SeChEWZZkiQ7gD8D3lJVPx9Vjuum3LtfcDx1S4SjwIGr6JYIi7kdeDtzR76Pdl9vHHWo68A7gPuSfB24FfjrEefppfvXxgPAV4FvMPf3/Kq4JH4+ST4OfAl4SZKZJHcD9wCvT3Kcuf/0555RZlzIAtn/Hng+cKj7u/qPI8nm7QckqT3XzZG7JF1PLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUoP8DYX+kIysi6B4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "WER_model = []\n",
    "\n",
    "for i in range(test_length):\n",
    "    \n",
    "    WER = 0\n",
    "    model_length = len(total_model_word[i])\n",
    "    target_length = len(test_set[i])\n",
    "    \n",
    "    smaller = min(model_length, target_length)\n",
    "    bigger = max(model_length, target_length)\n",
    "    \n",
    "    WER += bigger - smaller # accounts for insertions and deletions\n",
    "    \n",
    "    for j in range(smaller):\n",
    "\n",
    "        if total_model_word[i][j] != test_set[i][j]: # accounts for substitutions\n",
    "\n",
    "            WER += 1\n",
    "\n",
    "    WER_model.append(WER)\n",
    "    \n",
    "print(WER_model)\n",
    "\n",
    "zeros = 0\n",
    "for i in range(len(WER_model)):\n",
    "    \n",
    "    if WER_model[i] == 0:\n",
    "        zeros += 1\n",
    "        \n",
    "print(zeros)\n",
    "\n",
    "bins = np.linspace(-0.5, 12.5, 14)\n",
    "plt.hist(WER_model, bins = bins)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model set</th>\n",
       "      <th>Test set</th>\n",
       "      <th>WER</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lieralisationi</td>\n",
       "      <td>liberalisation</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ornate</td>\n",
       "      <td>ornate</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>utilise</td>\n",
       "      <td>utilise</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>midwife</td>\n",
       "      <td>midwife</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arturo</td>\n",
       "      <td>arturo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model set        Test set  WER  Accuracy  Length\n",
       "0  lieralisationi  liberalisation   12         0      14\n",
       "1          ornate          ornate    0         1       6\n",
       "2         utilise         utilise    0         1       7\n",
       "3         midwife         midwife    0         1       7\n",
       "4          arturo          arturo    0         1       6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'Model set': total_model_word, 'Test set': test_set, 'WER': WER_model, 'Accuracy': acc_vector, 'Length': len_vector}\n",
    "data = pd.DataFrame(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_error = test_length - acc\n",
    "\n",
    "errors = []\n",
    "\n",
    "for i in range(2, MAX_LENGTH + 1):\n",
    "    \n",
    "    error = 0\n",
    "    \n",
    "    for j in range(test_length):\n",
    "        \n",
    "        if (len_vector[j] == i) & (acc_vector[j] == 0):\n",
    "            \n",
    "            error += 1\n",
    "            \n",
    "    error = error/total_error\n",
    "    \n",
    "    errors.append(error)\n",
    "            \n",
    "#print(errors)        \n",
    "x = list(range(2,16)) \n",
    "height = errors\n",
    "plt.bar(x = x, height = height)\n",
    "plt.title('% of errors vs word length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activar pytorch_estoril (environment) en la terminal y ejecutar tensorboard --host 0.0.0.0 --logdir ./runs\n",
    "# Tensorboard se ejecutará en un cierto puerto y nos dará el enlace. Habrá que sustituir la IP 0.0.0.0 por la del equipo\n",
    "# en remoto en la que esté corriendo en el caso de Estoril 212.128.3.86:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# EFFECT OF SALT & PEPPER\n",
    "\n",
    "image = 255 * np.ones(shape = [height, width], dtype = np.uint8)\n",
    "image = cv2.putText(image, text = 'methamphetamine', org = (5, 30),\n",
    "    fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.62, color = (0, 0, 0),\n",
    "    thickness = 1, lineType = cv2.LINE_AA)\n",
    "image = transforms.ToPILImage()(image) \n",
    "image = transforms.ToTensor()(image)\n",
    "image = skimage.util.random_noise(image, mode='s&p') # we set some random noise to the image\n",
    "image = torch.from_numpy(image) # conversion to pytorch tensor again\n",
    "image = 1. - image\n",
    "#image = transforms.ToPILImage()(image)\n",
    "#plt.imshow(image, cmap='gray')\n",
    "#print(np.max(image))\n",
    "print(torch.min(image).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_estoril",
   "language": "python",
   "name": "pytorch_estoril"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
