{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n",
      "3.6.10\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "import cv2\n",
    "import skimage\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Ignore harmless warnings:\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import platform\n",
    "print(torch.__version__)\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TITAN X (Pascal)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ['hola', 'urjc', 'gavab', 'estoril', 'alvaro', 'victoria', 'jose', 'fuengirola'] * 512\n",
    "val_set = ['hola', 'urjc', 'gavab', 'estoril', 'alvaro', 'victoria', 'jose', 'fuengirola'] * 64\n",
    "\n",
    "len_train = len(train_set)\n",
    "len_val = len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# Function to convert letters (and therefore words) into PyTorch tensors:\n",
    "\n",
    "letters = ['SOS_token', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "          'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'EOS_token']\n",
    "\n",
    "def letter_to_vector(letter):\n",
    "    vector = torch.zeros(1, 1, len(letters))\n",
    "    for i in range(len(letters)):\n",
    "        if letters[i] == letter:\n",
    "            vector[0, 0, i] = 1.\n",
    "    return(vector)\n",
    "\n",
    "print(letter_to_vector('SOS_token'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_gen(word, n_patches, patch_height, patch_width, stepsize):\n",
    "    \n",
    "    image = 255 * np.ones(shape = [height, width], dtype = np.uint8)\n",
    "    image = cv2.putText(image, text = word, org = (5, 30),\n",
    "    fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.7, color = (0, 0, 0),\n",
    "    thickness = 2, lineType = cv2.LINE_AA)\n",
    "    image = transforms.ToPILImage()(image) # np.ndarray to PIL.Image.Image\n",
    "    patches_tensor = torch.empty(n_patches, 1, patch_height, patch_width)\n",
    "    \n",
    "    for p in range(n_patches):\n",
    "        \n",
    "        patch = transforms.functional.crop(image, 0, 0 + p * stepsize, patch_height, patch_width) # cropping of the image into patches\n",
    "        patch = transforms.ToTensor()(patch) # torch.Tensor of the patch (normalized)\n",
    "        #patch = skimage.util.random_noise(patch, mode='gaussian') # we set some random noise to the image\n",
    "        #patch = torch.from_numpy(patch) # conversion to pytorch tensor again\n",
    "        patch = 1. - patch # it will work better if we have white text over black background\n",
    "        patch = patch.view(1, 1, patch_height, patch_width) # CNN_model expects a 4-dimensional tensor (1 dimension for batch)\n",
    "        patch = patch.type(torch.FloatTensor) # conversion to float\n",
    "        patch = patch.cuda() # set to cuda\n",
    "        patches_tensor[p, 0, :, :] = patch\n",
    "        patches_tensor = patches_tensor.cuda()\n",
    "        \n",
    "    return patches_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting image and sliding window parameters:\n",
    "\n",
    "height = 48\n",
    "width = 192\n",
    "patch_height = 48\n",
    "patch_width = 10\n",
    "stepsize = 2\n",
    "color_channels = 1\n",
    "n_patches = int((width - patch_width)/stepsize + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to get a tuple for each image with (its 92 tensor patches, label):\n",
    "\n",
    "def complete_training_set(train_set):\n",
    "    complete_train_set = []\n",
    "    for word in train_set:\n",
    "        complete_train_set.append((patch_gen(word, n_patches, patch_height, patch_width, stepsize), word))\n",
    "        \n",
    "    return complete_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_train_set = complete_training_set(train_set = train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 92, 1, 48, 10])\n",
      "torch.Size([184, 1, 48, 10])\n",
      "('hola', 'estoril')\n"
     ]
    }
   ],
   "source": [
    "# Loading data in image batches:\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(comp_train_set, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "for image, label in train_loader:\n",
    "    break\n",
    "    \n",
    "image_cnn = image.view(-1, color_channels, patch_height, patch_width)   \n",
    "print(image.shape)\n",
    "print(image_cnn.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model and architecture:\n",
    "\n",
    "# CONVOLUTIONAL NEURAL NETWORK:\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1, 2) # padding???\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1, 2)\n",
    "        self.fc1 = nn.Linear(12*2*50, 1024)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, 12*2*50) # -1 para no tener que determinar aquí el tamaño del batch (se ajusta, podemos variarlo)\n",
    "        X = F.relu(self.fc1(X))\n",
    "\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER:\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        output = input.view(batch_size, n_patches, input_size)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECODER:\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, batch_first = True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = input.view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size, device=device),\n",
    "               torch.zeros(1, 1, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_size = 1024\n",
    "hidden_size = 256\n",
    "output_size = 28\n",
    "\n",
    "CNN_model = ConvolutionalNetwork().cuda()\n",
    "CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr = 0.001)\n",
    "\n",
    "Encoder_model = EncoderRNN(input_size = input_size, hidden_size = hidden_size).cuda()\n",
    "Encoder_optimizer = optim.SGD(Encoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "Decoder_model = DecoderRNN(hidden_size = hidden_size, output_size = output_size).cuda()\n",
    "Decoder_optimizer = optim.SGD(Decoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "[tensor(2.4829, device='cuda:0', grad_fn=<NllLossBackward>), tensor(2.2882, device='cuda:0', grad_fn=<NllLossBackward>), tensor(1.8988, device='cuda:0', grad_fn=<NllLossBackward>), tensor(1.3520, device='cuda:0', grad_fn=<NllLossBackward>), tensor(1.5173, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.4774, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.7076, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.4349, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.3039, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)]\n",
      "Duration: 39.60367764631907 minutes\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE MODEL:\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for b, (image, label) in enumerate(train_loader):\n",
    "        \n",
    "        b += 1\n",
    "        \n",
    "        encoder_hidden = Encoder_model.initHidden()\n",
    "        \n",
    "        image_cnn = image.view(-1, color_channels, patch_height, patch_width).cuda()\n",
    "        encoder_input = CNN_model(image_cnn)\n",
    "        encoder_outputs, encoder_hidden = Encoder_model(encoder_input, encoder_hidden)\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            \n",
    "            input_word = list(label[j]) \n",
    "            input_word_length = len(input_word)\n",
    "            \n",
    "            decoder_input = letter_to_vector('SOS_token').cuda() # We initialize the first Decoder input as the SOS token\n",
    "             \n",
    "            decoder_hidden = (encoder_hidden[0][0, j, :].view(1, 1, hidden_size), # We take the last hidden state of the Encoder\n",
    "                              encoder_hidden[1][0, j, :].view(1, 1, hidden_size)) # for each image/word (j) within the patch \n",
    "            # This would be the first hidden state of the Decoder for image/word (j)\n",
    "            \n",
    "            \n",
    "            for d in range(input_word_length):\n",
    "            \n",
    "                decoder_output, decoder_hidden = Decoder_model(decoder_input, decoder_hidden)\n",
    "                one_hot_decoder_output = torch.zeros(1, 1, output_size).cuda()\n",
    "                one_hot_decoder_output[0][0][torch.argmax(decoder_output)] = 1.\n",
    "                decoder_input = one_hot_decoder_output\n",
    "                \n",
    "                \n",
    "                if torch.equal(one_hot_decoder_output, letter_to_vector('EOS_token').cuda()) == True:\n",
    "                    break\n",
    "                    \n",
    "                \n",
    "                if d == 0:\n",
    "                \n",
    "                    output_word = decoder_output \n",
    "                    one_hot_input_letter = letter_to_vector(input_word[d]).type(torch.LongTensor).cuda()\n",
    "                    one_hot_input_word = one_hot_input_letter\n",
    "                    index = torch.argmax(one_hot_input_letter.view(output_size)).cuda()\n",
    "                    ground_letter = torch.tensor([index], dtype = torch.long).cuda()\n",
    "                    ground_word = ground_letter\n",
    "                \n",
    "                else:\n",
    "                \n",
    "                    output_word = torch.cat((output_word, decoder_output), dim = 0) # we concatenate the remaining output letters\n",
    "                    one_hot_input_letter = letter_to_vector(input_word[d]).type(torch.LongTensor).cuda()\n",
    "                    one_hot_input_word = torch.cat((one_hot_input_word, one_hot_input_letter), dim = 0)\n",
    "                    index = torch.argmax(one_hot_input_letter.view(output_size)).cuda()\n",
    "                    ground_letter = torch.tensor([index], dtype = torch.long).cuda()\n",
    "                    ground_word = torch.cat((ground_word, ground_letter), dim = 0)\n",
    "           \n",
    "        \n",
    "            one_hot_input_word = one_hot_input_word.view(-1, output_size)\n",
    "            output_word = output_word.view(-1, output_size) \n",
    "            \n",
    "            \n",
    "            loss = criterion(output_word, ground_word)\n",
    "            \n",
    "            CNN_optimizer.zero_grad()\n",
    "            Encoder_optimizer.zero_grad()\n",
    "            Decoder_optimizer.zero_grad()\n",
    "            \n",
    "            if j == (batch_size - 1):\n",
    "                loss.backward()\n",
    "                \n",
    "            else:\n",
    "                loss.backward(retain_graph=True)\n",
    "            \n",
    "            CNN_optimizer.step()\n",
    "            Encoder_optimizer.step()\n",
    "            Decoder_optimizer.step()\n",
    "            \n",
    "    train_losses.append(loss)\n",
    "            \n",
    "    print(i)\n",
    "print(train_losses)\n",
    "a = (time.time() - start_time)/60\n",
    "print(f'Duration: {(time.time() - start_time)/60} minutes')\n",
    "#print(encoder_input.shape)\n",
    "#print(encoder_hidden[0][0, 7, :])\n",
    "#print(encoder_outputs[7,91,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.yscale(\"log\")\n",
    "plt.plot(train_losses, label='training loss')\n",
    "#plt.savefig('8 words training losses lr=0,001, bs=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(CNN_model.state_dict(), 'CNN_model_8_words_batch.pt')\n",
    "#torch.save(Encoder_model.state_dict(), 'Encoder_model_8_words_batch.pt')\n",
    "#torch.save(Decoder_model.state_dict(), 'Decoder_model_8_words_batch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topv, topi = decoder_output.topk(1)\n",
    "topi.squeeze().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([1, 4, 6, 2, 5, 3]), batch_sizes=tensor([3, 2, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 0],\n",
      "        [6, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5])\n",
    "c = torch.tensor([6])\n",
    "packed = pack_sequence([a, b, c])\n",
    "print(packed)\n",
    "seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch_first=True)\n",
    "print(seq_unpacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "a[:-1]\n",
    "a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_estoril",
   "language": "python",
   "name": "pytorch_estoril"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
