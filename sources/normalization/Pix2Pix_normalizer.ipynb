{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1czVdIlqnImH"
   },
   "source": [
    "# Pix2Pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KD3ZgLs80vY"
   },
   "source": [
    "### Goals\n",
    "This notebook contains the implementation of a generative model based on the paper [*Image-to-Image Translation with Conditional Adversarial Networks*](https://arxiv.org/abs/1611.07004) by Isola et al. 2017, also known as Pix2Pix.\n",
    "\n",
    "The trained model would convert \"raw\" handwritten text images from the IAM dataset (\"input\" images that may contain elastic distortions and slopes and slants different from 0) in normalised handwritten text images (\"output\" images where distortions, slants and slopes have been removed). \n",
    "\n",
    "This has been done through paired image-to-image translation, the key concept behind the Pix2Pix model. The model will receive two different sets, the first of which will contain pre-processed images of handwritten words. In this set, the words have been normalised, which means that slopes and slants have been removed using classical algorithms. This would be the ground truth set. The second set will be created by performing son random operations (adding slants and/or slopes and/or elastic distortions) to the images in the first set, \"unnormalising\" them in the process. Therefore, given an image from the second set, the Pix2Pix network architecture should normalise the underlaying handwritten text, transforming that image into the corresponding image from the first, normalised set. \n",
    "\n",
    "Of course, the legibility of the word and the word itself must be preserved during the process.\n",
    "\n",
    "The following figure has been taken from the aforementioned paper and reflects the paired image-to-image translation that we are pursuing:\n",
    "\n",
    "\n",
    "![pix2pix example](pix2pix_ex.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wU8DDM6l9rZb"
   },
   "source": [
    "## Getting Started\n",
    "We will start by importing libraries and defining a visualization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfkorNJrnmNO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import skimage \n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import albumentations\n",
    "import h5py\n",
    "torch.manual_seed(0)\n",
    "\n",
    "path = \"/home/abarreiro/data/handwriting/seq2seq/IAM_words_48_192.hdf5\"\n",
    "path_raw = \"/home/abarreiro/data/handwriting/seq2seq/IAM_words_48_192_non_preprocessed.hdf5\"\n",
    "\n",
    "# set display defaults\n",
    "plt.rcParams['figure.figsize'] = (30, 30)        # large images\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # don't interpolate: show square pixels\n",
    "plt.rcParams['image.cmap'] = 'gray'  # use grayscale output rather than a (potentially misleading) color heatmap\n",
    "\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=12, size=(1, 48, 192)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_shifted = image_tensor\n",
    "    image_unflat = image_shifted.detach().cpu().view(-1, *size)\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=4)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "    return image_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting hdf5 file with PREPROCESSED images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_trn', 'X_tst', 'X_val', 'filename_trn', 'filename_tst', 'filename_val', 'image_length_trn', 'image_length_tst', 'image_length_val', 'target_dict_keys', 'target_dict_values', 'target_length_trn', 'target_length_tst', 'target_length_val', 'target_trn', 'target_tst', 'target_val']\n"
     ]
    }
   ],
   "source": [
    "filename = path\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    # List all groups\n",
    "    #print(\"Keys: %s\" % f.keys())\n",
    "    data_header = list(f.keys())\n",
    "    print(data_header)\n",
    "    data = []\n",
    "    \n",
    "    for item in data_header:\n",
    "        \n",
    "        # Getting data:\n",
    "        data.append(list(f[item]))\n",
    "        \n",
    "    # Creating dictionary between data names and data   \n",
    "    new_dict = {name: obj for name, obj in zip(data_header, data)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting hdf5 file with RAW images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_trn', 'X_tst', 'X_val', 'target_trn', 'target_tst', 'target_val']\n"
     ]
    }
   ],
   "source": [
    "filename = path_raw\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    # List all groups\n",
    "    #print(\"Keys: %s\" % f.keys())\n",
    "    data_header = list(f.keys())\n",
    "    print(data_header)\n",
    "    data = []\n",
    "    \n",
    "    for item in data_header:\n",
    "        \n",
    "        # Getting data:\n",
    "        data.append(list(f[item]))\n",
    "        \n",
    "    # Creating dictionary between data names and data   \n",
    "    new_dict_raw = {name: obj for name, obj in zip(data_header, data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe2faef0a58>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x2160 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(new_dict_raw['X_trn'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe2fae0b550>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x2160 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(new_dict['X_trn'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting an example of preprocessed and raw images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(new_dict['X_trn'][1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(new_dict_raw['X_trn'][1], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "Slope, slant and elastic transformations that we will apply to the normalised images. These will be the condition images of our Pix2Pix preprocessing architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_bbox(img):\\n    \\n    height = img.shape[0]\\n    width = img.shape[1]\\n    a = np.where(img != 0) \\n    \\n    bottom = np.max(a[0])\\n    right = np.max(a[1])\\n    x = np.min(a[1])\\n    y = np.min(a[0])\\n\\n    bbox = img[y:bottom+1, x:right+1]    \\n    w = bbox.shape[1]\\n    h = bbox.shape[0]\\n    \\n    bbox_params = [x, y, w, h]\\n    \\n    left_margin = x\\n    right_margin = width - right\\n    top_margin = y\\n    bottom_margin = height - bottom\\n    \\n    margin_sizes = [left_margin, right_margin, top_margin, bottom_margin]\\n    \\n    return bbox, bbox_params, margin_sizes\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_bbox(img):\n",
    "    \n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    a = np.where(img != 0) \n",
    "    \n",
    "    bottom = np.max(a[0])\n",
    "    right = np.max(a[1])\n",
    "    x = np.min(a[1])\n",
    "    y = np.min(a[0])\n",
    "\n",
    "    bbox = img[y:bottom+1, x:right+1]    \n",
    "    w = bbox.shape[1]\n",
    "    h = bbox.shape[0]\n",
    "    \n",
    "    bbox_params = [x, y, w, h]\n",
    "    \n",
    "    left_margin = x\n",
    "    right_margin = width - right\n",
    "    top_margin = y\n",
    "    bottom_margin = height - bottom\n",
    "    \n",
    "    margin_sizes = [left_margin, right_margin, top_margin, bottom_margin]\n",
    "    \n",
    "    return bbox, bbox_params, margin_sizes\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef erosion(img, kernel_size=2):\\n    # Sueiras et al. set kernel_size to 3, but visual inspection of the dilated/eroded images \\n    # suggests that 2 is a more reasonable and less extreme kernel size\\n    kernel = np.ones((kernel_size, kernel_size), 'uint8')\\n    eroded = cv2.erode(img, kernel, iterations=1)\\n    visible_pixels = len(np.where(eroded > 100)[0]) # the eroded text should be readable (> 100 in a scale 0-255)\\n    if visible_pixels > 50: # at least one character (~ 50 pixels) must be visible, not just one isolated pixel\\n        eroded = eroded\\n        \\n    else:\\n        eroded = img\\n        \\n    return eroded\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def erosion(img, kernel_size=2):\n",
    "    # Sueiras et al. set kernel_size to 3, but visual inspection of the dilated/eroded images \n",
    "    # suggests that 2 is a more reasonable and less extreme kernel size\n",
    "    kernel = np.ones((kernel_size, kernel_size), 'uint8')\n",
    "    eroded = cv2.erode(img, kernel, iterations=1)\n",
    "    visible_pixels = len(np.where(eroded > 100)[0]) # the eroded text should be readable (> 100 in a scale 0-255)\n",
    "    if visible_pixels > 50: # at least one character (~ 50 pixels) must be visible, not just one isolated pixel\n",
    "        eroded = eroded\n",
    "        \n",
    "    else:\n",
    "        eroded = img\n",
    "        \n",
    "    return eroded\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef dilation(img, margin_sizes, kernel_size=2):\\n    # Sueiras et al. set kernel_size to 3, but visual inspection of the dilated/eroded images \\n    # suggests that 2 is a more reasonable and less extreme kernel size\\n    pixel_excess = math.ceil((kernel_size - 1)//2) # number of pixels that might overflow the image limits\\n    # after dilation\\n    condition = np.any(np.array(margin_sizes) < pixel_excess) # if we overflow the image limits\\n    if condition == True:\\n        dilated = img # we don't do anything\\n        \\n    else:\\n        kernel = np.ones((kernel_size, kernel_size), 'uint8')\\n        dilated = cv2.dilate(img, kernel, iterations=1)\\n        \\n    return dilated\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def dilation(img, margin_sizes, kernel_size=2):\n",
    "    # Sueiras et al. set kernel_size to 3, but visual inspection of the dilated/eroded images \n",
    "    # suggests that 2 is a more reasonable and less extreme kernel size\n",
    "    pixel_excess = math.ceil((kernel_size - 1)//2) # number of pixels that might overflow the image limits\n",
    "    # after dilation\n",
    "    condition = np.any(np.array(margin_sizes) < pixel_excess) # if we overflow the image limits\n",
    "    if condition == True:\n",
    "        dilated = img # we don't do anything\n",
    "        \n",
    "    else:\n",
    "        kernel = np.ones((kernel_size, kernel_size), 'uint8')\n",
    "        dilated = cv2.dilate(img, kernel, iterations=1)\n",
    "        \n",
    "    return dilated\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef zoom_X(img):\\n    factor_X = random.uniform(0.9, 1.1) # 10% at most for zoom out or zoom\\n    new_width = math.ceil(factor_X*img.shape[1])\\n    dsize = (new_width, img.shape[0])\\n    zoomed_X = cv2.resize(img, dsize, interpolation = cv2.INTER_AREA)\\n    return zoomed_X\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def zoom_X(img):\n",
    "    factor_X = random.uniform(0.9, 1.1) # 10% at most for zoom out or zoom\n",
    "    new_width = math.ceil(factor_X*img.shape[1])\n",
    "    dsize = (new_width, img.shape[0])\n",
    "    zoomed_X = cv2.resize(img, dsize, interpolation = cv2.INTER_AREA)\n",
    "    return zoomed_X\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef zoom_Y(img):\\n    factor_Y = random.uniform(0.9, 1.1) # 10% at most for zoom out or zoom\\n    new_height = math.ceil(factor_Y*img.shape[0])\\n    dsize = (img.shape[1], new_height)\\n    zoomed_Y = cv2.resize(img, dsize, interpolation = cv2.INTER_AREA)\\n    return zoomed_Y\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def zoom_Y(img):\n",
    "    factor_Y = random.uniform(0.9, 1.1) # 10% at most for zoom out or zoom\n",
    "    new_height = math.ceil(factor_Y*img.shape[0])\n",
    "    dsize = (img.shape[1], new_height)\n",
    "    zoomed_Y = cv2.resize(img, dsize, interpolation = cv2.INTER_AREA)\n",
    "    return zoomed_Y\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef resize(new_img, src_img):\\n    \\n    height = src_img.shape[0]\\n    new_height = new_img.shape[0]\\n    width = src_img.shape[1] \\n    new_width = new_img.shape[1]\\n    \\n    width_ratio = width/new_width\\n    height_resized = math.ceil(new_height*width_ratio)\\n\\n    height_ratio = height/new_height\\n    width_resized = math.ceil(new_width*height_ratio)\\n    \\n    if new_width > width or new_height > height:\\n        \\n        if height_resized < height:\\n            \\n            dsize = (width, height_resized)\\n            img_rescaled = cv2.resize(new_img, dsize, interpolation = cv2.INTER_AREA)\\n            \\n        else:\\n            dsize = (width_resized, height)\\n            img_rescaled = cv2.resize(new_img, dsize, interpolation = cv2.INTER_AREA)\\n            \\n    else:\\n        img_rescaled = new_img\\n           \\n    return img_rescaled\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def resize(new_img, src_img):\n",
    "    \n",
    "    height = src_img.shape[0]\n",
    "    new_height = new_img.shape[0]\n",
    "    width = src_img.shape[1] \n",
    "    new_width = new_img.shape[1]\n",
    "    \n",
    "    width_ratio = width/new_width\n",
    "    height_resized = math.ceil(new_height*width_ratio)\n",
    "\n",
    "    height_ratio = height/new_height\n",
    "    width_resized = math.ceil(new_width*height_ratio)\n",
    "    \n",
    "    if new_width > width or new_height > height:\n",
    "        \n",
    "        if height_resized < height:\n",
    "            \n",
    "            dsize = (width, height_resized)\n",
    "            img_rescaled = cv2.resize(new_img, dsize, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "        else:\n",
    "            dsize = (width_resized, height)\n",
    "            img_rescaled = cv2.resize(new_img, dsize, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "    else:\n",
    "        img_rescaled = new_img\n",
    "           \n",
    "    return img_rescaled\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef fitting(src_img, new_bbox, bbox, bbox_params, margin_sizes):\\n    \\n    height = src_img.shape[0]\\n    width = src_img.shape[1]\\n    x, y, w, h = bbox_params # parameters of the original bbox to put the top-left edge of the new bbox in the first's position\\n    left_margin, right_margin, top_margin, bottom_margin = margin_sizes\\n    \\n    new_height = new_bbox.shape[0]\\n    new_width = new_bbox.shape[1]\\n    new_img = np.ones_like(src_img)*np.min(new_bbox)\\n    \\n    #new_img[(height - new_height)//2:-(height - new_height)//2, (width - new_width)//2:-(width - new_width)//2] = new_bbox\\n    \\n    fits_vertical = new_height + top_margin < height\\n    fits_horizont = new_width + left_margin < width\\n\\n    if fits_vertical == False and fits_horizont == False:\\n        \\n        diff_vertical = (new_height + top_margin) - height\\n        diff_horizont = (new_width + left_margin) - width\\n        new_x = x - diff_horizont\\n        new_y = y - diff_vertical\\n        \\n        new_img[new_y:(new_y + new_height), new_x:(new_x + new_width)] = new_bbox\\n        \\n    elif fits_vertical == False and fits_horizont == True:\\n        \\n        diff_vertical = (new_height + top_margin) - height\\n        new_x = x\\n        new_y = y - diff_vertical\\n        new_img[new_y:(new_y + new_height), new_x:(new_x + new_width)] = new_bbox\\n        \\n    elif fits_vertical == True and fits_horizont == False:\\n        \\n        diff_horizont = (new_width + left_margin) - width\\n        new_x = x - diff_horizont\\n        new_y = y\\n        new_img[new_y:(new_y + new_height), new_x:(new_x + new_width)] = new_bbox\\n        \\n    else:\\n        new_x = x\\n        new_y = y\\n        new_img[new_y:(new_y + new_height), new_x:(new_x + new_width)] = new_bbox\\n        \\n    #new_bbox_params = [new_x, new_y, new_width, new_height]\\n\\n    return new_img\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def fitting(src_img, new_bbox, bbox, bbox_params, margin_sizes):\n",
    "    \n",
    "    height = src_img.shape[0]\n",
    "    width = src_img.shape[1]\n",
    "    x, y, w, h = bbox_params # parameters of the original bbox to put the top-left edge of the new bbox in the first's position\n",
    "    left_margin, right_margin, top_margin, bottom_margin = margin_sizes\n",
    "    \n",
    "    new_height = new_bbox.shape[0]\n",
    "    new_width = new_bbox.shape[1]\n",
    "    new_img = np.ones_like(src_img)*np.min(new_bbox)\n",
    "    \n",
    "    #new_img[(height - new_height)//2:-(height - new_height)//2, (width - new_width)//2:-(width - new_width)//2] = new_bbox\n",
    "    \n",
    "    fits_vertical = new_height + top_margin < height\n",
    "    fits_horizont = new_width + left_margin < width\n",
    "\n",
    "    if fits_vertical == False and fits_horizont == False:\n",
    "        \n",
    "        diff_vertical = (new_height + top_margin) - height\n",
    "        diff_horizont = (new_width + left_margin) - width\n",
    "        new_x = x - diff_horizont\n",
    "        new_y = y - diff_vertical\n",
    "        \n",
    "        new_img[new_y:(new_y + new_height), new_x:(new_x + new_width)] = new_bbox\n",
    "        \n",
    "    elif fits_vertical == False and fits_horizont == True:\n",
    "        \n",
    "        diff_vertical = (new_height + top_margin) - height\n",
    "        new_x = x\n",
    "        new_y = y - diff_vertical\n",
    "        new_img[new_y:(new_y + new_height), new_x:(new_x + new_width)] = new_bbox\n",
    "        \n",
    "    elif fits_vertical == True and fits_horizont == False:\n",
    "        \n",
    "        diff_horizont = (new_width + left_margin) - width\n",
    "        new_x = x - diff_horizont\n",
    "        new_y = y\n",
    "        new_img[new_y:(new_y + new_height), new_x:(new_x + new_width)] = new_bbox\n",
    "        \n",
    "    else:\n",
    "        new_x = x\n",
    "        new_y = y\n",
    "        new_img[new_y:(new_y + new_height), new_x:(new_x + new_width)] = new_bbox\n",
    "        \n",
    "    #new_bbox_params = [new_x, new_y, new_width, new_height]\n",
    "\n",
    "    return new_img\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef displacement(new_img, new_bbox_params, d_x):\\n    # d_x has to be the amount of pixels\\n    new_x, new_y, new_width, new_height = new_bbox_params\\n    total_width = new_x + new_width + d_x\\n    if total_width > new_img.shape[1]:\\n        new_img_disp = new_img\\n        \\n    else:\\n        new_img_disp = new_img\\n        new_img_disp[:, d_x:] = new_img_disp[:, 0:-d_x]\\n        new_img_disp[:, 0:d_x] = np.min(new_img)\\n        \\n    return new_img_disp\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def displacement(new_img, new_bbox_params, d_x):\n",
    "    # d_x has to be the amount of pixels\n",
    "    new_x, new_y, new_width, new_height = new_bbox_params\n",
    "    total_width = new_x + new_width + d_x\n",
    "    if total_width > new_img.shape[1]:\n",
    "        new_img_disp = new_img\n",
    "        \n",
    "    else:\n",
    "        new_img_disp = new_img\n",
    "        new_img_disp[:, d_x:] = new_img_disp[:, 0:-d_x]\n",
    "        new_img_disp[:, 0:d_x] = np.min(new_img)\n",
    "        \n",
    "    return new_img_disp\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Alternative distortions via albumentations (paper by Simard et al. 2003)\\n\\ndef elastic_warp(original, alpha=16, sigma=8, alpha_affine=1):\\n    \\n    elastic = albumentations.augmentations.geometric.transforms.ElasticTransform(alpha=alpha, sigma=sigma, alpha_affine=1, \\n                                                                                 interpolation=1, border_mode=0, value=None, \\n                                                                                 mask_value=None, always_apply=False,\\n                                                                                 approximate=True, same_dxdy=False, p=1)\\n\\n    # Enlarging original word image in order to avoid being cropped after elastic transformation   \\n\\n    nh, nw = original.shape\\n    dw = nw//2 # left and right margins\\n    dh = nh//2 # bottom and top margins\\n    img = cv2.copyMakeBorder(original, dh, dh, dw, dw, borderType=cv2.BORDER_CONSTANT, value=(0,0,0))\\n\\n    # Bounding box of the original image\\n\\n    original_bbox, original_bbox_params, original_margin_sizes = get_bbox(original)\\n\\n    # Applying elastic transformation to the enlarged image\\n\\n    elastic_transf = elastic(image = img)\\n    bbox, bbox_params, margin_sizes = get_bbox(elastic_transf['image'])\\n    new_img = bbox\\n    #img_rescaled = resize(new_img, original)\\n    #new_img, new_bbox_params = fitting(original, img_rescaled, original_bbox, original_bbox_params, original_margin_sizes)\\n\\n    return new_img\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Alternative distortions via albumentations (paper by Simard et al. 2003)\n",
    "\n",
    "def elastic_warp(original, alpha=16, sigma=8, alpha_affine=1):\n",
    "    \n",
    "    elastic = albumentations.augmentations.geometric.transforms.ElasticTransform(alpha=alpha, sigma=sigma, alpha_affine=1, \n",
    "                                                                                 interpolation=1, border_mode=0, value=None, \n",
    "                                                                                 mask_value=None, always_apply=False,\n",
    "                                                                                 approximate=True, same_dxdy=False, p=1)\n",
    "\n",
    "    # Enlarging original word image in order to avoid being cropped after elastic transformation   \n",
    "\n",
    "    nh, nw = original.shape\n",
    "    dw = nw//2 # left and right margins\n",
    "    dh = nh//2 # bottom and top margins\n",
    "    img = cv2.copyMakeBorder(original, dh, dh, dw, dw, borderType=cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "\n",
    "    # Bounding box of the original image\n",
    "\n",
    "    original_bbox, original_bbox_params, original_margin_sizes = get_bbox(original)\n",
    "\n",
    "    # Applying elastic transformation to the enlarged image\n",
    "\n",
    "    elastic_transf = elastic(image = img)\n",
    "    bbox, bbox_params, margin_sizes = get_bbox(elastic_transf['image'])\n",
    "    new_img = bbox\n",
    "    #img_rescaled = resize(new_img, original)\n",
    "    #new_img, new_bbox_params = fitting(original, img_rescaled, original_bbox, original_bbox_params, original_margin_sizes)\n",
    "\n",
    "    return new_img\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef DataAugmentation(img):\\n    \\n    src_img = img.copy()\\n    bbox, bbox_params, margin_sizes = get_bbox(img)\\n    \\n    # Erosion/Dilation:\\n    if random.random() < 0.33:   # ~33% of the images are eroded\\n        img = erosion(img)\\n        \\n    elif random.random() < 0.33: # ~33% of the images are dilated\\n        img = dilation(img, margin_sizes)\\n       \\n    img, _, _ = get_bbox(img) # we have a new bbox after er./dil.  \\n    \\n    #Rotation:\\n    if (random.random() < 0.5 and img.max() > 0): # to perform a rotation we need a non-zero matrix\\n        angle = random.uniform(-10, 10)\\n        img = ndimage.rotate(img, angle, reshape=True)\\n        img, _, _ = get_bbox(img) # ... we'll get a new bbox after rotation\\n    \\n    #Zoom in X axis:\\n    if random.random() < 0.5:\\n        img = zoom_X(img)\\n              \\n    #Zoom in Y axis:\\n    if random.random() < 0.5:\\n        img = zoom_Y(img)\\n            \\n    # Warp distortion:\\n    if random.random() < 0.5:\\n        img = elastic_warp(img)\\n\\n    \\n    #Displacement:\\n    #p_disp = random.uniform(0,1)\\n    #if p_disp < 0.7:\\n    #    percentage_disp = random.uniform(0,0.1)\\n    #    d_x = math.ceil(percentage_disp*img.shape[1])\\n    #    new_img = displacement(new_img, new_bbox_params, d_x)\\n    #    \\n    #else:\\n    #    new_img = new_img\\n    \\n    \\n    # Rescaling the new bbox in order to fit in the 48x192 original format:\\n    img = resize(img, src_img)\\n    \\n    # Pasting the new bbox in the 48x192 image\\n    img = fitting(src_img, img, bbox, bbox_params, margin_sizes)\\n    \\n    # Normalisation\\n    img = img / 255\\n    \\n    # Salt & Pepper noise:\\n    if random.random() < 0.5:\\n        img = skimage.util.random_noise(img, mode = 's&p', amount = 0.10)\\n    \\n    return img\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def DataAugmentation(img):\n",
    "    \n",
    "    src_img = img.copy()\n",
    "    bbox, bbox_params, margin_sizes = get_bbox(img)\n",
    "    \n",
    "    # Erosion/Dilation:\n",
    "    if random.random() < 0.33:   # ~33% of the images are eroded\n",
    "        img = erosion(img)\n",
    "        \n",
    "    elif random.random() < 0.33: # ~33% of the images are dilated\n",
    "        img = dilation(img, margin_sizes)\n",
    "       \n",
    "    img, _, _ = get_bbox(img) # we have a new bbox after er./dil.  \n",
    "    \n",
    "    #Rotation:\n",
    "    if (random.random() < 0.5 and img.max() > 0): # to perform a rotation we need a non-zero matrix\n",
    "        angle = random.uniform(-10, 10)\n",
    "        img = ndimage.rotate(img, angle, reshape=True)\n",
    "        img, _, _ = get_bbox(img) # ... we'll get a new bbox after rotation\n",
    "    \n",
    "    #Zoom in X axis:\n",
    "    if random.random() < 0.5:\n",
    "        img = zoom_X(img)\n",
    "              \n",
    "    #Zoom in Y axis:\n",
    "    if random.random() < 0.5:\n",
    "        img = zoom_Y(img)\n",
    "            \n",
    "    # Warp distortion:\n",
    "    if random.random() < 0.5:\n",
    "        img = elastic_warp(img)\n",
    "\n",
    "    \n",
    "    #Displacement:\n",
    "    #p_disp = random.uniform(0,1)\n",
    "    #if p_disp < 0.7:\n",
    "    #    percentage_disp = random.uniform(0,0.1)\n",
    "    #    d_x = math.ceil(percentage_disp*img.shape[1])\n",
    "    #    new_img = displacement(new_img, new_bbox_params, d_x)\n",
    "    #    \n",
    "    #else:\n",
    "    #    new_img = new_img\n",
    "    \n",
    "    \n",
    "    # Rescaling the new bbox in order to fit in the 48x192 original format:\n",
    "    img = resize(img, src_img)\n",
    "    \n",
    "    # Pasting the new bbox in the 48x192 image\n",
    "    img = fitting(src_img, img, bbox, bbox_params, margin_sizes)\n",
    "    \n",
    "    # Normalisation\n",
    "    img = img / 255\n",
    "    \n",
    "    # Salt & Pepper noise:\n",
    "    if random.random() < 0.5:\n",
    "        img = skimage.util.random_noise(img, mode = 's&p', amount = 0.10)\n",
    "    \n",
    "    return img\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_Data_Aug(image_set, n_channels=1, height=48, width=192):\\n    set_length = len(image_set)\\n    images_da = torch.empty((set_length, n_channels, height, width), dtype=torch.float)\\n    counter = 0\\n    for image in image_set:\\n        try:\\n            if random.random() > 0.3:\\n                image = DataAugmentation(image) # normalisation included\\n                image = torch.FloatTensor(image)\\n                images_da[counter, :, :, :] = image\\n                counter += 1\\n            else:\\n                image = image / 255 # normalisation\\n                image = torch.FloatTensor(image)\\n                images_da[counter, :, :, :] = image\\n                counter += 1\\n        except:\\n            image = image / 255 # normalisation\\n            image = torch.FloatTensor(image)\\n            images_da[counter, :, :, :] = image\\n            counter += 1\\n\\n    return images_da\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_Data_Aug(image_set, n_channels=1, height=48, width=192):\n",
    "    set_length = len(image_set)\n",
    "    images_da = torch.empty((set_length, n_channels, height, width), dtype=torch.float)\n",
    "    counter = 0\n",
    "    for image in image_set:\n",
    "        try:\n",
    "            if random.random() > 0.3:\n",
    "                image = DataAugmentation(image) # normalisation included\n",
    "                image = torch.FloatTensor(image)\n",
    "                images_da[counter, :, :, :] = image\n",
    "                counter += 1\n",
    "            else:\n",
    "                image = image / 255 # normalisation\n",
    "                image = torch.FloatTensor(image)\n",
    "                images_da[counter, :, :, :] = image\n",
    "                counter += 1\n",
    "        except:\n",
    "            image = image / 255 # normalisation\n",
    "            image = torch.FloatTensor(image)\n",
    "            images_da[counter, :, :, :] = image\n",
    "            counter += 1\n",
    "\n",
    "    return images_da\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef transformations(img):\\n    \\n    #Performs random transformations (slant, slope and elastic distortions)\\n    #to the normalised images\\n    \\n    # Random shear:\\n    if random.uniform(0,1) < 0.75:\\n        shear_angle = -random.uniform(0,1)\\n        M = np.float32([[1, -shear_angle, 0.5*img.shape[0]*shear_angle], [0, 1, 0]])\\n        img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]), flags=cv2.WARP_INVERSE_MAP|cv2.INTER_LINEAR)\\n    # Random slope:\\n    if random.uniform(0,1) < 0.75:\\n        rot_angle = random.uniform(-5,5)\\n        img = ndimage.rotate(img, rot_angle, reshape=False)\\n    # Random elastic distortion:   \\n    if random.uniform(0,1) < 0.75:\\n        img = elastic_warp(original = img)\\n        \\n    return img\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def transformations(img):\n",
    "    \n",
    "    #Performs random transformations (slant, slope and elastic distortions)\n",
    "    #to the normalised images\n",
    "    \n",
    "    # Random shear:\n",
    "    if random.uniform(0,1) < 0.75:\n",
    "        shear_angle = -random.uniform(0,1)\n",
    "        M = np.float32([[1, -shear_angle, 0.5*img.shape[0]*shear_angle], [0, 1, 0]])\n",
    "        img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]), flags=cv2.WARP_INVERSE_MAP|cv2.INTER_LINEAR)\n",
    "    # Random slope:\n",
    "    if random.uniform(0,1) < 0.75:\n",
    "        rot_angle = random.uniform(-5,5)\n",
    "        img = ndimage.rotate(img, rot_angle, reshape=False)\n",
    "    # Random elastic distortion:   \n",
    "    if random.uniform(0,1) < 0.75:\n",
    "        img = elastic_warp(original = img)\n",
    "        \n",
    "    return img\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_transformed_images(image_set):\\n\\n    #Applies the function 'transformations' to a given set of images\\n\\n    transformed_images = []\\n    for image in image_set:\\n        try:\\n            transformed = transformations(image)\\n            transformed_images += [transformed]\\n        except:\\n            transformed = image\\n            transformed_images += [transformed]\\n    return transformed_images\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_transformed_images(image_set):\n",
    "\n",
    "    #Applies the function 'transformations' to a given set of images\n",
    "\n",
    "    transformed_images = []\n",
    "    for image in image_set:\n",
    "        try:\n",
    "            transformed = transformations(image)\n",
    "            transformed_images += [transformed]\n",
    "        except:\n",
    "            transformed = image\n",
    "            transformed_images += [transformed]\n",
    "    return transformed_images\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch generator\n",
    "\n",
    "These functions will extract random batches from the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_batch(set_random_sample, set_length, batch_size):\n",
    "    sorted_set_rs = []\n",
    "    j = 0\n",
    "    while (j + batch_size < set_length):\n",
    "        sorted_set_rs.append(np.sort(set_random_sample[j:j+batch_size]))\n",
    "        j = j + batch_size\n",
    "    \n",
    "    sorted_set_rs.append(np.sort(set_random_sample[j:]))\n",
    "    sorted_set_rs = np.concatenate(sorted_set_rs)\n",
    "    return sorted_set_rs\n",
    "\n",
    "def data_generator(len_set, image_set, random_sampling, size=(1, 48, 192)):\n",
    "    \n",
    "    f = h5py.File(path, \"r\")\n",
    "    f_raw = h5py.File(path_raw, \"r\")\n",
    "    j = 0\n",
    "    while 1:\n",
    "    \n",
    "        indices = random_sampling[j:j+batch_size]\n",
    "        data_X = f[image_set][indices]\n",
    "        data_X_raw = f_raw[image_set][indices]\n",
    "        \n",
    "        reals = torch.FloatTensor(data_X)\n",
    "        reals = reals / 255\n",
    "        reals = reals.view(-1, *size)\n",
    "        \n",
    "        conditions = torch.FloatTensor(data_X_raw)\n",
    "        #conditions = torch.FloatTensor(data_X)\n",
    "        conditions = conditions / 255\n",
    "        conditions = conditions.view(-1, *size)\n",
    "        \n",
    "        yield conditions, reals\n",
    "        \n",
    "        if j + 2*batch_size >= len_set: # drop_last = True\n",
    "            j = 0\n",
    "            break\n",
    "        else:\n",
    "            j +=  batch_size\n",
    "        \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NjFyvNTG1CqY"
   },
   "source": [
    "## U-Net Architecture\n",
    "\n",
    "Now we will build the U-Net architecture, which is the generator and main component of the Pix2Pix GAN model.\n",
    "\n",
    "The figure below is from the paper, [*U-Net: Convolutional Networks for Biomedical Image Segmentation*](https://arxiv.org/abs/1505.04597), by Ronneberger et al. 2015. It shows the U-Net architecture and how it contracts and then expands.\n",
    "\n",
    "<!-- \"[i]t consists of a contracting path (left side) and an expansive path (right side)\" (Renneberger, 2015) -->\n",
    "\n",
    "![Figure 1 from the paper, U-Net: Convolutional Networks for Biomedical Image Segmentation](https://drive.google.com/uc?export=view&id=1XgJRexE2CmsetRYyTLA7L8dsEwx7aQZY)\n",
    "\n",
    "In other words, images are first fed through many convolutional layers which reduce height and width while increasing the channels, which the authors refer to as the \"contracting path.\" For example, a set of two 2 x 2 convolutions with a stride of 2, will take a 1 x 28 x 28 (channels, height, width) grayscale image and result in a 2 x 14 x 14 representation. The \"expanding path\" does the opposite, gradually growing the image with fewer and fewer channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xvY4ZNyUviY9"
   },
   "outputs": [],
   "source": [
    "def crop(image, new_shape):\n",
    "    '''\n",
    "    Function for cropping an image tensor: Given an image tensor and the new shape,\n",
    "    crops to the center pixels (assumes that the input's size and the new size are\n",
    "    even numbers).\n",
    "    Parameters:\n",
    "        image: image tensor of shape (batch size, channels, height, width)\n",
    "        new_shape: a torch.Size object with the shape you want x to have\n",
    "    '''\n",
    "    middle_height = image.shape[2] // 2\n",
    "    middle_width = image.shape[3] // 2\n",
    "    starting_height = middle_height - new_shape[2] // 2\n",
    "    final_height = starting_height + new_shape[2]\n",
    "    starting_width = middle_width - new_shape[3] // 2\n",
    "    final_width = starting_width + new_shape[3]\n",
    "    cropped_image = image[:, :, starting_height:final_height, starting_width:final_width]\n",
    "    return cropped_image\n",
    "\n",
    "class ContractingBlock(nn.Module):\n",
    "    '''\n",
    "    ContractingBlock Class\n",
    "    Performs two convolutions followed by a max pool operation.\n",
    "    Values:\n",
    "        input_channels: the number of channels to expect from a given input\n",
    "    '''\n",
    "    def __init__(self, input_channels, use_dropout=False, use_bn=True):\n",
    "        super(ContractingBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, input_channels * 2, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(input_channels * 2, input_channels * 2, kernel_size=3, padding=1)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        if use_bn:\n",
    "            self.batchnorm = nn.BatchNorm2d(input_channels * 2)\n",
    "        self.use_bn = use_bn\n",
    "        if use_dropout:\n",
    "            self.dropout = nn.Dropout()\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Function for completing a forward pass of ContractingBlock: \n",
    "        Given an image tensor, completes a contracting block and returns the transformed tensor.\n",
    "        Parameters:\n",
    "            x: image tensor of shape (batch size, channels, height, width)\n",
    "        '''\n",
    "        #print('original', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1', x.shape)\n",
    "        if self.use_bn:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2', x.shape)\n",
    "        if self.use_bn:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxpool(x)\n",
    "        #print('maxpooling', x.shape)\n",
    "        return x\n",
    "\n",
    "class ExpandingBlock(nn.Module):\n",
    "    '''\n",
    "    ExpandingBlock Class:\n",
    "    Performs an upsampling, a convolution, a concatenation of its two inputs,\n",
    "    followed by two more convolutions with optional dropout\n",
    "    Values:\n",
    "        input_channels: the number of channels to expect from a given input\n",
    "    '''\n",
    "    def __init__(self, input_channels, use_dropout=False, use_bn=True):\n",
    "        super(ExpandingBlock, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv1 = nn.Conv2d(input_channels, input_channels // 2, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(input_channels, input_channels // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(input_channels // 2, input_channels // 2, kernel_size=2, padding=1)\n",
    "        if use_bn:\n",
    "            self.batchnorm = nn.BatchNorm2d(input_channels // 2)\n",
    "        self.use_bn = use_bn\n",
    "        self.activation = nn.ReLU()\n",
    "        if use_dropout:\n",
    "            self.dropout = nn.Dropout()\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "    def forward(self, x, skip_con_x):\n",
    "        '''\n",
    "        Function for completing a forward pass of ExpandingBlock: \n",
    "        Given an image tensor, completes an expanding block and returns the transformed tensor.\n",
    "        Parameters:\n",
    "            x: image tensor of shape (batch size, channels, height, width)\n",
    "            skip_con_x: the image tensor from the contracting path (from the opposing block of x)\n",
    "                    for the skip connection\n",
    "        '''\n",
    "        #print('original', x.shape)\n",
    "        x = self.upsample(x)\n",
    "        #print('upsampling', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1', x.shape)\n",
    "        skip_con_x = crop(skip_con_x, x.shape)\n",
    "        #print('skip connection', x.shape)\n",
    "        x = torch.cat([x, skip_con_x], axis=1)\n",
    "        #print('concat', x.shape)\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2', x.shape)\n",
    "        if self.use_bn:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv3(x)\n",
    "        #print('conv3', x.shape)\n",
    "        if self.use_bn:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class FeatureMapBlock(nn.Module):\n",
    "    '''\n",
    "    FeatureMapBlock Class\n",
    "    The final layer of a U-Net - \n",
    "    maps each pixel to a pixel with the correct number of output dimensions\n",
    "    using a 1x1 convolution.\n",
    "    Values:\n",
    "        input_channels: the number of channels to expect from a given input\n",
    "        output_channels: the number of channels to expect for a given output\n",
    "    '''\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(FeatureMapBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Function for completing a forward pass of FeatureMapBlock: \n",
    "        Given an image tensor, returns it mapped to the desired number of channels.\n",
    "        Parameters:\n",
    "            x: image tensor of shape (batch size, channels, height, width)\n",
    "        '''\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    '''\n",
    "    UNet Class\n",
    "    A series of 4 contracting blocks followed by 4 expanding blocks to \n",
    "    transform an input image into the corresponding paired image, with an upfeature\n",
    "    layer at the start and a downfeature layer at the end.\n",
    "    Values:\n",
    "        input_channels: the number of channels to expect from a given input\n",
    "        output_channels: the number of channels to expect for a given output\n",
    "    '''\n",
    "    def __init__(self, input_channels, output_channels, hidden_channels=32):\n",
    "        super(UNet, self).__init__()\n",
    "        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n",
    "        self.contract1 = ContractingBlock(hidden_channels, use_dropout=True)\n",
    "        self.contract2 = ContractingBlock(hidden_channels * 2, use_dropout=True)\n",
    "        self.contract3 = ContractingBlock(hidden_channels * 4, use_dropout=True)\n",
    "        self.expand0 = ExpandingBlock(hidden_channels * 8)\n",
    "        self.expand1 = ExpandingBlock(hidden_channels * 4)\n",
    "        self.expand2 = ExpandingBlock(hidden_channels * 2)\n",
    "        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Function for completing a forward pass of UNet: \n",
    "        Given an image tensor, passes it through U-Net and returns the output.\n",
    "        Parameters:\n",
    "            x: image tensor of shape (batch size, channels, height, width)\n",
    "        '''\n",
    "        x0 = self.upfeature(x)\n",
    "        #print('upfeature', x0.shape)\n",
    "        x1 = self.contract1(x0)\n",
    "        #print('contract1', x1.shape)\n",
    "        x2 = self.contract2(x1)\n",
    "        #print('contract2', x2.shape)\n",
    "        x3 = self.contract3(x2)\n",
    "        #print('contact3', x3.shape)\n",
    "        x4 = self.expand0(x3, x2)\n",
    "        #print('expand1', x4.shape)\n",
    "        x5 = self.expand1(x4, x1)\n",
    "        #print('expand2', x5.shape)\n",
    "        x6 = self.expand2(x5, x0)\n",
    "        #print('expand3', x6.shape)\n",
    "        xn = self.downfeature(x6)\n",
    "        #print('downfeature', xn.shape)\n",
    "        \n",
    "        return self.sigmoid(xn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T6ndvjc_1KXx"
   },
   "source": [
    "## PatchGAN Discriminator\n",
    "\n",
    "Next, we will define a discriminator based on the contracting path of the U-Net to allow you to evaluate the realism of the generated images. Recall that for the Pix2Pix architecture, the discriminator outputs a one-channel matrix of classifications instead of a single value. Our discriminator's final layer will simply map from the final number of hidden channels to a single prediction for every pixel of the layer before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nVuJPjV1f92"
   },
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED CLASS: Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Structured like the contracting path of the U-Net, the discriminator will\n",
    "    output a matrix of values classifying corresponding portions of the image as real or fake. \n",
    "    Parameters:\n",
    "        input_channels: the number of image input channels\n",
    "        hidden_channels: the initial number of discriminator convolutional filters\n",
    "    '''\n",
    "    def __init__(self, input_channels, hidden_channels=8):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n",
    "        self.contract1 = ContractingBlock(hidden_channels, use_bn=False)\n",
    "        self.contract2 = ContractingBlock(hidden_channels * 2)\n",
    "        self.final = nn.Conv2d(hidden_channels * 4, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        #print('x, y', x.shape, y.shape)\n",
    "        x = torch.cat([x, y], axis=1)\n",
    "        #print('concatenate', x.shape)\n",
    "        x0 = self.upfeature(x)\n",
    "        #print('upfeature', x0.shape)\n",
    "        x1 = self.contract1(x0)\n",
    "        #print('conctract1', x1.shape)\n",
    "        x2 = self.contract2(x1)\n",
    "        #print('contract2', x2.shape)\n",
    "        xn = self.final(x2)\n",
    "        #print('final', xn.shape)\n",
    "        \n",
    "        return xn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRk_8azSq3tF"
   },
   "source": [
    "## Training Preparation\n",
    "\n",
    "Now we can begin putting everything together for training. We start by defining some new parameters:\n",
    "  *   **real_dim**: the number of channels of the real image and the number expected in the output image\n",
    "  *   **adv_criterion**: an adversarial loss function to keep track of how well the GAN is fooling the discriminator and how well the discriminator is catching the GAN\n",
    "  *   **recon_criterion**: a loss function that rewards similar images to the ground truth, which \"reconstruct\" the image\n",
    "  *   **lambda_recon**: a parameter for how heavily the reconstruction loss should be weighed\n",
    "  *   **n_epochs**: the number of times you iterate through the entire dataset when training\n",
    "  *   **input_dim**: the number of channels of the input image\n",
    "  *   **display_step**: how often to display/visualize the images\n",
    "  *   **batch_size**: the number of images per forward/backward pass\n",
    "  *   **lr**: the learning rate\n",
    "  *   **target_shape**: the size of the output image (in pixels)\n",
    "  *   **device**: the device type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXptQZcwrBrq"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# New parameters\n",
    "adv_criterion = nn.BCEWithLogitsLoss() \n",
    "recon_criterion = nn.L1Loss() \n",
    "lambda_recon = 50\n",
    "\n",
    "len_trn = 47926\n",
    "len_val = 7558\n",
    "n_epochs = 500000\n",
    "input_dim = 1\n",
    "real_dim = 1\n",
    "display_step = 25\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "target_shape = (48, 192)\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPOUC6-nVDCv"
   },
   "source": [
    "We will then pre-process the images of the dataset to make sure they're all the same size and that the size change due to U-Net layers is accounted for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t7vKN1POUjud"
   },
   "source": [
    "Next, we can initialize your generator (U-Net) and discriminator, as well as their optimizers. Finally, we could also load our pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBY3Y9UrUgVX"
   },
   "outputs": [],
   "source": [
    "gen = UNet(input_dim, real_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "disc = Discriminator(input_dim + real_dim).to(device)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Feel free to change pretrained to False if you're training the model from scratch\n",
    "pretrained = False\n",
    "if pretrained:\n",
    "    loaded_state = torch.load(\"pix2pix_15000.pth\")\n",
    "    gen.load_state_dict(loaded_state[\"gen\"])\n",
    "    gen_opt.load_state_dict(loaded_state[\"gen_opt\"])\n",
    "    disc.load_state_dict(loaded_state[\"disc\"])\n",
    "    disc_opt.load_state_dict(loaded_state[\"disc_opt\"])\n",
    "else:\n",
    "    gen = gen.apply(weights_init)\n",
    "    disc = disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcpFbNDYzJrh"
   },
   "source": [
    "While there are some changes to the U-Net architecture for Pix2Pix, the most important distinguishing feature of Pix2Pix is its adversarial loss. We will be implementing that here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZE-Eyj0LOpm"
   },
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED CLASS: get_gen_loss\n",
    "def get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon):\n",
    "    '''\n",
    "    Return the loss of the generator given inputs.\n",
    "    Parameters:\n",
    "        gen: the generator; takes the condition and returns potential images\n",
    "        disc: the discriminator; takes images and the condition and\n",
    "          returns real/fake prediction matrices\n",
    "        real: the real images (e.g. normalised) to be used to evaluate the reconstruction\n",
    "        condition: the source images (e.g. unnormalised) which are used to produce the real images\n",
    "        adv_criterion: the adversarial loss function; takes the discriminator \n",
    "                  predictions and the true labels and returns a adversarial \n",
    "                  loss (which you aim to minimize)\n",
    "        recon_criterion: the reconstruction loss function; takes the generator \n",
    "                    outputs and the real images and returns a reconstructuion \n",
    "                    loss (which you aim to minimize)\n",
    "        lambda_recon: the degree to which the reconstruction loss should be weighted in the sum\n",
    "    '''\n",
    "    # Steps: 1) Generate the fake images, based on the conditions.\n",
    "    #        2) Evaluate the fake images and the condition with the discriminator.\n",
    "    #        3) Calculate the adversarial and reconstruction losses.\n",
    "    #        4) Add the two losses, weighting the reconstruction loss appropriately.\n",
    "    fake = gen(condition)\n",
    "    disc_fake_hat = disc(fake, condition)\n",
    "    gen_adv_loss = adv_criterion(disc_fake_hat, torch.ones_like(disc_fake_hat))\n",
    "    gen_rec_loss = recon_criterion(real, fake)\n",
    "    gen_loss = gen_adv_loss + lambda_recon * gen_rec_loss\n",
    "    \n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMDZWZTz3ivA"
   },
   "source": [
    "## Pix2Pix Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373,
     "referenced_widgets": [
      "aa7565ec3f294fd6b9c592bd5fc0dfcb",
      "fe98210470c3421c9a39734dc1203817",
      "a47c53e27edd4ef2b5bc79b3d64c44d3",
      "54a72fb618d146babc5830644ff65992",
      "6bf1b15c1b8e42758c8e9768115d6f8e",
      "bc3d591d82414f86888f2512bb3eb02a",
      "8cdec6ea735847709cc610fef8dc5755",
      "5042b39eadc14d5ab8b310e23d9c7d96"
     ]
    },
    "colab_type": "code",
    "id": "fy6UBV60HtnY",
    "outputId": "c174bb25-acbf-4507-c6e2-6bf7ef08661c"
   },
   "outputs": [],
   "source": [
    "from skimage import color\n",
    "import numpy as np\n",
    "random.seed(111)\n",
    "\n",
    "def train(epoch, image_to_tensorboard):\n",
    "    train_losses = []\n",
    "    mean_generator_loss = 0\n",
    "    mean_discriminator_loss = 0\n",
    "\n",
    "    random_sampling = random.sample(range(len_trn), len_trn)\n",
    "    random_sampling = sort_by_batch(random_sampling, len_trn, batch_size)\n",
    "    train_loader = data_generator(len_trn, image_set = 'X_trn', random_sampling = random_sampling)\n",
    "    \n",
    "    cur_step = 0\n",
    "\n",
    "    for conditions_trn, reals_trn in tqdm(train_loader, total=int(len_trn / batch_size)):\n",
    "        cur_batch_size = len(conditions_trn)\n",
    "        conditions_trn = conditions_trn.to(device)\n",
    "        reals_trn = reals_trn.to(device)\n",
    "\n",
    "        ### Update discriminator ###\n",
    "        disc_opt.zero_grad() # Zero out the gradient before backpropagation\n",
    "        with torch.no_grad():\n",
    "            fake = gen(conditions_trn)\n",
    "        disc_fake_hat = disc(fake.detach(), conditions_trn) # Detach generator\n",
    "        disc_fake_loss = adv_criterion(disc_fake_hat, torch.zeros_like(disc_fake_hat))\n",
    "        disc_real_hat = disc(reals_trn, conditions_trn)\n",
    "        disc_real_loss = adv_criterion(disc_real_hat, torch.ones_like(disc_real_hat))\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "        disc_loss.backward(retain_graph=True) # Update gradients\n",
    "        disc_opt.step() # Update optimizer\n",
    "\n",
    "        ### Update generator ###\n",
    "        gen_opt.zero_grad()\n",
    "        gen_loss = get_gen_loss(gen, disc, reals_trn, conditions_trn, adv_criterion, recon_criterion, lambda_recon)\n",
    "        gen_loss.backward() # Update gradients\n",
    "        gen_opt.step() # Update optimizer\n",
    "        \n",
    "        train_losses += [gen_loss.item()]\n",
    "\n",
    "        cur_step += 1\n",
    "        \n",
    "    return np.mean(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import color\n",
    "import numpy as np\n",
    "random.seed(111)\n",
    "\n",
    "def validation(epoch, image_to_tensorboard, writer, tensorboard_img_step):\n",
    "    validation_losses = []\n",
    "    mean_generator_loss = 0\n",
    "    mean_discriminator_loss = 0\n",
    "    \n",
    "    random_sampling = random.sample(range(len_val), len_val)\n",
    "    random_sampling = sort_by_batch(random_sampling, len_val, batch_size)\n",
    "    val_loader = data_generator(len_val, image_set = 'X_val', random_sampling = random_sampling)\n",
    "    \n",
    "    cur_step = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for conditions_val, reals_val in tqdm(val_loader, total=int(len_val / batch_size)):\n",
    "            cur_batch_size = len(conditions_val)\n",
    "            conditions_val = conditions_val.to(device)\n",
    "            reals_val = reals_val.to(device)\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            fake = gen(conditions_val)\n",
    "\n",
    "            disc_fake_hat = disc(fake.detach(), conditions_val) # Detach generator\n",
    "            disc_fake_loss = adv_criterion(disc_fake_hat, torch.zeros_like(disc_fake_hat))\n",
    "            disc_real_hat = disc(reals_val, conditions_val)\n",
    "            disc_real_loss = adv_criterion(disc_real_hat, torch.ones_like(disc_real_hat))\n",
    "            disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "\n",
    "            ### Update generator ###\n",
    "            gen_loss = get_gen_loss(gen, disc, reals_val, conditions_val, adv_criterion, recon_criterion, lambda_recon)\n",
    "\n",
    "            # Keep track of the average discriminator loss\n",
    "            mean_discriminator_loss += disc_loss.item() / display_step\n",
    "            # Keep track of the average generator loss\n",
    "            mean_generator_loss += gen_loss.item() / display_step\n",
    "\n",
    "            validation_losses += [gen_loss.item()]\n",
    "            \n",
    "            ### Visualization code ###\n",
    "            if cur_step % display_step == 0:\n",
    "                if cur_step > 0:\n",
    "                    print(f\"Epoch {epoch}: Step {cur_step}: Generator (U-Net) loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n",
    "                else:\n",
    "                    print(\"Raw, pre-processed and generated images\")\n",
    "\n",
    "                condition_images = show_tensor_images(conditions_val, size=(input_dim, *target_shape))\n",
    "                real_images = show_tensor_images(reals_val, size=(real_dim, *target_shape))\n",
    "                fake_images = show_tensor_images(fake, size=(real_dim, *target_shape))\n",
    "                '''\n",
    "                if image_to_tensorboard:\n",
    "                    \n",
    "                    writer.add_image('Images/Condition', condition_images, tensorboard_img_step)\n",
    "                    writer.add_image('Images/Real', real_images, tensorboard_img_step)\n",
    "                    writer.add_image('Images/Fake', fake_images, tensorboard_img_step)\n",
    "                    \n",
    "                    tensorboard_img_step += display_step\n",
    "                '''   \n",
    "                mean_generator_loss = 0\n",
    "                mean_discriminator_loss = 0\n",
    "            \n",
    "            cur_step += 1\n",
    "            \n",
    "    return np.mean(validation_losses), tensorboard_img_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patience():\n",
    "    \n",
    "    def __init__(self, save_model, patience):\n",
    "        self.save_model = save_model\n",
    "        self.patience = patience\n",
    "        self.current_patience = patience\n",
    "        self.min_loss_val = float('inf')\n",
    "        \n",
    "    def more_patience(self,loss_val):\n",
    "        self.current_patience -= 1\n",
    "        if self.current_patience == 0:\n",
    "            return False\n",
    "\n",
    "        if loss_val < self.min_loss_val:\n",
    "            self.min_loss_val = loss_val\n",
    "            self.current_patience = patience\n",
    "            '''\n",
    "            if self.save_model:\n",
    "                model_name = f\"IAM_Gen_lambda_50_from_raw_to_preprocessed.pt\"\n",
    "                print(\"Saved best model.\")\n",
    "                \n",
    "                torch.save({'gen_IAM_txt': gen.state_dict(),\n",
    "                            'gen_opt_IAM_txt': gen_opt.state_dict(),\n",
    "                            'disc_IAM_txt': disc.state_dict(),\n",
    "                            'disc_opt_IAM_txt': disc_opt.state_dict()\n",
    "                        }, model_name)\n",
    "                \n",
    "                torch.save(gen.state_dict(), 'Gen_'+model_name)\n",
    "                torch.save(disc.state_dict(), 'Disc_'+model_name)\n",
    "            '''\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e2bd7e297044aeab284ba2e2e7aa9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=374.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "upfeature torch.Size([128, 32, 48, 192])\n",
      "contract1 torch.Size([128, 64, 24, 96])\n",
      "contract2 torch.Size([128, 128, 12, 48])\n",
      "contact3 torch.Size([128, 256, 6, 24])\n",
      "expand1 torch.Size([128, 128, 12, 48])\n",
      "expand2 torch.Size([128, 64, 24, 96])\n",
      "expand3 torch.Size([128, 32, 48, 192])\n",
      "downfeature torch.Size([128, 1, 48, 192])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#writer = SummaryWriter()\n",
    "\n",
    "torch.manual_seed(111)\n",
    "patience = 200\n",
    "add_to_tensorboard = True\n",
    "image_to_tensorboard = True\n",
    "save_model = True\n",
    "tensorboard_img_step = 0\n",
    "\n",
    "patience_controler = Patience(save_model, patience)\n",
    "\n",
    "for num_epoch in range(n_epochs):\n",
    "\n",
    "    train_loss = train(epoch=num_epoch, image_to_tensorboard=image_to_tensorboard)        \n",
    "    valid_loss, tensorboard_img_step = validation(epoch=num_epoch, image_to_tensorboard=image_to_tensorboard,\n",
    "                                                  writer=None, tensorboard_img_step=tensorboard_img_step)\n",
    "    '''\n",
    "    if add_to_tensorboard:\n",
    "        writer.add_scalar('GenLoss/training', train_loss, num_epoch)\n",
    "        writer.add_scalar('GenLoss/validation', valid_loss, num_epoch)\n",
    "    '''           \n",
    "    if not patience_controler.more_patience(valid_loss):\n",
    "        print(\"NO MORE PATIENCE!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C3W2: Pix2Pix.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "GANSC3-2B"
   ]
  },
  "kernelspec": {
   "display_name": "pytorch_estoril",
   "language": "python",
   "name": "pytorch_estoril"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "5042b39eadc14d5ab8b310e23d9c7d96": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54a72fb618d146babc5830644ff65992": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5042b39eadc14d5ab8b310e23d9c7d96",
      "placeholder": "​",
      "style": "IPY_MODEL_8cdec6ea735847709cc610fef8dc5755",
      "value": " 23/275 [00:15&lt;02:49,  1.49it/s]"
     }
    },
    "6bf1b15c1b8e42758c8e9768115d6f8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8cdec6ea735847709cc610fef8dc5755": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a47c53e27edd4ef2b5bc79b3d64c44d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "  8%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc3d591d82414f86888f2512bb3eb02a",
      "max": 275,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6bf1b15c1b8e42758c8e9768115d6f8e",
      "value": 23
     }
    },
    "aa7565ec3f294fd6b9c592bd5fc0dfcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a47c53e27edd4ef2b5bc79b3d64c44d3",
       "IPY_MODEL_54a72fb618d146babc5830644ff65992"
      ],
      "layout": "IPY_MODEL_fe98210470c3421c9a39734dc1203817"
     }
    },
    "bc3d591d82414f86888f2512bb3eb02a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe98210470c3421c9a39734dc1203817": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
