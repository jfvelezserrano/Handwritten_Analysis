{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antes de empezar\n",
    "\n",
    "conda activate python3.6_cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import albumentations\n",
    "\n",
    "from scipy import ndimage\n",
    "import math\n",
    "import random\n",
    "import skimage \n",
    "import h5py\n",
    "\n",
    "import cProfile, pstats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Ignore harmless warnings:\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "3.6.10\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(torch.__version__)\n",
    "print(platform.python_version())\n",
    "torch.cuda.get_device_name(0)\n",
    "path = \"/home/abarreiro/data/handwriting/seq2seq/IAM_words_48_192.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definiendo diccionario, codificación y longitud máxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "# Dictionary used in seq2seq paper\n",
    "decoder_dict = {0: '0', 1: '!', 2: 'L', 3: 'z', 4: 'G', 5: 'm', 6: '6', 7: '/', 8: 'j', 9: 's', 10: 'S', 11: '5',\n",
    "                12: 'R', 13: ')', 14: 'u', 15: 'y', 16: '9', 17: 'g', 18: '3', 19: '1', 20: 'e', 21: \"'\", 22: ':',\n",
    "                23: 'Q', 24: '2', 25: 'a', 26: 't', 27: 'A', 28: '7', 29: ';', 30: 'i', 31: 'H', 32: 'W', 33: ',',\n",
    "                34: '(', 35: 'O', 36: 'U', 37: 'K', 38: 'd', 39: '*', 40: '.', 41: '?', 42: 'q', 43: '-', 44: 'r',\n",
    "                45: 'n', 46: '&', 47: 'C', 48: '\"', 49: 'h', 50: 'v', 51: 'f', 52: 'E', 53: 'p', 54: 'x', 55: '+',\n",
    "                56: 'w', 57: 'b', 58: 'o', 59: ' ', 60: 'B', 61: 'P', 62: 'D', 63: 'I', 64: 'J', 65: 'V', 66: 'N',\n",
    "                67: 'M', 68: '8', 69: 'k', 70: 'c', 71: '4', 72: 'T', 73: 'X', 74: 'l', 75: 'Z', 76: 'F', 77: 'Y',\n",
    "                78: 'START', 79: 'END', 80: 'PAD'}\n",
    "\n",
    "inverse_decoder_dict = {v: k for k, v in decoder_dict.items()}\n",
    "print(inverse_decoder_dict['END'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One_hot_mapping assigns to each number in decoder_dict its corresponding one-hot vector:\n",
    "\n",
    "one_hot_mapping = {}\n",
    "\n",
    "cont = 0\n",
    "for item in decoder_dict:\n",
    "    vector = torch.zeros(1, 1, len(decoder_dict))\n",
    "    vector[0, 0, cont] = 1.0\n",
    "    one_hot_mapping[item] = vector\n",
    "    cont += 1\n",
    "\n",
    "# Inverse_one_hot_mapping assigns to each one-hot vector its corresponding number in decoder_dict\n",
    "inverse_one_hot_mapping = {v: k for k, v in one_hot_mapping.items()}\n",
    "\n",
    "# One_hot_to_char assigns to each possible one-hot vector its corresponding character from decoder_dict\n",
    "one_hot_to_char = {}\n",
    "for one_hot, char in zip(inverse_one_hot_mapping, inverse_decoder_dict):\n",
    "    one_hot_to_char[one_hot] = char \n",
    "    \n",
    "# char_to_one_hot converts each character 'END', 'a', etc into a one-hot vector\n",
    "char_to_one_hot = {}\n",
    "for char, one_hot in zip(inverse_decoder_dict, inverse_one_hot_mapping): \n",
    "    char_to_one_hot[char] = one_hot\n",
    "    \n",
    "# Some examples...\n",
    "\n",
    "#print(one_hot_mapping[80])\n",
    "#print(inverse_one_hot_mapping[one_hot_mapping[80]])\n",
    "#print(one_hot_to_char[one_hot_mapping[80]])\n",
    "#print(char_to_one_hot['END'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 19\n",
    "output_size = len(decoder_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definiendo funciones para el Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport h5py\\nfilename = path\\n\\nwith h5py.File(filename, \"r\") as f:\\n    # List all groups\\n    #print(\"Keys: %s\" % f.keys())\\n    data_header = list(f.keys())\\n    print(data_header)\\n    data = []\\n    \\n    for item in data_header:\\n        \\n        # Getting data:\\n        data.append(list(f[item]))\\n        \\n    # Creating dictionary between data names and data   \\n    new_dict = {name: obj for name, obj in zip(data_header, data)}\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import h5py\n",
    "filename = path\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    # List all groups\n",
    "    #print(\"Keys: %s\" % f.keys())\n",
    "    data_header = list(f.keys())\n",
    "    print(data_header)\n",
    "    data = []\n",
    "    \n",
    "    for item in data_header:\n",
    "        \n",
    "        # Getting data:\n",
    "        data.append(list(f[item]))\n",
    "        \n",
    "    # Creating dictionary between data names and data   \n",
    "    new_dict = {name: obj for name, obj in zip(data_header, data)}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.New_DA_transf_proyectivas import ElasticDistortion\n",
    "from ipynb.fs.full.New_DA_transf_proyectivas import RandomTransform\n",
    "from ipynb.fs.full.New_DA_transf_proyectivas import move_img\n",
    "from ipynb.fs.full.New_DA_transf_proyectivas import resize_down\n",
    "from ipynb.fs.full.New_DA_transf_proyectivas import resize_up\n",
    "from ipynb.fs.full.New_DA_transf_proyectivas import img_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Data_Aug(image_set, batch_size=128):\n",
    "    images_da = [0]*batch_size\n",
    "    counter = 0\n",
    "    for image in image_set:\n",
    "        try:\n",
    "            image = img_augmented(image)         \n",
    "        except:\n",
    "            image = image / 255 # normalisation\n",
    "            \n",
    "        image = torch.FloatTensor(image)\n",
    "        images_da[counter] = image\n",
    "        counter += 1\n",
    "        \n",
    "    return images_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generando patches y etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_gen_IAM(image_set, batch_size=128, n_patches=92, patch_height=48, patch_width=10, stepsize=2):\n",
    "    total_pt = [0]*batch_size*n_patches\n",
    "    counter = 0\n",
    "    #n_patches = int((width - patch_width)/stepsize + 1) \n",
    "    for image in image_set:\n",
    "        #patches_tensor = torch.empty(patch_height, patch_width)    \n",
    "        start = 0\n",
    "        for p in range(n_patches):\n",
    "\n",
    "            total_pt[p + counter*n_patches] = image[:, start:start + patch_width].unsqueeze(0) # sliding window\n",
    "            start += stepsize # updating the bottom-left position of the patch adding the stepsize\n",
    "            \n",
    "        #total_pt += [patches_tensor]\n",
    "        counter += 1\n",
    "    total_pt = torch.cat(total_pt, dim=0).unsqueeze(1)\n",
    "    return total_pt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_one_hot_target_IAM(labels, seq_len=MAX_LENGTH+2, output_size=output_size, batch_size=128):\n",
    "    # labels: tensor containing the labels of the words in the batch\n",
    "    # each word label consists of a vector of length 19 (MAX LENGTH). The 19 elements are the encoded characters of the word\n",
    "    # (according to Jorge's decoder dict, and completed with PADs to reach length = 19)\n",
    "    one_hot_target = torch.empty(batch_size, seq_len, output_size) # future one-hot encoding tensor for the words of the batch\n",
    "    START = inverse_decoder_dict['START'] # code number of the START token (according to Jorge's decoder_dict)\n",
    "    END = inverse_decoder_dict['END']\n",
    "    PAD = inverse_decoder_dict['PAD']\n",
    "\n",
    "    for j, word in enumerate(labels):\n",
    "        \n",
    "        It_has_PADs = torch.any(word == PAD).item() # (majority case: the label vector of the word is completed with PADs)\n",
    "        one_hot_target[j, 0, :] = one_hot_mapping[START] # START token's one-hot vector goes first\n",
    "        \n",
    "        for k, letter in enumerate(word):\n",
    "            one_hot_target[j, k + 1, :] = one_hot_mapping[letter.item()] # one-hot encoding of the rest of letters (including PADs)\n",
    "            \n",
    "        one_hot_target[j, -1, :] = one_hot_mapping[END] # last = END token\n",
    "        \n",
    "        if It_has_PADs == True: # if we had PADs\n",
    "            \n",
    "            array_of_PADs = torch.where(word == PAD)[0] \n",
    "            first_PAD = torch.min(array_of_PADs).item() # we store the first position where it appeared\n",
    "            first_PAD = first_PAD + 1 # (recall that we added the START as first element, so the indices won't match)\n",
    "            one_hot_target[j, first_PAD, :] = one_hot_mapping[END] # we replace that first PAD by an END\n",
    "            one_hot_target[j, -1, :] = one_hot_mapping[PAD] # then the last element was a PAD, and not the END token\n",
    "            \n",
    "    return one_hot_target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef one_hot_conversion(decoder_output, output_size):\\n    \\n    one_hot_output_letter = torch.zeros(1, 1, output_size)\\n    index = torch.argmax(decoder_output, dim = 2).item()\\n    one_hot_output_letter[0, 0, index] = 1.\\n    \\n    return one_hot_output_letter\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def one_hot_conversion(decoder_output, output_size):\n",
    "    \n",
    "    one_hot_output_letter = torch.zeros(1, 1, output_size)\n",
    "    index = torch.argmax(decoder_output, dim = 2).item()\n",
    "    one_hot_output_letter[0, 0, index] = 1.\n",
    "    \n",
    "    return one_hot_output_letter\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generando datos por batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_batch(set_random_sample, set_length, batch_size):\n",
    "    '''\n",
    "    This function takes an array of size = set_length of indices\n",
    "    and sorts it in sub-arrays of size = batch_size\n",
    "    '''\n",
    "    sorted_set_rs = []\n",
    "    j = 0\n",
    "    while (j + batch_size < set_length):\n",
    "        sorted_set_rs.append(np.sort(set_random_sample[j:j+batch_size]))\n",
    "        j = j + batch_size\n",
    "    \n",
    "    sorted_set_rs.append(np.sort(set_random_sample[j:])) # adding last, smaller batch\n",
    "    sorted_set_rs = np.concatenate(sorted_set_rs) # concatenate everything in a single array\n",
    "    return sorted_set_rs\n",
    "\n",
    "def data_generator(image_set, target_set, random_sampling, mode, batch_size=128):\n",
    "    \n",
    "    f = h5py.File(path, \"r\")\n",
    "    j = 0\n",
    "    len_set = len(f[target_set])\n",
    "    \n",
    "    while 1:\n",
    "        indices = random_sampling[j:j+batch_size]\n",
    "        data_X = f[image_set][indices]\n",
    "\n",
    "        if mode == 'training':\n",
    "            data_X = get_Data_Aug(data_X)\n",
    "            \n",
    "        elif mode == 'validation':\n",
    "            data_X = data_X / 255\n",
    "            data_X = torch.FloatTensor(data_X)\n",
    "            \n",
    "        data_X = patch_gen_IAM(data_X)\n",
    "        \n",
    "        data_y = f[target_set][indices]\n",
    "        data_y = torch.ByteTensor(data_y) # for ~ 80 characters, 8-bit representation should be enough\n",
    "        data_y[data_y == 100] = 80 # replacing Jorge's coding of PAD token by ours\n",
    "        \n",
    "        yield data_X, data_y\n",
    "        \n",
    "        if j + 2*batch_size >= len_set: # drop last, smaller batch\n",
    "            j = 0\n",
    "            break\n",
    "        else:\n",
    "            j += batch_size\n",
    "        \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definiendo la arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, IN_CHANNELS, FILTERS_CNN_1, FILTERS_CNN_2, NEURONS_IN_DENSE_LAYER,\n",
    "                 PATCH_HEIGHT, PATCH_WIDTH, STRIDE, PADDING, KERNEL_SIZE, dropout_p):\n",
    "        super().__init__()\n",
    "        self.IN_CHANNELS = IN_CHANNELS\n",
    "        self.FILTERS_CNN_1 = FILTERS_CNN_1\n",
    "        self.FILTERS_CNN_2 = FILTERS_CNN_2\n",
    "        self.NEURONS_IN_DENSE_LAYER = NEURONS_IN_DENSE_LAYER\n",
    "        self.PATCH_HEIGHT_AFTER_POOLING = PATCH_HEIGHT//4\n",
    "        self.PATCH_WIDTH_AFTER_POOLING = PATCH_WIDTH//4\n",
    "        self.STRIDE = STRIDE\n",
    "        self.PADDING = PADDING\n",
    "        self.KERNEL_SIZE = KERNEL_SIZE\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = self.IN_CHANNELS, out_channels = self.FILTERS_CNN_1,\n",
    "                               kernel_size = self.KERNEL_SIZE, stride = self.STRIDE, padding = self.PADDING)\n",
    "        self.conv2 = nn.Conv2d(in_channels = self.FILTERS_CNN_1, out_channels = self.FILTERS_CNN_2,\n",
    "                               kernel_size = self.KERNEL_SIZE, stride = self.STRIDE, padding = self.PADDING)\n",
    "        self.fc1 = nn.Linear(self.PATCH_HEIGHT_AFTER_POOLING * self.PATCH_WIDTH_AFTER_POOLING * self.FILTERS_CNN_2, \n",
    "                             self.NEURONS_IN_DENSE_LAYER)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, self.PATCH_HEIGHT_AFTER_POOLING*self.PATCH_WIDTH_AFTER_POOLING*self.FILTERS_CNN_2)\n",
    "        X = self.dropout(self.fc1(X))\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, encoder_seq_len, num_layers, num_directions, dropout_p):        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = encoder_seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "        self.dropout = dropout_p\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first = True, dropout = self.dropout, \n",
    "                           bidirectional = True)\n",
    "\n",
    "    def forward(self, input, hidden):        \n",
    "        output = input.view(self.batch_size, self.seq_len, self.input_size)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(self.num_layers * self.num_directions, self.batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(self.num_layers * self.num_directions, self.batch_size, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_size, hidden_size, dropout_p, batch_size, encoder_seq_len, decoder_seq_len):\n",
    "        super(BahdanauDecoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.encoder_seq_len = encoder_seq_len\n",
    "        self.decoder_seq_len = decoder_seq_len\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        #self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "\n",
    "        self.fc_hidden = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.fc_encoder = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.weight_vector = torch.FloatTensor(self.batch_size, self.hidden_size, self.decoder_seq_len)\n",
    "        self.weight = nn.Parameter(nn.init.xavier_uniform_(self.weight_vector)) #xavier initializer avoids nans\n",
    "        #self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.LSTM(self.output_size + self.hidden_size, self.hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, inputs, hidden, encoder_outputs):\n",
    "        #embedded = self.dropout(embedded)\n",
    "\n",
    "        # Calculating Alignment Scores\n",
    "        hidden_state = hidden[0].view(self.batch_size, self.decoder_seq_len, self.hidden_size)\n",
    "        x = torch.tanh(self.fc_hidden(hidden_state) + self.fc_encoder(encoder_outputs))\n",
    "\n",
    "        alignment_scores = torch.bmm(x, self.weight)\n",
    "        alignment_scores = alignment_scores.view(self.batch_size, self.decoder_seq_len, self.encoder_seq_len)\n",
    " \n",
    "        # Softmaxing alignment scores to get Attention weights\n",
    "        attn_weights = F.softmax(alignment_scores, dim = 2)\n",
    "\n",
    "        # Multiplying the Attention weights with encoder outputs to get the context vector\n",
    "        context_vector = torch.bmm(attn_weights, encoder_outputs)\n",
    "\n",
    "        # Concatenating context vector with embedded input word\n",
    "        output = torch.cat((inputs, context_vector), 2)\n",
    "        # Passing the concatenated vector as input to the LSTM cell\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        # Passing the LSTM output through a Linear layer acting as a classifier\n",
    "        output = F.log_softmax(self.classifier(output), dim = 2)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "CNN_model = ConvolutionalNetwork(IN_CHANNELS = 1, FILTERS_CNN_1 = 20, FILTERS_CNN_2 = 50, NEURONS_IN_DENSE_LAYER = 1024, \n",
    "                                 PATCH_HEIGHT = 48, PATCH_WIDTH = 10, STRIDE = 1, PADDING = 2, KERNEL_SIZE = 5, dropout_p = 0.5).to(device)\n",
    "CNN_optimizer = torch.optim.AdamW(CNN_model.parameters(), lr = 0.001)\n",
    "CNN_scheduler = torch.optim.lr_scheduler.StepLR(CNN_optimizer, step_size = 1, gamma = 0.98) # decreasing lr 2% every epoch\n",
    "\n",
    "Encoder_model = EncoderRNN(input_size = 1024, hidden_size = 256, batch_size = 128, encoder_seq_len = 92, \n",
    "                           num_layers = 2, num_directions = 2, dropout_p = 0.5).to(device)\n",
    "Encoder_optimizer = torch.optim.AdamW(Encoder_model.parameters(), lr = 0.001)\n",
    "Encoder_scheduler = torch.optim.lr_scheduler.StepLR(Encoder_optimizer, step_size = 1, gamma = 0.98)\n",
    "\n",
    "Decoder_model = BahdanauDecoder(output_size = len(decoder_dict), hidden_size = 256, dropout_p = 0, batch_size = 128,\n",
    "                               encoder_seq_len = 92, decoder_seq_len = 1).to(device)\n",
    "Decoder_optimizer = torch.optim.AdamW(Decoder_model.parameters(), lr = 0.001)\n",
    "Decoder_scheduler = torch.optim.lr_scheduler.StepLR(Decoder_optimizer, step_size = 1, gamma = 0.98)\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#writer = SummaryWriter(log_dir='/home/abarreiro/runs/Variation_DA_Jorge/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_losses = []\n",
    "    \n",
    "    random_sampling = random.sample(range(len_trn), len_trn)\n",
    "    random_sampling = sort_by_batch(random_sampling, len_trn, batch_size)\n",
    "    train_loader = data_generator(image_set = 'X_trn', target_set = 'target_trn',\n",
    "                                  random_sampling = random_sampling, mode = 'training')\n",
    "    \n",
    "    for num_batch, (images, labels) in enumerate(train_loader):         \n",
    "        num_batch += 1\n",
    "\n",
    "        encoder_hidden = Encoder_model.initHidden()\n",
    "        encoder_input = CNN_model(images)\n",
    "        encoder_outputs, encoder_hidden = Encoder_model(encoder_input, encoder_hidden)\n",
    "        encoder_outputs = encoder_outputs.view(batch_size, encoder_seq_len, enc_num_directions, enc_hidden_size)\n",
    "        encoder_outputs = encoder_outputs[:, :, 0, :] + encoder_outputs[:, :, 1, :]\n",
    "        encoder_outputs = encoder_outputs.view(batch_size, encoder_seq_len, enc_hidden_size)\n",
    "\n",
    "        hidden_state = encoder_hidden[0][-2, :, :].view(1, batch_size, enc_hidden_size) + encoder_hidden[0][-1, :, :].view(1, batch_size, enc_hidden_size)\n",
    "        cell_state = encoder_hidden[1][-2, :, :].view(1, batch_size, enc_hidden_size) + encoder_hidden[1][-1, :, :].view(1, batch_size, enc_hidden_size)\n",
    "        decoder_hidden = (hidden_state, cell_state)\n",
    "        decoder_input = get_one_hot_target_IAM(labels)\n",
    "        \n",
    "        decoder_output_total = []\n",
    "        for num_letter in range(MAX_LENGTH + 2):\n",
    "            \n",
    "            decoder_input_letter = decoder_input[:, num_letter, :].unsqueeze(1)\n",
    "            decoder_output, decoder_hidden, attn_weights = Decoder_model(decoder_input_letter, decoder_hidden, encoder_outputs)\n",
    "            decoder_output_total += [decoder_output]\n",
    "\n",
    "        decoder_output_total = torch.cat(decoder_output_total, dim = 1)\n",
    "        decoder_output = decoder_output_total[:, :-1, :] # remove END token from output\n",
    "        \n",
    "        ground_truth = torch.argmax(decoder_input, dim = 2)\n",
    "        ground_truth = ground_truth[:, 1:] # remove START token from input\n",
    "\n",
    "        decoder_output = decoder_output.reshape(-1, output_size)\n",
    "        ground_truth = ground_truth.flatten()\n",
    "        \n",
    "        loss = criterion(decoder_output, ground_truth)\n",
    "        \n",
    "        CNN_optimizer.zero_grad()\n",
    "        Encoder_optimizer.zero_grad()\n",
    "        Decoder_optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        CNN_optimizer.step()\n",
    "        Encoder_optimizer.step()\n",
    "        Decoder_optimizer.step()\n",
    "        \n",
    "        train_losses += [loss.item()]\n",
    "        \n",
    "    return np.mean(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    \n",
    "    valid_losses = []\n",
    "    \n",
    "    random_sampling_val = random.sample(range(len_val), len_val)\n",
    "    random_sampling_val = sort_by_batch(random_sampling_val, len_val, batch_size)\n",
    "    val_loader = data_generator(image_set = 'X_val', target_set = 'target_val',\n",
    "                                random_sampling = random_sampling_val, mode = 'validation')\n",
    "    \n",
    "    with torch.no_grad():       \n",
    "        for num_batch_val, (images_val, labels_val) in enumerate(val_loader):        \n",
    "            num_batch_val += 1\n",
    "            encoder_hidden_val = Encoder_model.initHidden()\n",
    "            encoder_input_val = CNN_model(images_val)\n",
    "            encoder_outputs_val, encoder_hidden_val = Encoder_model(encoder_input_val, encoder_hidden_val)\n",
    "            encoder_outputs_val = encoder_outputs_val.view(batch_size, encoder_seq_len, enc_num_directions, enc_hidden_size)\n",
    "            encoder_outputs_val = encoder_outputs_val[:, :, 0, :] + encoder_outputs_val[:, :, 1, :]\n",
    "            \n",
    "            hidden_state_val = encoder_hidden_val[0][-2, :, :].view(1, batch_size, enc_hidden_size) + encoder_hidden_val[0][-1, :, :].view(1, batch_size, enc_hidden_size)\n",
    "            cell_state_val = encoder_hidden_val[1][-2, :, :].view(1, batch_size, enc_hidden_size) + encoder_hidden_val[1][-1, :, :].view(1, batch_size, enc_hidden_size)\n",
    "            decoder_hidden_val = (hidden_state_val, cell_state_val)\n",
    "            decoder_input_val = get_one_hot_target_IAM(labels_val)\n",
    "            \n",
    "            decoder_output_total_val = []\n",
    "            for num_letter_val in range(MAX_LENGTH + 2):\n",
    "            \n",
    "                decoder_input_letter_val = decoder_input_val[:, num_letter_val, :].unsqueeze(1)\n",
    "                decoder_output_val, decoder_hidden_val, attn_weights_val = Decoder_model(decoder_input_letter_val, decoder_hidden_val, encoder_outputs_val)\n",
    "                decoder_output_total_val += [decoder_output_val]\n",
    "                \n",
    "            decoder_output_total_val = torch.cat(decoder_output_total_val, dim = 1)\n",
    "            decoder_output_val = decoder_output_total_val[:, :-1, :]\n",
    "\n",
    "            ground_truth_val = torch.argmax(decoder_input_val, dim = 2)\n",
    "            ground_truth_val = ground_truth_val[:, 1:]\n",
    "            \n",
    "            decoder_output_val = decoder_output_val.reshape(-1, output_size)\n",
    "            ground_truth_val = ground_truth_val.flatten()\n",
    "            \n",
    "            loss_val = criterion(decoder_output_val, ground_truth_val)\n",
    "            valid_losses += [loss_val.item()]\n",
    "            \n",
    "    return np.mean(valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patience():\n",
    "    \n",
    "    def __init__(self, patience):\n",
    "        self.patience = patience\n",
    "        self.current_patience = patience\n",
    "        self.min_loss_val = float('inf')\n",
    "\n",
    "    def more_patience(self,loss_val):\n",
    "        self.current_patience -= 1\n",
    "        if self.current_patience == 0:\n",
    "            return False\n",
    "\n",
    "        if loss_val < self.min_loss_val:\n",
    "            self.min_loss_val = loss_val\n",
    "            self.current_patience = patience\n",
    "            \n",
    "            model_name = f\"DA_Jorge_tesis_transf_proyectivas.pt\"\n",
    "            print(\", saved best model.\")\n",
    "            \n",
    "            torch.save({\n",
    "                'CNN_model': CNN_model.state_dict(),\n",
    "                'CNN_optimizer': CNN_optimizer.state_dict(),\n",
    "                'Encoder_model': Encoder_model.state_dict(),\n",
    "                'Encoder_optimizer': Encoder_optimizer.state_dict(),\n",
    "                'Decoder_model': Decoder_model.state_dict(),\n",
    "                'Decoder_optimizer': Decoder_optimizer.state_dict(),\n",
    "            }, \"/home/abarreiro/Variaciones_sobre_arquitectura_original/modelos/\" + model_name)\n",
    "            '''\n",
    "            torch.save(CNN_model.state_dict(), 'CNN_'+model_name)\n",
    "            torch.save(Encoder_model.state_dict(), 'Encoder_'+model_name)\n",
    "            torch.save(Decoder_model.state_dict(), 'Decoder_'+model_name)\n",
    "            '''\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train loss: 0.8555049037232119 Valid loss: 0.5985215669971401 Duration: 4.26476248105367 minutes\n",
      ", saved best model.\n",
      "Epoch: 1 Train loss: 0.5981243804176861 Valid loss: 0.5047390021510043 Duration: 7.55973786910375 minutes\n",
      ", saved best model.\n",
      "Epoch: 2 Train loss: 0.519980482956305 Valid loss: 0.42906328581147274 Duration: 10.84703921477 minutes\n",
      ", saved best model.\n",
      "Epoch: 3 Train loss: 0.4525890756737102 Valid loss: 0.3760189105898647 Duration: 14.142994046211243 minutes\n",
      ", saved best model.\n",
      "Epoch: 4 Train loss: 0.3963115008916447 Valid loss: 0.3321405356213198 Duration: 17.42906018892924 minutes\n",
      ", saved best model.\n",
      "Epoch: 5 Train loss: 0.3473249529612893 Valid loss: 0.2884119898080826 Duration: 20.724386397997538 minutes\n",
      ", saved best model.\n",
      "Epoch: 6 Train loss: 0.30326143775713 Valid loss: 0.260041207075119 Duration: 24.025274582703908 minutes\n",
      ", saved best model.\n",
      "Epoch: 7 Train loss: 0.26404727322054417 Valid loss: 0.21960326069492406 Duration: 27.33185313542684 minutes\n",
      ", saved best model.\n",
      "Epoch: 8 Train loss: 0.22637426845211397 Valid loss: 0.18899766671455512 Duration: 30.63899536927541 minutes\n",
      ", saved best model.\n",
      "Epoch: 9 Train loss: 0.18767056363151674 Valid loss: 0.1634167637360298 Duration: 33.93926017681758 minutes\n",
      ", saved best model.\n",
      "Epoch: 10 Train loss: 0.15787976761074626 Valid loss: 0.1307347236295878 Duration: 37.23704863389333 minutes\n",
      ", saved best model.\n",
      "Epoch: 11 Train loss: 0.1344659577278211 Valid loss: 0.11610234238333621 Duration: 40.53876090844472 minutes\n",
      ", saved best model.\n",
      "Epoch: 12 Train loss: 0.11754255550589791 Valid loss: 0.10666339743440434 Duration: 43.83824813365936 minutes\n",
      ", saved best model.\n",
      "Epoch: 13 Train loss: 0.10512342851828127 Valid loss: 0.10305572212752649 Duration: 47.141800379753114 minutes\n",
      ", saved best model.\n",
      "Epoch: 14 Train loss: 0.09376943073569134 Valid loss: 0.08996107422951925 Duration: 50.44098558823268 minutes\n",
      ", saved best model.\n",
      "Epoch: 15 Train loss: 0.08528815286562723 Valid loss: 0.08337003087340775 Duration: 53.73955039580663 minutes\n",
      ", saved best model.\n",
      "Epoch: 16 Train loss: 0.07837336035295922 Valid loss: 0.08074439809483996 Duration: 57.03308281501134 minutes\n",
      ", saved best model.\n",
      "Epoch: 17 Train loss: 0.07133613221028591 Valid loss: 0.0739246927580591 Duration: 60.34775415658951 minutes\n",
      ", saved best model.\n",
      "Epoch: 18 Train loss: 0.06643561868465202 Valid loss: 0.07146618755187019 Duration: 63.65574496984482 minutes\n",
      ", saved best model.\n",
      "Epoch: 19 Train loss: 0.06146260211493242 Valid loss: 0.06923723422874839 Duration: 66.95779379208882 minutes\n",
      ", saved best model.\n",
      "Epoch: 20 Train loss: 0.05813896671973448 Valid loss: 0.06962516329298585 Duration: 70.27462573448817 minutes\n",
      "Epoch: 21 Train loss: 0.05409939687719001 Valid loss: 0.06478032829650378 Duration: 73.56900225480398 minutes\n",
      ", saved best model.\n",
      "Epoch: 22 Train loss: 0.05007403918646555 Valid loss: 0.06682072333612685 Duration: 76.86506473620733 minutes\n",
      "Epoch: 23 Train loss: 0.04775312483151329 Valid loss: 0.062397054538635886 Duration: 80.17056984106699 minutes\n",
      ", saved best model.\n",
      "Epoch: 24 Train loss: 0.045311656765599936 Valid loss: 0.06577055105718516 Duration: 83.47209992806117 minutes\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "patience = 150\n",
    "\n",
    "patience_controler = Patience(patience)\n",
    "start_time = time.time()\n",
    "\n",
    "len_trn = 47926\n",
    "len_val = 7558\n",
    "len_tst = 20292\n",
    "\n",
    "encoder_seq_len = 92\n",
    "enc_num_directions = 2\n",
    "batch_size = 128\n",
    "hidden_size = 256\n",
    "enc_hidden_size = 256\n",
    "dec_hidden_size = 256\n",
    "\n",
    "\n",
    "for num_epoch in range(5000000):\n",
    "\n",
    "    train_loss = train()        \n",
    "    valid_loss = validation()\n",
    "    \n",
    "    CNN_scheduler.step()\n",
    "    Encoder_scheduler.step()\n",
    "    Decoder_scheduler.step()\n",
    "    \n",
    "    #writer.add_scalar('Loss/train', train_loss, num_epoch)\n",
    "    #writer.add_scalar('Loss/validation', valid_loss, num_epoch)\n",
    "    \n",
    "    print(f'Epoch: {num_epoch} Train loss: {train_loss} Valid loss: {valid_loss} Duration: {(time.time() - start_time)/60} minutes',)\n",
    "\n",
    "    if not patience_controler.more_patience(valid_loss):\n",
    "        print(\"Se acabó la paciencia\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activar pytorch_estoril (environment) en la terminal y ejecutar tensorboard --host 0.0.0.0 --logdir ./runs\n",
    "# Tensorboard se ejecutará en un cierto puerto y nos dará el enlace. Habrá que sustituir la IP 0.0.0.0 por la del equipo\n",
    "# en remoto en la que esté corriendo en el caso de Estoril 212.128.3.86:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_estoril",
   "language": "python",
   "name": "pytorch_estoril"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
