{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antes de empezar\n",
    "\n",
    "conda activate python3.6_cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "import cv2\n",
    "import skimage\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Ignore harmless warnings:\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "3.6.10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TITAN X (Pascal)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "print(torch.__version__)\n",
    "print(platform.python_version())\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['START', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "          'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'END', 'PAD']\n",
    "\n",
    "mapeo = {}\n",
    "\n",
    "cont = 0\n",
    "for key in letters:\n",
    "    vector = torch.zeros(1, 1, len(letters))\n",
    "    vector[0,0,cont] = 1.0\n",
    "    mapeo[key] = vector\n",
    "    cont += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapeo['START']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_gen(word, n_patches, color_channels, patch_height, patch_width, stepsize):\n",
    "    \n",
    "    image = 255 * np.ones(shape = (height, width), dtype = np.uint8)\n",
    "    image = cv2.putText(image, text = word, org = (5, 30),\n",
    "        fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.62, color = (0, 0, 0),\n",
    "        thickness = 1, lineType = cv2.LINE_AA)\n",
    "    #image = skimage.util.random_noise(image, mode='s&p')\n",
    "    image = transforms.ToPILImage()(image) # np.ndarray to PIL.Image.Image\n",
    "    patches_tensor = torch.empty(n_patches, color_channels, patch_height, patch_width)\n",
    "    patches_tensor = patches_tensor.cuda(0)\n",
    "    \n",
    "    for p in range(n_patches):\n",
    "        \n",
    "        patch = transforms.functional.crop(image, 0, 0 + p * stepsize, patch_height, patch_width) # cropping of the image into patches\n",
    "        patch = transforms.ToTensor()(patch) # torch.Tensor of the patch (normalized)\n",
    "        #patch = torch.from_numpy(patch) # conversion to pytorch tensor again\n",
    "        patch = 1. - patch # it will work better if we have white text over black background\n",
    "        patch = patch.view(1, 1, patch_height, patch_width) # CNN_model expects a 4-dimensional tensor (1 dimension for batch)\n",
    "        #patch = patch.type(torch.FloatTensor) # conversion to float\n",
    "        patch = patch.cuda(0) # set to cuda\n",
    "        patches_tensor[p, 0, :, :] = patch\n",
    "                \n",
    "    return patches_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 48\n",
    "width = 192\n",
    "patch_height = 48\n",
    "patch_width = 10\n",
    "stepsize = 2\n",
    "color_channels = 1\n",
    "batch_size=512\n",
    "MAX_LENGTH = 15\n",
    "n_patches = int((width - patch_width)/stepsize + 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_set(word_set):    \n",
    "    complete_set = []\n",
    "    \n",
    "    for word in word_set:        \n",
    "        complete_set.append((patch_gen(word, n_patches, color_channels, patch_height, patch_width, stepsize), word))\n",
    "        \n",
    "    return complete_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_target(labels, seq_len, output_size,batch_size):    \n",
    "    one_hot_target = torch.empty(batch_size, seq_len, output_size) \n",
    "\n",
    "    for j,word in enumerate(labels):\n",
    "        length = len(word)\n",
    "        one_hot_target[j, 0, :] = mapeo['START']\n",
    "\n",
    "        for k,letter in enumerate(word):\n",
    "            one_hot_target[j, k + 1, :] = mapeo[letter]\n",
    "\n",
    "        one_hot_target[j, length + 1, :] = mapeo['END']\n",
    "        one_hot_target[j, length + 2: seq_len, :] = mapeo['PAD']\n",
    "        \n",
    "    return one_hot_target        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_conversion(decoder_output, output_size):\n",
    "    \n",
    "    one_hot_output_letter = torch.zeros(1, 1, output_size).cuda(0)\n",
    "    index = torch.argmax(decoder_output, dim = 2).item()\n",
    "    one_hot_output_letter[0, 0, index] = 1.\n",
    "    \n",
    "    return one_hot_output_letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definiendo la arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTROS_EN_CNN_1 = 4\n",
    "NEURONS_IN_DENSE_LAYER = 1024\n",
    "PATCH_HEIGHT_AFTER_POOLING = patch_height//2\n",
    "PATCH_WIDTH_AFTER_POOLING = patch_width//2\n",
    "kernel_size = 1\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=FILTROS_EN_CNN_1, kernel_size=kernel_size, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(PATCH_HEIGHT_AFTER_POOLING*PATCH_WIDTH_AFTER_POOLING*FILTROS_EN_CNN_1, NEURONS_IN_DENSE_LAYER)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = F.relu((self.conv1(X)))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, PATCH_HEIGHT_AFTER_POOLING*PATCH_WIDTH_AFTER_POOLING*FILTROS_EN_CNN_1)\n",
    "        X = self.fc1(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, input, hidden, batch_size, seq_len):        \n",
    "        output = input.view(batch_size, seq_len, self.input_size)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, batch_first = True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 2)\n",
    "        # dim = 2 porque esta última dimensión es la correspondiente a output_size, que es sobre\n",
    "        # la que queremos hacer el softmax\n",
    "\n",
    "    def forward(self, input, hidden, batch_size, seq_len):        \n",
    "        output = input.view(batch_size, seq_len, self.output_size)\n",
    "        #output = F.relu(output) # la relu se metía aquí porque en el\n",
    "        #caso NLP del ejemplo de PyTorch previamente había una capa de embedding\n",
    "        #No nos hace falta porque nuestro tensor de inputs ya es one-hot\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "               torch.zeros(1, batch_size, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "encoder_input_size = 1024\n",
    "hidden_size = 256\n",
    "output_size = len(letters)\n",
    "\n",
    "CNN_model = ConvolutionalNetwork().cuda(0)\n",
    "CNN_optimizer = torch.optim.Adam(CNN_model.parameters())\n",
    "\n",
    "Encoder_model = EncoderRNN(input_size = encoder_input_size, hidden_size = hidden_size).cuda(0)\n",
    "Encoder_optimizer = optim.SGD(Encoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "Decoder_model = DecoderRNN(hidden_size = hidden_size, output_size = output_size).cuda(0)\n",
    "Decoder_optimizer = optim.SGD(Decoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2d1f19dd68>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5000\n",
    "patience = 100\n",
    "min_loss_val = 10 # huge initial value for the minimum validation loss\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(decoder_output, ground_truth):\n",
    "    loss = 0\n",
    "        \n",
    "    for j in range(batch_size):\n",
    "        loss += criterion(decoder_output[j], ground_truth[j])               \n",
    "    \n",
    "    loss = loss/batch_size\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    train_losses = []\n",
    "    \n",
    "    for num_batch in range(0, 64):\n",
    "    \n",
    "        list_random_strings = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            list_random_strings.append(''.join(random.choice(string.ascii_lowercase) for _ in range(random.randint(1,MAX_LENGTH))))\n",
    "\n",
    "        train_set = complete_set(list_random_strings)\n",
    "        train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        for b, (image, labels) in enumerate(train_loader):\n",
    "            \n",
    "            encoder_hidden = Encoder_model.initHidden(batch_size)\n",
    "\n",
    "            image_cnn = image.view(-1, color_channels, patch_height, patch_width).cuda(0)\n",
    "            encoder_input = CNN_model(image_cnn)\n",
    "            encoder_output, encoder_hidden = Encoder_model(encoder_input, encoder_hidden, batch_size = batch_size, seq_len = n_patches)\n",
    "\n",
    "            decoder_hidden = encoder_hidden\n",
    "            decoder_input = get_one_hot_target(labels=labels, batch_size = batch_size, seq_len = MAX_LENGTH + 2,output_size = output_size).cuda(0)\n",
    "            decoder_output, decoder_hidden = Decoder_model(decoder_input, decoder_hidden, batch_size = batch_size, seq_len = MAX_LENGTH + 2)\n",
    "\n",
    "            output_indices = torch.tensor(list(range(0, MAX_LENGTH + 2 -1))).cuda(0) # removing last token from the output\n",
    "            decoder_output = torch.index_select(decoder_output, dim = 1, index = output_indices)\n",
    "\n",
    "            ground_truth = torch.argmax(decoder_input, dim = 2)\n",
    "            target_indices = torch.tensor(list(range(1, MAX_LENGTH + 2))).cuda(0) # remove SOS token from the input\n",
    "            ground_truth = torch.index_select(ground_truth, dim = 1, index = target_indices)\n",
    "\n",
    "            loss = calculate_loss(decoder_output,ground_truth)\n",
    "\n",
    "            CNN_optimizer.zero_grad()\n",
    "            Encoder_optimizer.zero_grad()\n",
    "            Decoder_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            CNN_optimizer.step()\n",
    "            Encoder_optimizer.step()\n",
    "            Decoder_optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "        return np.mean(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    \n",
    "    valid_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for num_batch in range(0, 16):\n",
    "\n",
    "            list_random_strings = []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "\n",
    "                list_random_strings.append(''.join(random.choice(string.ascii_lowercase) for _ in range(random.randint(1,MAX_LENGTH))))\n",
    "\n",
    "            validation_set = complete_set(list_random_strings)\n",
    "            val_loader = DataLoader(validation_set, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "        \n",
    "            for b, (image_val, label_val) in enumerate(val_loader):        \n",
    "\n",
    "                encoder_hidden_val = Encoder_model.initHidden(batch_size = batch_size)\n",
    "                image_cnn_val = image_val.view(-1, color_channels, patch_height, patch_width).cuda(0)\n",
    "                encoder_input_val = CNN_model(image_cnn_val)\n",
    "                encoder_output_val, encoder_hidden_val = Encoder_model(encoder_input_val, encoder_hidden_val, batch_size = batch_size, seq_len = n_patches)\n",
    "\n",
    "                decoder_hidden_val = encoder_hidden_val\n",
    "                decoder_input_val = get_one_hot_target(labels=label_val, batch_size = batch_size, seq_len = MAX_LENGTH + 2,output_size = output_size).cuda(0)\n",
    "                decoder_output_val, decoder_hidden_val = Decoder_model(decoder_input_val, decoder_hidden_val,batch_size = batch_size, seq_len = MAX_LENGTH + 2)\n",
    "\n",
    "                output_indices_val = torch.tensor(list(range(0, MAX_LENGTH + 1))).cuda(0) # remove last token from the output\n",
    "                decoder_output_val = torch.index_select(decoder_output_val, dim = 1, index = output_indices_val)\n",
    "\n",
    "                ground_truth_val = torch.argmax(decoder_input_val, dim = 2)\n",
    "                target_indices_val = torch.tensor(list(range(1, MAX_LENGTH + 2))).cuda(0) # remove START token from the input\n",
    "                ground_truth_val = torch.index_select(ground_truth_val, dim = 1, index = target_indices_val)\n",
    "\n",
    "                loss = calculate_loss(decoder_output_val,ground_truth_val)\n",
    "                valid_losses.append(loss.item())\n",
    "        return np.mean(valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patience():\n",
    "    \n",
    "    def __init__(self, patience):\n",
    "        self.patience = patience\n",
    "        self.current_patience = patience\n",
    "        self.min_loss_val = float('inf')\n",
    "\n",
    "    def more_patience(self,loss_val):\n",
    "        self.current_patience -= 1\n",
    "        if self.current_patience == 0:\n",
    "            return False\n",
    "\n",
    "        if loss_val < self.min_loss_val:\n",
    "            self.min_loss_val = loss_val\n",
    "            self.current_patience = patience\n",
    "\n",
    "            model_name = f\"{height}x{width}_by{patch_width}_jump{stepsize}_batch{batch_size} NN_{FILTROS_EN_CNN_1}_{NEURONS_IN_DENSE_LAYER}_{kernel_size}_{hidden_size}_INFINITE_RANDOM_SAMPLE\"\n",
    "            print(\", saved best model.\")\n",
    "            \n",
    "            torch.save(CNN_model.state_dict(), 'CNN_'+model_name)\n",
    "            torch.save(Encoder_model.state_dict(), 'Encoder_'+model_name)\n",
    "            torch.save(Decoder_model.state_dict(), 'Decoder_'+model_name)\n",
    "    \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train loss: 3.376540184020996 Valid loss: 3.3753931671380997 Duration: 1.8478599707285563 minutes\n",
      ", saved best model.\n",
      "Epoch: 1 Train loss: 3.375528335571289 Valid loss: 3.374031573534012 Duration: 3.6963781078656512 minutes\n",
      ", saved best model.\n",
      "Epoch: 2 Train loss: 3.373711585998535 Valid loss: 3.3726245164871216 Duration: 5.540303285916647 minutes\n",
      ", saved best model.\n",
      "Epoch: 3 Train loss: 3.37314772605896 Valid loss: 3.3715538680553436 Duration: 7.390025305747986 minutes\n",
      ", saved best model.\n",
      "Epoch: 4 Train loss: 3.3716237545013428 Valid loss: 3.370556116104126 Duration: 9.23621216615041 minutes\n",
      ", saved best model.\n",
      "Epoch: 5 Train loss: 3.3712573051452637 Valid loss: 3.3695359230041504 Duration: 11.081617279847462 minutes\n",
      ", saved best model.\n",
      "Epoch: 6 Train loss: 3.369041919708252 Valid loss: 3.368569076061249 Duration: 12.924023954073588 minutes\n",
      ", saved best model.\n",
      "Epoch: 7 Train loss: 3.3684046268463135 Valid loss: 3.3674750477075577 Duration: 14.769381566842396 minutes\n",
      ", saved best model.\n",
      "Epoch: 8 Train loss: 3.3675007820129395 Valid loss: 3.3665535002946854 Duration: 16.612138934930165 minutes\n",
      ", saved best model.\n",
      "Epoch: 9 Train loss: 3.365861415863037 Valid loss: 3.3651305437088013 Duration: 18.456882997353873 minutes\n",
      ", saved best model.\n",
      "Epoch: 10 Train loss: 3.365473508834839 Valid loss: 3.364535018801689 Duration: 20.300858891010286 minutes\n",
      ", saved best model.\n",
      "Epoch: 11 Train loss: 3.364992380142212 Valid loss: 3.3629697263240814 Duration: 22.145511901378633 minutes\n",
      ", saved best model.\n",
      "Epoch: 12 Train loss: 3.3635315895080566 Valid loss: 3.362081378698349 Duration: 23.995430914560952 minutes\n",
      ", saved best model.\n",
      "Epoch: 13 Train loss: 3.3616843223571777 Valid loss: 3.360432356595993 Duration: 25.847035348415375 minutes\n",
      ", saved best model.\n",
      "Epoch: 14 Train loss: 3.3611855506896973 Valid loss: 3.3585292249917984 Duration: 27.697253147761028 minutes\n",
      ", saved best model.\n",
      "Epoch: 15 Train loss: 3.358236074447632 Valid loss: 3.357171893119812 Duration: 29.54827412366867 minutes\n",
      ", saved best model.\n",
      "Epoch: 16 Train loss: 3.358255386352539 Valid loss: 3.3547996133565903 Duration: 31.39708865483602 minutes\n",
      ", saved best model.\n",
      "Epoch: 17 Train loss: 3.3548178672790527 Valid loss: 3.353352412581444 Duration: 33.24743424256643 minutes\n",
      ", saved best model.\n",
      "Epoch: 18 Train loss: 3.352973461151123 Valid loss: 3.3518521040678024 Duration: 35.0996453166008 minutes\n",
      ", saved best model.\n",
      "Epoch: 19 Train loss: 3.351839303970337 Valid loss: 3.350231170654297 Duration: 36.95185258388519 minutes\n",
      ", saved best model.\n",
      "Epoch: 20 Train loss: 3.349083423614502 Valid loss: 3.3483076840639114 Duration: 38.803393840789795 minutes\n",
      ", saved best model.\n",
      "Epoch: 21 Train loss: 3.348219633102417 Valid loss: 3.3468460738658905 Duration: 40.655469699700674 minutes\n",
      ", saved best model.\n",
      "Epoch: 22 Train loss: 3.3464815616607666 Valid loss: 3.3456045240163803 Duration: 42.507199080785114 minutes\n",
      ", saved best model.\n",
      "Epoch: 23 Train loss: 3.346964120864868 Valid loss: 3.344242200255394 Duration: 44.357811359564465 minutes\n",
      ", saved best model.\n",
      "Epoch: 24 Train loss: 3.344472885131836 Valid loss: 3.3417716175317764 Duration: 46.210024805863696 minutes\n",
      ", saved best model.\n",
      "Epoch: 25 Train loss: 3.3412206172943115 Valid loss: 3.340475618839264 Duration: 48.06126159826915 minutes\n",
      ", saved best model.\n",
      "Epoch: 26 Train loss: 3.3410706520080566 Valid loss: 3.3372976183891296 Duration: 49.91259753704071 minutes\n",
      ", saved best model.\n",
      "Epoch: 27 Train loss: 3.3368988037109375 Valid loss: 3.3365132063627243 Duration: 51.76246777375539 minutes\n",
      ", saved best model.\n",
      "Epoch: 28 Train loss: 3.335451126098633 Valid loss: 3.333574950695038 Duration: 53.61410215695699 minutes\n",
      ", saved best model.\n",
      "Epoch: 29 Train loss: 3.333798885345459 Valid loss: 3.331270322203636 Duration: 55.464573657512666 minutes\n",
      ", saved best model.\n",
      "Epoch: 30 Train loss: 3.331367254257202 Valid loss: 3.3304641246795654 Duration: 57.314724127451576 minutes\n",
      ", saved best model.\n",
      "Epoch: 31 Train loss: 3.3303027153015137 Valid loss: 3.326943889260292 Duration: 59.16545263926188 minutes\n",
      ", saved best model.\n",
      "Epoch: 32 Train loss: 3.3267884254455566 Valid loss: 3.3239850252866745 Duration: 61.014916344483694 minutes\n",
      ", saved best model.\n",
      "Epoch: 33 Train loss: 3.324674129486084 Valid loss: 3.321953609585762 Duration: 62.86571133136749 minutes\n",
      ", saved best model.\n",
      "Epoch: 34 Train loss: 3.320157051086426 Valid loss: 3.31938536465168 Duration: 64.71705178817113 minutes\n",
      ", saved best model.\n",
      "Epoch: 35 Train loss: 3.318603277206421 Valid loss: 3.3168540596961975 Duration: 66.56770809491475 minutes\n",
      ", saved best model.\n",
      "Epoch: 36 Train loss: 3.3155577182769775 Valid loss: 3.313661277294159 Duration: 68.41869800885519 minutes\n",
      ", saved best model.\n",
      "Epoch: 37 Train loss: 3.308701276779175 Valid loss: 3.3121621310710907 Duration: 70.26863023440043 minutes\n",
      ", saved best model.\n",
      "Epoch: 38 Train loss: 3.309048891067505 Valid loss: 3.308849558234215 Duration: 72.11744375626246 minutes\n",
      ", saved best model.\n",
      "Epoch: 39 Train loss: 3.3101656436920166 Valid loss: 3.307040050625801 Duration: 73.96913267374039 minutes\n",
      ", saved best model.\n",
      "Epoch: 40 Train loss: 3.30733585357666 Valid loss: 3.3066759258508682 Duration: 75.82250771522521 minutes\n",
      ", saved best model.\n",
      "Epoch: 41 Train loss: 3.3098156452178955 Valid loss: 3.3022963851690292 Duration: 77.67308997313181 minutes\n",
      ", saved best model.\n",
      "Epoch: 42 Train loss: 3.3020060062408447 Valid loss: 3.30192768573761 Duration: 79.5245844523112 minutes\n",
      ", saved best model.\n",
      "Epoch: 43 Train loss: 3.303835153579712 Valid loss: 3.29919970035553 Duration: 81.37609357436499 minutes\n",
      ", saved best model.\n",
      "Epoch: 44 Train loss: 3.3004395961761475 Valid loss: 3.296286255121231 Duration: 83.22803678115208 minutes\n",
      ", saved best model.\n",
      "Epoch: 45 Train loss: 3.2980856895446777 Valid loss: 3.2921778708696365 Duration: 85.08265326023101 minutes\n",
      ", saved best model.\n",
      "Epoch: 46 Train loss: 3.2949252128601074 Valid loss: 3.294072851538658 Duration: 86.93618823687235 minutes\n",
      "Epoch: 47 Train loss: 3.295779228210449 Valid loss: 3.291822239756584 Duration: 88.78620928128561 minutes\n",
      ", saved best model.\n",
      "Epoch: 48 Train loss: 3.2818713188171387 Valid loss: 3.2884842455387115 Duration: 90.63617457946141 minutes\n",
      ", saved best model.\n",
      "Epoch: 49 Train loss: 3.294738531112671 Valid loss: 3.2861403971910477 Duration: 92.48913724819819 minutes\n",
      ", saved best model.\n",
      "Epoch: 50 Train loss: 3.28820538520813 Valid loss: 3.283951371908188 Duration: 94.34066064357758 minutes\n",
      ", saved best model.\n",
      "Epoch: 51 Train loss: 3.2816269397735596 Valid loss: 3.2841774076223373 Duration: 96.19152737061182 minutes\n",
      "Epoch: 52 Train loss: 3.289813756942749 Valid loss: 3.281229615211487 Duration: 98.04055146773656 minutes\n",
      ", saved best model.\n",
      "Epoch: 53 Train loss: 3.2895007133483887 Valid loss: 3.278372496366501 Duration: 99.89119486808777 minutes\n",
      ", saved best model.\n",
      "Epoch: 54 Train loss: 3.2821919918060303 Valid loss: 3.277898222208023 Duration: 101.74315579732259 minutes\n",
      ", saved best model.\n",
      "Epoch: 55 Train loss: 3.2736926078796387 Valid loss: 3.2778248339891434 Duration: 103.59437296787898 minutes\n",
      ", saved best model.\n",
      "Epoch: 56 Train loss: 3.2674241065979004 Valid loss: 3.2739937603473663 Duration: 105.44526064793268 minutes\n",
      ", saved best model.\n",
      "Epoch: 57 Train loss: 3.269949436187744 Valid loss: 3.2733021080493927 Duration: 107.29690234263738 minutes\n",
      ", saved best model.\n",
      "Epoch: 58 Train loss: 3.274315357208252 Valid loss: 3.2702810168266296 Duration: 109.15037554105123 minutes\n",
      ", saved best model.\n",
      "Epoch: 59 Train loss: 3.2707626819610596 Valid loss: 3.2682088762521744 Duration: 111.00127557118734 minutes\n",
      ", saved best model.\n",
      "Epoch: 60 Train loss: 3.265793800354004 Valid loss: 3.267490953207016 Duration: 112.85624098380407 minutes\n",
      ", saved best model.\n",
      "Epoch: 61 Train loss: 3.2672576904296875 Valid loss: 3.2642393857240677 Duration: 114.70471319357554 minutes\n",
      ", saved best model.\n",
      "Epoch: 62 Train loss: 3.265615701675415 Valid loss: 3.2644432336091995 Duration: 116.55517757733664 minutes\n",
      "Epoch: 63 Train loss: 3.2575106620788574 Valid loss: 3.264330565929413 Duration: 118.4050411661466 minutes\n",
      "Epoch: 64 Train loss: 3.2622458934783936 Valid loss: 3.261928051710129 Duration: 120.25034100612005 minutes\n",
      ", saved best model.\n",
      "Epoch: 65 Train loss: 3.261601209640503 Valid loss: 3.2622323036193848 Duration: 122.09547619024913 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 Train loss: 3.2614071369171143 Valid loss: 3.2560299932956696 Duration: 123.94006844758988 minutes\n",
      ", saved best model.\n",
      "Epoch: 67 Train loss: 3.2583107948303223 Valid loss: 3.2573185116052628 Duration: 125.78571435610453 minutes\n",
      "Epoch: 68 Train loss: 3.256436824798584 Valid loss: 3.254881963133812 Duration: 127.63258495330811 minutes\n",
      ", saved best model.\n",
      "Epoch: 69 Train loss: 3.2501394748687744 Valid loss: 3.2534548342227936 Duration: 129.4792286435763 minutes\n",
      ", saved best model.\n",
      "Epoch: 70 Train loss: 3.258434772491455 Valid loss: 3.2517505884170532 Duration: 131.32735763788224 minutes\n",
      ", saved best model.\n",
      "Epoch: 71 Train loss: 3.2547192573547363 Valid loss: 3.251824975013733 Duration: 133.17536105712256 minutes\n",
      "Epoch: 72 Train loss: 3.2496731281280518 Valid loss: 3.2492272555828094 Duration: 135.02197453975677 minutes\n",
      ", saved best model.\n",
      "Epoch: 73 Train loss: 3.253000020980835 Valid loss: 3.250415727496147 Duration: 136.8662702004115 minutes\n",
      "Epoch: 74 Train loss: 3.233665704727173 Valid loss: 3.2464836835861206 Duration: 138.71210193634033 minutes\n",
      ", saved best model.\n",
      "Epoch: 75 Train loss: 3.2444241046905518 Valid loss: 3.2454414069652557 Duration: 140.555944331487 minutes\n",
      ", saved best model.\n",
      "Epoch: 76 Train loss: 3.24176287651062 Valid loss: 3.2429239004850388 Duration: 142.39962017933527 minutes\n",
      ", saved best model.\n",
      "Epoch: 77 Train loss: 3.2464711666107178 Valid loss: 3.244896277785301 Duration: 144.24375615517297 minutes\n",
      "Epoch: 78 Train loss: 3.2466561794281006 Valid loss: 3.241507798433304 Duration: 146.08915034135183 minutes\n",
      ", saved best model.\n",
      "Epoch: 79 Train loss: 3.2452738285064697 Valid loss: 3.2408342510461807 Duration: 147.9365441640218 minutes\n",
      ", saved best model.\n",
      "Epoch: 80 Train loss: 3.244809865951538 Valid loss: 3.236842840909958 Duration: 149.78135441939037 minutes\n",
      ", saved best model.\n",
      "Epoch: 81 Train loss: 3.238452911376953 Valid loss: 3.2404438108205795 Duration: 151.62721435626347 minutes\n",
      "Epoch: 82 Train loss: 3.2378857135772705 Valid loss: 3.2366726249456406 Duration: 153.4713354229927 minutes\n",
      ", saved best model.\n",
      "Epoch: 83 Train loss: 3.2539780139923096 Valid loss: 3.236600413918495 Duration: 155.3189672589302 minutes\n",
      ", saved best model.\n",
      "Epoch: 84 Train loss: 3.236027240753174 Valid loss: 3.2339862287044525 Duration: 157.16429948012035 minutes\n",
      ", saved best model.\n",
      "Epoch: 85 Train loss: 3.2259321212768555 Valid loss: 3.2327266186475754 Duration: 159.01152187188467 minutes\n",
      ", saved best model.\n",
      "Epoch: 86 Train loss: 3.234492540359497 Valid loss: 3.23333340883255 Duration: 160.8611313978831 minutes\n",
      "Epoch: 87 Train loss: 3.2331383228302 Valid loss: 3.230442464351654 Duration: 162.70824096997578 minutes\n",
      ", saved best model.\n",
      "Epoch: 88 Train loss: 3.2220566272735596 Valid loss: 3.2299597412347794 Duration: 164.55187569856645 minutes\n",
      ", saved best model.\n",
      "Epoch: 89 Train loss: 3.233182430267334 Valid loss: 3.2272107303142548 Duration: 166.395665204525 minutes\n",
      ", saved best model.\n",
      "Epoch: 90 Train loss: 3.233069658279419 Valid loss: 3.2237138748168945 Duration: 168.23976337909698 minutes\n",
      ", saved best model.\n",
      "Epoch: 91 Train loss: 3.231388807296753 Valid loss: 3.2305100858211517 Duration: 170.08502955436705 minutes\n",
      "Epoch: 92 Train loss: 3.222830057144165 Valid loss: 3.2324596494436264 Duration: 171.93111230929694 minutes\n",
      "Epoch: 93 Train loss: 3.2306926250457764 Valid loss: 3.230024129152298 Duration: 173.77845431566237 minutes\n",
      "Epoch: 94 Train loss: 3.2188000679016113 Valid loss: 3.222431257367134 Duration: 175.62600688934327 minutes\n",
      ", saved best model.\n",
      "Epoch: 95 Train loss: 3.215737819671631 Valid loss: 3.2224236875772476 Duration: 177.47322372198104 minutes\n",
      ", saved best model.\n",
      "Epoch: 96 Train loss: 3.225080728530884 Valid loss: 3.2252388149499893 Duration: 179.32018662691115 minutes\n",
      "Epoch: 97 Train loss: 3.218529462814331 Valid loss: 3.2225293815135956 Duration: 181.16780043840407 minutes\n",
      "Epoch: 98 Train loss: 3.218996047973633 Valid loss: 3.2237739711999893 Duration: 183.01571154991785 minutes\n",
      "Epoch: 99 Train loss: 3.2197794914245605 Valid loss: 3.217030256986618 Duration: 184.86202718019484 minutes\n",
      ", saved best model.\n",
      "Epoch: 100 Train loss: 3.2268991470336914 Valid loss: 3.221398204565048 Duration: 186.70867182413738 minutes\n",
      "Epoch: 101 Train loss: 3.214798927307129 Valid loss: 3.2215040177106857 Duration: 188.55524257421493 minutes\n",
      "Epoch: 102 Train loss: 3.228837251663208 Valid loss: 3.2151452898979187 Duration: 190.40287640492122 minutes\n",
      ", saved best model.\n",
      "Epoch: 103 Train loss: 3.221494197845459 Valid loss: 3.2148633152246475 Duration: 192.25125156641008 minutes\n",
      ", saved best model.\n",
      "Epoch: 104 Train loss: 3.219489812850952 Valid loss: 3.214835211634636 Duration: 194.1003652215004 minutes\n",
      ", saved best model.\n",
      "Epoch: 105 Train loss: 3.2028262615203857 Valid loss: 3.2139024287462234 Duration: 195.94646691878637 minutes\n",
      ", saved best model.\n",
      "Epoch: 106 Train loss: 3.220471143722534 Valid loss: 3.216172471642494 Duration: 197.79451628923417 minutes\n",
      "Epoch: 107 Train loss: 3.2276182174682617 Valid loss: 3.21108141541481 Duration: 199.64190668265024 minutes\n",
      ", saved best model.\n",
      "Epoch: 108 Train loss: 3.2267069816589355 Valid loss: 3.2113945931196213 Duration: 201.48879072666168 minutes\n",
      "Epoch: 109 Train loss: 3.2203919887542725 Valid loss: 3.2123216688632965 Duration: 203.33623222112655 minutes\n",
      "Epoch: 110 Train loss: 3.2067275047302246 Valid loss: 3.213706314563751 Duration: 205.18114445209503 minutes\n",
      "Epoch: 111 Train loss: 3.204700231552124 Valid loss: 3.2093627899885178 Duration: 207.02777303059895 minutes\n",
      ", saved best model.\n",
      "Epoch: 112 Train loss: 3.2057607173919678 Valid loss: 3.207606539130211 Duration: 208.87526462078094 minutes\n",
      ", saved best model.\n",
      "Epoch: 113 Train loss: 3.210597038269043 Valid loss: 3.2113634049892426 Duration: 210.72090714772543 minutes\n",
      "Epoch: 114 Train loss: 3.2204232215881348 Valid loss: 3.2109336853027344 Duration: 212.56587046782175 minutes\n",
      "Epoch: 115 Train loss: 3.2024645805358887 Valid loss: 3.2099051028490067 Duration: 214.41318140824634 minutes\n",
      "Epoch: 116 Train loss: 3.208604574203491 Valid loss: 3.207357257604599 Duration: 216.25965631405512 minutes\n",
      ", saved best model.\n",
      "Epoch: 117 Train loss: 3.203989028930664 Valid loss: 3.2094829082489014 Duration: 218.10590705474218 minutes\n",
      "Epoch: 118 Train loss: 3.197427749633789 Valid loss: 3.2055660635232925 Duration: 219.9510246872902 minutes\n",
      ", saved best model.\n",
      "Epoch: 119 Train loss: 3.214580774307251 Valid loss: 3.2054274529218674 Duration: 221.80019126733143 minutes\n",
      ", saved best model.\n",
      "Epoch: 120 Train loss: 3.206242561340332 Valid loss: 3.20688496530056 Duration: 223.64624143044153 minutes\n",
      "Epoch: 121 Train loss: 3.1940598487854004 Valid loss: 3.2053784877061844 Duration: 225.4936860481898 minutes\n",
      ", saved best model.\n",
      "Epoch: 122 Train loss: 3.2180402278900146 Valid loss: 3.204715833067894 Duration: 227.34074098666508 minutes\n",
      ", saved best model.\n",
      "Epoch: 123 Train loss: 3.2034740447998047 Valid loss: 3.207648351788521 Duration: 229.1856371720632 minutes\n",
      "Epoch: 124 Train loss: 3.2121593952178955 Valid loss: 3.202079936861992 Duration: 231.0302478313446 minutes\n",
      ", saved best model.\n",
      "Epoch: 125 Train loss: 3.2054953575134277 Valid loss: 3.201925054192543 Duration: 232.87613940636317 minutes\n",
      ", saved best model.\n",
      "Epoch: 126 Train loss: 3.1839640140533447 Valid loss: 3.1998010873794556 Duration: 234.72195817629498 minutes\n",
      ", saved best model.\n",
      "Epoch: 127 Train loss: 3.2018580436706543 Valid loss: 3.1981932520866394 Duration: 236.57208077112833 minutes\n",
      ", saved best model.\n",
      "Epoch: 128 Train loss: 3.220689535140991 Valid loss: 3.2013847678899765 Duration: 238.42091861963272 minutes\n",
      "Epoch: 129 Train loss: 3.205146312713623 Valid loss: 3.195388525724411 Duration: 240.27091540495556 minutes\n",
      ", saved best model.\n",
      "Epoch: 130 Train loss: 3.204911231994629 Valid loss: 3.2009191662073135 Duration: 242.12165062824886 minutes\n",
      "Epoch: 131 Train loss: 3.186410903930664 Valid loss: 3.19824655354023 Duration: 243.97540273269018 minutes\n",
      "Epoch: 132 Train loss: 3.189913034439087 Valid loss: 3.1980163753032684 Duration: 245.82567178408306 minutes\n",
      "Epoch: 133 Train loss: 3.194840669631958 Valid loss: 3.1927679628133774 Duration: 247.67385028998058 minutes\n",
      ", saved best model.\n",
      "Epoch: 134 Train loss: 3.20133638381958 Valid loss: 3.193745255470276 Duration: 249.52067120869955 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 135 Train loss: 3.189509153366089 Valid loss: 3.1994022876024246 Duration: 251.36976888577144 minutes\n",
      "Epoch: 136 Train loss: 3.182701349258423 Valid loss: 3.1975756734609604 Duration: 253.21848467985788 minutes\n",
      "Epoch: 137 Train loss: 3.1799941062927246 Valid loss: 3.195961594581604 Duration: 255.06802299817403 minutes\n",
      "Epoch: 138 Train loss: 3.220182180404663 Valid loss: 3.191705137491226 Duration: 256.915893137455 minutes\n",
      ", saved best model.\n",
      "Epoch: 139 Train loss: 3.2061257362365723 Valid loss: 3.1920445263385773 Duration: 258.7643669644992 minutes\n",
      "Epoch: 140 Train loss: 3.1863276958465576 Valid loss: 3.1923710107803345 Duration: 260.6100320617358 minutes\n",
      "Epoch: 141 Train loss: 3.1696221828460693 Valid loss: 3.1847199201583862 Duration: 262.45689551830293 minutes\n",
      ", saved best model.\n",
      "Epoch: 142 Train loss: 3.1983001232147217 Valid loss: 3.1936338245868683 Duration: 264.30527269442877 minutes\n",
      "Epoch: 143 Train loss: 3.193655252456665 Valid loss: 3.1920854598283768 Duration: 266.1527902285258 minutes\n",
      "Epoch: 144 Train loss: 3.189366102218628 Valid loss: 3.189332351088524 Duration: 268.00278647343316 minutes\n",
      "Epoch: 145 Train loss: 3.189544439315796 Valid loss: 3.189122349023819 Duration: 269.85114579995474 minutes\n",
      "Epoch: 146 Train loss: 3.1705305576324463 Valid loss: 3.1875950694084167 Duration: 271.69527955849964 minutes\n",
      "Epoch: 147 Train loss: 3.1670942306518555 Valid loss: 3.187999188899994 Duration: 273.5413713653882 minutes\n",
      "Epoch: 148 Train loss: 3.201631546020508 Valid loss: 3.1862533539533615 Duration: 275.385998527209 minutes\n",
      "Epoch: 149 Train loss: 3.183619260787964 Valid loss: 3.183597654104233 Duration: 277.23351191282273 minutes\n",
      ", saved best model.\n",
      "Epoch: 150 Train loss: 3.1810503005981445 Valid loss: 3.1847445517778397 Duration: 279.07930236260097 minutes\n",
      "Epoch: 151 Train loss: 3.176985025405884 Valid loss: 3.1843255162239075 Duration: 280.924543317159 minutes\n",
      "Epoch: 152 Train loss: 3.1772212982177734 Valid loss: 3.177170544862747 Duration: 282.7683876991272 minutes\n",
      ", saved best model.\n",
      "Epoch: 153 Train loss: 3.1629269123077393 Valid loss: 3.1799842417240143 Duration: 284.61582825581235 minutes\n",
      "Epoch: 154 Train loss: 3.177349328994751 Valid loss: 3.179198980331421 Duration: 286.4618403593699 minutes\n",
      "Epoch: 155 Train loss: 3.1699466705322266 Valid loss: 3.1789747029542923 Duration: 288.3110388000806 minutes\n",
      "Epoch: 156 Train loss: 3.171851873397827 Valid loss: 3.1775220930576324 Duration: 290.160732126236 minutes\n",
      "Epoch: 157 Train loss: 3.1691999435424805 Valid loss: 3.1761946380138397 Duration: 292.00918848514556 minutes\n",
      ", saved best model.\n",
      "Epoch: 158 Train loss: 3.17584228515625 Valid loss: 3.1772452145814896 Duration: 293.85869561433793 minutes\n",
      "Epoch: 159 Train loss: 3.177804708480835 Valid loss: 3.175089105963707 Duration: 295.70677063067757 minutes\n",
      ", saved best model.\n",
      "Epoch: 160 Train loss: 3.1737422943115234 Valid loss: 3.1701312363147736 Duration: 297.55322449207307 minutes\n",
      ", saved best model.\n",
      "Epoch: 161 Train loss: 3.172325372695923 Valid loss: 3.1731809228658676 Duration: 299.40283675193785 minutes\n",
      "Epoch: 162 Train loss: 3.1701314449310303 Valid loss: 3.1705472022295 Duration: 301.2496279239655 minutes\n",
      "Epoch: 163 Train loss: 3.1843655109405518 Valid loss: 3.1700210869312286 Duration: 303.09682496786115 minutes\n",
      ", saved best model.\n",
      "Epoch: 164 Train loss: 3.1733949184417725 Valid loss: 3.172599583864212 Duration: 304.9428480267525 minutes\n",
      "Epoch: 165 Train loss: 3.175555467605591 Valid loss: 3.170459523797035 Duration: 306.7914535800616 minutes\n",
      "Epoch: 166 Train loss: 3.1717753410339355 Valid loss: 3.171050861477852 Duration: 308.6375985066096 minutes\n",
      "Epoch: 167 Train loss: 3.163816452026367 Valid loss: 3.1678752303123474 Duration: 310.4865504264832 minutes\n",
      ", saved best model.\n",
      "Epoch: 168 Train loss: 3.1755542755126953 Valid loss: 3.172697424888611 Duration: 312.3387277285258 minutes\n",
      "Epoch: 169 Train loss: 3.1653976440429688 Valid loss: 3.167332962155342 Duration: 314.18916701873144 minutes\n",
      ", saved best model.\n",
      "Epoch: 170 Train loss: 3.151482105255127 Valid loss: 3.1689267456531525 Duration: 316.03960068623223 minutes\n",
      "Epoch: 171 Train loss: 3.1784110069274902 Valid loss: 3.1663475334644318 Duration: 317.8857479214668 minutes\n",
      ", saved best model.\n",
      "Epoch: 172 Train loss: 3.1803500652313232 Valid loss: 3.1671680212020874 Duration: 319.73387555281323 minutes\n",
      "Epoch: 173 Train loss: 3.1721608638763428 Valid loss: 3.1641877740621567 Duration: 321.58171958525975 minutes\n",
      ", saved best model.\n",
      "Epoch: 174 Train loss: 3.152695417404175 Valid loss: 3.166572719812393 Duration: 323.4267023205757 minutes\n",
      "Epoch: 175 Train loss: 3.1683807373046875 Valid loss: 3.160665363073349 Duration: 325.2727072993914 minutes\n",
      ", saved best model.\n",
      "Epoch: 176 Train loss: 3.165382146835327 Valid loss: 3.1587800085544586 Duration: 327.1199422399203 minutes\n",
      ", saved best model.\n",
      "Epoch: 177 Train loss: 3.146458625793457 Valid loss: 3.166982814669609 Duration: 328.96788190205893 minutes\n",
      "Epoch: 178 Train loss: 3.1784746646881104 Valid loss: 3.160296991467476 Duration: 330.8130416750908 minutes\n",
      "Epoch: 179 Train loss: 3.16658616065979 Valid loss: 3.162552535533905 Duration: 332.65869035720823 minutes\n",
      "Epoch: 180 Train loss: 3.1546199321746826 Valid loss: 3.1624260246753693 Duration: 334.50750659306846 minutes\n",
      "Epoch: 181 Train loss: 3.162804365158081 Valid loss: 3.1605107337236404 Duration: 336.3552685578664 minutes\n",
      "Epoch: 182 Train loss: 3.1669435501098633 Valid loss: 3.158125102519989 Duration: 338.20034319957097 minutes\n",
      ", saved best model.\n",
      "Epoch: 183 Train loss: 3.160414457321167 Valid loss: 3.1606361716985703 Duration: 340.04884921312333 minutes\n",
      "Epoch: 184 Train loss: 3.162081718444824 Valid loss: 3.160255581140518 Duration: 341.8969454010328 minutes\n",
      "Epoch: 185 Train loss: 3.1646814346313477 Valid loss: 3.156787723302841 Duration: 343.74495495557784 minutes\n",
      ", saved best model.\n",
      "Epoch: 186 Train loss: 3.1572344303131104 Valid loss: 3.1568954586982727 Duration: 345.5925717035929 minutes\n",
      "Epoch: 187 Train loss: 3.146360158920288 Valid loss: 3.161862537264824 Duration: 347.44023226102195 minutes\n",
      "Epoch: 188 Train loss: 3.1634156703948975 Valid loss: 3.1542651504278183 Duration: 349.28742545048397 minutes\n",
      ", saved best model.\n",
      "Epoch: 189 Train loss: 3.1585566997528076 Valid loss: 3.1620518416166306 Duration: 351.13182840744656 minutes\n",
      "Epoch: 190 Train loss: 3.165602684020996 Valid loss: 3.1569295078516006 Duration: 352.97315559784573 minutes\n",
      "Epoch: 191 Train loss: 3.1685304641723633 Valid loss: 3.1555329710245132 Duration: 354.8182339310646 minutes\n",
      "Epoch: 192 Train loss: 3.145143508911133 Valid loss: 3.1520864069461823 Duration: 356.66246387561165 minutes\n",
      ", saved best model.\n",
      "Epoch: 193 Train loss: 3.160240888595581 Valid loss: 3.156122863292694 Duration: 358.50906987190245 minutes\n",
      "Epoch: 194 Train loss: 3.139442205429077 Valid loss: 3.1564823985099792 Duration: 360.3546109199524 minutes\n",
      "Epoch: 195 Train loss: 3.1428534984588623 Valid loss: 3.1555745005607605 Duration: 362.198588347435 minutes\n",
      "Epoch: 196 Train loss: 3.163548231124878 Valid loss: 3.1536058336496353 Duration: 364.0425086975098 minutes\n",
      "Epoch: 197 Train loss: 3.177248477935791 Valid loss: 3.151514157652855 Duration: 365.888849679629 minutes\n",
      ", saved best model.\n",
      "Epoch: 198 Train loss: 3.1471469402313232 Valid loss: 3.14766626060009 Duration: 367.73366549412407 minutes\n",
      ", saved best model.\n",
      "Epoch: 199 Train loss: 3.1400654315948486 Valid loss: 3.1502689868211746 Duration: 369.5789572676023 minutes\n",
      "Epoch: 200 Train loss: 3.157376289367676 Valid loss: 3.151305004954338 Duration: 371.4256043752035 minutes\n",
      "Epoch: 201 Train loss: 3.1502649784088135 Valid loss: 3.1577101200819016 Duration: 373.27167792717614 minutes\n",
      "Epoch: 202 Train loss: 3.141451120376587 Valid loss: 3.149045541882515 Duration: 375.1185685237249 minutes\n",
      "Epoch: 203 Train loss: 3.134361505508423 Valid loss: 3.1546608358621597 Duration: 376.9644578655561 minutes\n",
      "Epoch: 204 Train loss: 3.141641139984131 Valid loss: 3.1472123116254807 Duration: 378.81060728232063 minutes\n",
      ", saved best model.\n",
      "Epoch: 205 Train loss: 3.135462522506714 Valid loss: 3.1498163789510727 Duration: 380.6594924648603 minutes\n",
      "Epoch: 206 Train loss: 3.1447062492370605 Valid loss: 3.152026027441025 Duration: 382.50500643253326 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 207 Train loss: 3.13274884223938 Valid loss: 3.149019256234169 Duration: 384.3481112599373 minutes\n",
      "Epoch: 208 Train loss: 3.1421282291412354 Valid loss: 3.1489887684583664 Duration: 386.1926651159922 minutes\n",
      "Epoch: 209 Train loss: 3.157188892364502 Valid loss: 3.1452592462301254 Duration: 388.037515215079 minutes\n",
      ", saved best model.\n",
      "Epoch: 210 Train loss: 3.1479272842407227 Valid loss: 3.144139811396599 Duration: 389.8833063840866 minutes\n",
      ", saved best model.\n",
      "Epoch: 211 Train loss: 3.1527867317199707 Valid loss: 3.139383226633072 Duration: 391.73087379932406 minutes\n",
      ", saved best model.\n",
      "Epoch: 212 Train loss: 3.136479377746582 Valid loss: 3.1453002393245697 Duration: 393.5755486885707 minutes\n",
      "Epoch: 213 Train loss: 3.1396279335021973 Valid loss: 3.14705428481102 Duration: 395.42091976801555 minutes\n",
      "Epoch: 214 Train loss: 3.152351140975952 Valid loss: 3.147614896297455 Duration: 397.26717706918714 minutes\n",
      "Epoch: 215 Train loss: 3.155599355697632 Valid loss: 3.1433197259902954 Duration: 399.1155768434207 minutes\n",
      "Epoch: 216 Train loss: 3.147789478302002 Valid loss: 3.1410482823848724 Duration: 400.9617247144381 minutes\n",
      "Epoch: 217 Train loss: 3.1166703701019287 Valid loss: 3.140998899936676 Duration: 402.8097273508708 minutes\n",
      "Epoch: 218 Train loss: 3.126570701599121 Valid loss: 3.140225112438202 Duration: 404.6574650565783 minutes\n",
      "Epoch: 219 Train loss: 3.151318311691284 Valid loss: 3.1444890946149826 Duration: 406.5056123693784 minutes\n",
      "Epoch: 220 Train loss: 3.1296908855438232 Valid loss: 3.138428181409836 Duration: 408.3535838007927 minutes\n",
      ", saved best model.\n",
      "Epoch: 221 Train loss: 3.139177083969116 Valid loss: 3.134855628013611 Duration: 410.19758135875065 minutes\n",
      ", saved best model.\n",
      "Epoch: 222 Train loss: 3.1258704662323 Valid loss: 3.137369528412819 Duration: 412.045184079806 minutes\n",
      "Epoch: 223 Train loss: 3.1555542945861816 Valid loss: 3.1355519741773605 Duration: 413.8927744626999 minutes\n",
      "Epoch: 224 Train loss: 3.1307997703552246 Valid loss: 3.140238046646118 Duration: 415.73731048901874 minutes\n",
      "Epoch: 225 Train loss: 3.150162935256958 Valid loss: 3.136826530098915 Duration: 417.58425397078196 minutes\n",
      "Epoch: 226 Train loss: 3.1229071617126465 Valid loss: 3.140538275241852 Duration: 419.4277529875437 minutes\n",
      "Epoch: 227 Train loss: 3.1377294063568115 Valid loss: 3.1393236964941025 Duration: 421.2711673259735 minutes\n",
      "Epoch: 228 Train loss: 3.1425669193267822 Valid loss: 3.1369031071662903 Duration: 423.11701989968617 minutes\n",
      "Epoch: 229 Train loss: 3.1429433822631836 Valid loss: 3.135963097214699 Duration: 424.96513303915657 minutes\n",
      "Epoch: 230 Train loss: 3.1267337799072266 Valid loss: 3.138849586248398 Duration: 426.81243834495547 minutes\n",
      "Epoch: 231 Train loss: 3.136068105697632 Valid loss: 3.1366765201091766 Duration: 428.6601867556572 minutes\n",
      "Epoch: 232 Train loss: 3.145737648010254 Valid loss: 3.1376603096723557 Duration: 430.50605154037476 minutes\n",
      "Epoch: 233 Train loss: 3.1354126930236816 Valid loss: 3.1338908821344376 Duration: 432.3522670348485 minutes\n",
      ", saved best model.\n",
      "Epoch: 234 Train loss: 3.130200147628784 Valid loss: 3.129025772213936 Duration: 434.196045478185 minutes\n",
      ", saved best model.\n",
      "Epoch: 235 Train loss: 3.0987865924835205 Valid loss: 3.1333316415548325 Duration: 436.04116056760154 minutes\n",
      "Epoch: 236 Train loss: 3.1109774112701416 Valid loss: 3.1290238797664642 Duration: 437.88659780025483 minutes\n",
      ", saved best model.\n",
      "Epoch: 237 Train loss: 3.124072790145874 Valid loss: 3.1316637098789215 Duration: 439.7306329846382 minutes\n",
      "Epoch: 238 Train loss: 3.1292521953582764 Valid loss: 3.1312084645032883 Duration: 441.5744256695112 minutes\n",
      "Epoch: 239 Train loss: 3.138453960418701 Valid loss: 3.1224843859672546 Duration: 443.4200210769971 minutes\n",
      ", saved best model.\n",
      "Epoch: 240 Train loss: 3.1370556354522705 Valid loss: 3.126976817846298 Duration: 445.2634643038114 minutes\n",
      "Epoch: 241 Train loss: 3.118542194366455 Valid loss: 3.1291310489177704 Duration: 447.11059109369916 minutes\n",
      "Epoch: 242 Train loss: 3.120222568511963 Valid loss: 3.1273903101682663 Duration: 448.95730353593825 minutes\n",
      "Epoch: 243 Train loss: 3.1563520431518555 Valid loss: 3.123522937297821 Duration: 450.8017262339592 minutes\n",
      "Epoch: 244 Train loss: 3.1262598037719727 Valid loss: 3.127371609210968 Duration: 452.6482589284579 minutes\n",
      "Epoch: 245 Train loss: 3.1422157287597656 Valid loss: 3.128747135400772 Duration: 454.49426996707916 minutes\n",
      "Epoch: 246 Train loss: 3.1208930015563965 Valid loss: 3.1258545964956284 Duration: 456.3397075772285 minutes\n",
      "Epoch: 247 Train loss: 3.1272244453430176 Valid loss: 3.1274950951337814 Duration: 458.1855709433556 minutes\n",
      "Epoch: 248 Train loss: 3.1244192123413086 Valid loss: 3.1272891759872437 Duration: 460.03000439008076 minutes\n",
      "Epoch: 249 Train loss: 3.112346649169922 Valid loss: 3.122674375772476 Duration: 461.8752101778984 minutes\n",
      "Epoch: 250 Train loss: 3.1169936656951904 Valid loss: 3.123673290014267 Duration: 463.7188747962316 minutes\n",
      "Epoch: 251 Train loss: 3.139892339706421 Valid loss: 3.1197234839200974 Duration: 465.56404837369917 minutes\n",
      ", saved best model.\n",
      "Epoch: 252 Train loss: 3.1125338077545166 Valid loss: 3.1300345808267593 Duration: 467.40851006507876 minutes\n",
      "Epoch: 253 Train loss: 3.1289222240448 Valid loss: 3.124857619404793 Duration: 469.2529363910357 minutes\n",
      "Epoch: 254 Train loss: 3.1354405879974365 Valid loss: 3.1254902333021164 Duration: 471.0984681248665 minutes\n",
      "Epoch: 255 Train loss: 3.120495319366455 Valid loss: 3.119373381137848 Duration: 472.9442258477211 minutes\n",
      ", saved best model.\n",
      "Epoch: 256 Train loss: 3.1163272857666016 Valid loss: 3.1163135319948196 Duration: 474.7882957220078 minutes\n",
      ", saved best model.\n",
      "Epoch: 257 Train loss: 3.115403890609741 Valid loss: 3.1242601424455643 Duration: 476.6344028274218 minutes\n",
      "Epoch: 258 Train loss: 3.129464864730835 Valid loss: 3.1189526170492172 Duration: 478.47919077475865 minutes\n",
      "Epoch: 259 Train loss: 3.1187942028045654 Valid loss: 3.117197945713997 Duration: 480.3233081062635 minutes\n",
      "Epoch: 260 Train loss: 3.1125781536102295 Valid loss: 3.119496077299118 Duration: 482.1686737616857 minutes\n",
      "Epoch: 261 Train loss: 3.1221139430999756 Valid loss: 3.1143623143434525 Duration: 484.01377193927766 minutes\n",
      ", saved best model.\n",
      "Epoch: 262 Train loss: 3.138026714324951 Valid loss: 3.115000993013382 Duration: 485.8927898446719 minutes\n",
      "Epoch: 263 Train loss: 3.1154329776763916 Valid loss: 3.1168036609888077 Duration: 487.79708300034207 minutes\n",
      "Epoch: 264 Train loss: 3.139145612716675 Valid loss: 3.117973580956459 Duration: 489.63980542818706 minutes\n",
      "Epoch: 265 Train loss: 3.116957426071167 Valid loss: 3.114913210272789 Duration: 491.48381779591244 minutes\n",
      "Epoch: 266 Train loss: 3.139281749725342 Valid loss: 3.1149806082248688 Duration: 493.3274187922478 minutes\n",
      "Epoch: 267 Train loss: 3.1193289756774902 Valid loss: 3.1133508533239365 Duration: 495.1713541984558 minutes\n",
      ", saved best model.\n",
      "Epoch: 268 Train loss: 3.1118931770324707 Valid loss: 3.1088801324367523 Duration: 497.01438352664314 minutes\n",
      ", saved best model.\n",
      "Epoch: 269 Train loss: 3.1156537532806396 Valid loss: 3.110120937228203 Duration: 498.856670820713 minutes\n",
      "Epoch: 270 Train loss: 3.105659008026123 Valid loss: 3.117933616042137 Duration: 500.70156280994416 minutes\n",
      "Epoch: 271 Train loss: 3.0956430435180664 Valid loss: 3.1135062277317047 Duration: 502.5456163644791 minutes\n",
      "Epoch: 272 Train loss: 3.1039018630981445 Valid loss: 3.105405271053314 Duration: 504.3893438140551 minutes\n",
      ", saved best model.\n",
      "Epoch: 273 Train loss: 3.1224381923675537 Valid loss: 3.114147737622261 Duration: 506.23615114688874 minutes\n",
      "Epoch: 274 Train loss: 3.1310317516326904 Valid loss: 3.103616863489151 Duration: 508.0835255265236 minutes\n",
      ", saved best model.\n",
      "Epoch: 275 Train loss: 3.0926880836486816 Valid loss: 3.110327035188675 Duration: 509.9301141222318 minutes\n",
      "Epoch: 276 Train loss: 3.088432788848877 Valid loss: 3.1049800664186478 Duration: 511.7763357877731 minutes\n",
      "Epoch: 277 Train loss: 3.0933492183685303 Valid loss: 3.107922002673149 Duration: 513.6210674444834 minutes\n",
      "Epoch: 278 Train loss: 3.1087136268615723 Valid loss: 3.107078403234482 Duration: 515.4655071814855 minutes\n",
      "Epoch: 279 Train loss: 3.0959701538085938 Valid loss: 3.0986304581165314 Duration: 517.311723168691 minutes\n",
      ", saved best model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 280 Train loss: 3.1051852703094482 Valid loss: 3.10958893597126 Duration: 519.1537463267645 minutes\n",
      "Epoch: 281 Train loss: 3.101224660873413 Valid loss: 3.1031460911035538 Duration: 520.996068139871 minutes\n",
      "Epoch: 282 Train loss: 3.1026952266693115 Valid loss: 3.0989574640989304 Duration: 522.837954934438 minutes\n",
      "Epoch: 283 Train loss: 3.104508876800537 Valid loss: 3.103582486510277 Duration: 524.6807196855545 minutes\n",
      "Epoch: 284 Train loss: 3.127877712249756 Valid loss: 3.1109063923358917 Duration: 526.5224078814189 minutes\n",
      "Epoch: 285 Train loss: 3.0992085933685303 Valid loss: 3.098812773823738 Duration: 528.3654521743457 minutes\n",
      "Epoch: 286 Train loss: 3.0845866203308105 Valid loss: 3.1005632132291794 Duration: 530.2122662146886 minutes\n",
      "Epoch: 287 Train loss: 3.0982649326324463 Valid loss: 3.1044293493032455 Duration: 532.0592505852381 minutes\n",
      "Epoch: 288 Train loss: 3.0885558128356934 Valid loss: 3.1029745787382126 Duration: 533.9034483790398 minutes\n",
      "Epoch: 289 Train loss: 3.086184024810791 Valid loss: 3.09874264895916 Duration: 535.7476617574691 minutes\n",
      "Epoch: 290 Train loss: 3.10017728805542 Valid loss: 3.099100649356842 Duration: 537.5916251937548 minutes\n",
      "Epoch: 291 Train loss: 3.1176300048828125 Valid loss: 3.0987427830696106 Duration: 539.4362894972165 minutes\n",
      "Epoch: 292 Train loss: 3.111226797103882 Valid loss: 3.0990219563245773 Duration: 541.2810819387436 minutes\n",
      "Epoch: 293 Train loss: 3.1248998641967773 Valid loss: 3.098811060190201 Duration: 543.12652703921 minutes\n",
      "Epoch: 294 Train loss: 3.0941977500915527 Valid loss: 3.1029272377490997 Duration: 544.9724948803583 minutes\n",
      "Epoch: 296 Train loss: 3.097050189971924 Valid loss: 3.1024493277072906 Duration: 548.6633895039558 minutes\n",
      "Epoch: 297 Train loss: 3.109408378601074 Valid loss: 3.0964770168066025 Duration: 550.5122445424398 minutes\n",
      ", saved best model.\n",
      "Epoch: 298 Train loss: 3.117927312850952 Valid loss: 3.1009826958179474 Duration: 552.3619768619537 minutes\n",
      "Epoch: 299 Train loss: 3.1023809909820557 Valid loss: 3.096636116504669 Duration: 554.2120917320251 minutes\n",
      "Epoch: 300 Train loss: 3.09835147857666 Valid loss: 3.0905056595802307 Duration: 556.0621769070625 minutes\n",
      ", saved best model.\n",
      "Epoch: 301 Train loss: 3.088330030441284 Valid loss: 3.092286691069603 Duration: 557.9151988943418 minutes\n",
      "Epoch: 302 Train loss: 3.085063934326172 Valid loss: 3.098220482468605 Duration: 559.7634400486946 minutes\n",
      "Epoch: 303 Train loss: 3.0853769779205322 Valid loss: 3.089512139558792 Duration: 561.6122178713481 minutes\n",
      ", saved best model.\n",
      "Epoch: 304 Train loss: 3.104917526245117 Valid loss: 3.089411646127701 Duration: 563.4593121647835 minutes\n",
      ", saved best model.\n",
      "Epoch: 305 Train loss: 3.0953733921051025 Valid loss: 3.0908223539590836 Duration: 565.3085517088572 minutes\n",
      "Epoch: 306 Train loss: 3.103660821914673 Valid loss: 3.0963222086429596 Duration: 567.1555512110392 minutes\n",
      "Epoch: 307 Train loss: 3.087153196334839 Valid loss: 3.0900896340608597 Duration: 569.0019543528557 minutes\n",
      "Epoch: 308 Train loss: 3.1137406826019287 Valid loss: 3.0857429057359695 Duration: 570.8500256419181 minutes\n",
      ", saved best model.\n",
      "Epoch: 309 Train loss: 3.091257333755493 Valid loss: 3.092447876930237 Duration: 572.7007051110268 minutes\n",
      "Epoch: 310 Train loss: 3.086580753326416 Valid loss: 3.0862910002470016 Duration: 574.5512016018232 minutes\n",
      "Epoch: 311 Train loss: 3.0866105556488037 Valid loss: 3.084289565682411 Duration: 576.4021801193555 minutes\n",
      ", saved best model.\n",
      "Epoch: 312 Train loss: 3.102792739868164 Valid loss: 3.0834480971097946 Duration: 578.2503065625826 minutes\n",
      ", saved best model.\n",
      "Epoch: 313 Train loss: 3.090928554534912 Valid loss: 3.087470918893814 Duration: 580.1053823033968 minutes\n",
      "Epoch: 314 Train loss: 3.0923075675964355 Valid loss: 3.0880760699510574 Duration: 581.9597310185433 minutes\n",
      "Epoch: 315 Train loss: 3.0953240394592285 Valid loss: 3.0883932411670685 Duration: 583.8092844406764 minutes\n",
      "Epoch: 316 Train loss: 3.0925796031951904 Valid loss: 3.083142951130867 Duration: 585.653989036878 minutes\n",
      ", saved best model.\n",
      "Epoch: 317 Train loss: 3.069049596786499 Valid loss: 3.084671050310135 Duration: 587.5019959410032 minutes\n",
      "Epoch: 318 Train loss: 3.070323944091797 Valid loss: 3.0836633145809174 Duration: 589.3552325288455 minutes\n",
      "Epoch: 319 Train loss: 3.0964536666870117 Valid loss: 3.0894299894571304 Duration: 591.2039394299189 minutes\n",
      "Epoch: 320 Train loss: 3.0697624683380127 Valid loss: 3.0835929811000824 Duration: 593.0523647189141 minutes\n",
      "Epoch: 321 Train loss: 3.0648677349090576 Valid loss: 3.086587205529213 Duration: 594.9013156970342 minutes\n",
      "Epoch: 322 Train loss: 3.076286554336548 Valid loss: 3.083352878689766 Duration: 596.7519827365875 minutes\n",
      "Epoch: 323 Train loss: 3.0950429439544678 Valid loss: 3.0846425145864487 Duration: 598.6020800789197 minutes\n",
      "Epoch: 324 Train loss: 3.0898256301879883 Valid loss: 3.081127867102623 Duration: 600.4582499265671 minutes\n",
      ", saved best model.\n",
      "Epoch: 325 Train loss: 3.0790982246398926 Valid loss: 3.081491470336914 Duration: 602.3062037706375 minutes\n",
      "Epoch: 326 Train loss: 3.094977378845215 Valid loss: 3.0810481756925583 Duration: 604.1526312232018 minutes\n",
      ", saved best model.\n",
      "Epoch: 327 Train loss: 3.0789647102355957 Valid loss: 3.0843127071857452 Duration: 605.9979888796806 minutes\n",
      "Epoch: 328 Train loss: 3.0847110748291016 Valid loss: 3.079164817929268 Duration: 607.8440059741338 minutes\n",
      ", saved best model.\n",
      "Epoch: 329 Train loss: 3.0965795516967773 Valid loss: 3.0744978934526443 Duration: 609.6881984114647 minutes\n",
      ", saved best model.\n",
      "Epoch: 330 Train loss: 3.0676801204681396 Valid loss: 3.0765760391950607 Duration: 611.5311957955361 minutes\n",
      "Epoch: 331 Train loss: 3.0920090675354004 Valid loss: 3.085571125149727 Duration: 613.3769959330559 minutes\n",
      "Epoch: 332 Train loss: 3.0719447135925293 Valid loss: 3.074366122484207 Duration: 615.2214379707972 minutes\n",
      ", saved best model.\n",
      "Epoch: 333 Train loss: 3.0750367641448975 Valid loss: 3.0735528022050858 Duration: 617.067209482193 minutes\n",
      ", saved best model.\n",
      "Epoch: 334 Train loss: 3.0876283645629883 Valid loss: 3.0830924808979034 Duration: 618.9132659395535 minutes\n",
      "Epoch: 335 Train loss: 3.07831072807312 Valid loss: 3.0768356770277023 Duration: 620.7597627003987 minutes\n",
      "Epoch: 336 Train loss: 3.0748136043548584 Valid loss: 3.067293167114258 Duration: 622.6056114236513 minutes\n",
      ", saved best model.\n",
      "Epoch: 337 Train loss: 3.0849528312683105 Valid loss: 3.0744592547416687 Duration: 624.4519292632739 minutes\n",
      "Epoch: 338 Train loss: 3.0726428031921387 Valid loss: 3.076356828212738 Duration: 626.299119190375 minutes\n",
      "Epoch: 339 Train loss: 3.0791237354278564 Valid loss: 3.0754158049821854 Duration: 628.1438930630684 minutes\n",
      "Epoch: 340 Train loss: 3.0550496578216553 Valid loss: 3.0695000141859055 Duration: 629.987578745683 minutes\n",
      "Epoch: 341 Train loss: 3.0786077976226807 Valid loss: 3.075289413332939 Duration: 631.8321572303772 minutes\n",
      "Epoch: 342 Train loss: 3.0558276176452637 Valid loss: 3.0761167407035828 Duration: 633.6766513943672 minutes\n",
      "Epoch: 343 Train loss: 3.068150520324707 Valid loss: 3.078347012400627 Duration: 635.5223662932714 minutes\n",
      "Epoch: 344 Train loss: 3.065382242202759 Valid loss: 3.0727603882551193 Duration: 637.3684992074966 minutes\n",
      "Epoch: 345 Train loss: 3.0346622467041016 Valid loss: 3.0666608810424805 Duration: 639.2122080922127 minutes\n",
      ", saved best model.\n",
      "Epoch: 346 Train loss: 3.073383092880249 Valid loss: 3.0730721950531006 Duration: 641.0588613828023 minutes\n",
      "Epoch: 347 Train loss: 3.05761456489563 Valid loss: 3.0686129480600357 Duration: 642.9007721940676 minutes\n",
      "Epoch: 348 Train loss: 3.080655336380005 Valid loss: 3.0679410845041275 Duration: 644.7426817258199 minutes\n",
      "Epoch: 349 Train loss: 3.0798592567443848 Valid loss: 3.077128693461418 Duration: 646.5854887247085 minutes\n",
      "Epoch: 350 Train loss: 3.0503060817718506 Valid loss: 3.069508597254753 Duration: 648.4308508117994 minutes\n",
      "Epoch: 351 Train loss: 3.0971105098724365 Valid loss: 3.0665415674448013 Duration: 650.2736739873886 minutes\n",
      ", saved best model.\n",
      "Epoch: 352 Train loss: 3.0696821212768555 Valid loss: 3.0639481842517853 Duration: 652.1166665593784 minutes\n",
      ", saved best model.\n",
      "Epoch: 353 Train loss: 3.0456364154815674 Valid loss: 3.0682995468378067 Duration: 653.9594482461612 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 354 Train loss: 3.0770821571350098 Valid loss: 3.064258188009262 Duration: 655.8030522068342 minutes\n",
      "Epoch: 355 Train loss: 3.086073160171509 Valid loss: 3.066983073949814 Duration: 657.6505787650744 minutes\n",
      "Epoch: 356 Train loss: 3.056857109069824 Valid loss: 3.0677777230739594 Duration: 659.495664858818 minutes\n",
      "Epoch: 357 Train loss: 3.057933807373047 Valid loss: 3.0614647269248962 Duration: 661.3401191194852 minutes\n",
      ", saved best model.\n",
      "Epoch: 358 Train loss: 3.059025526046753 Valid loss: 3.0653997510671616 Duration: 663.1827475627264 minutes\n",
      "Epoch: 359 Train loss: 3.056196451187134 Valid loss: 3.059045895934105 Duration: 665.0259864648183 minutes\n",
      ", saved best model.\n",
      "Epoch: 360 Train loss: 3.086130380630493 Valid loss: 3.066851496696472 Duration: 666.8687577724456 minutes\n",
      "Epoch: 361 Train loss: 3.061101198196411 Valid loss: 3.059284895658493 Duration: 668.710777759552 minutes\n",
      "Epoch: 362 Train loss: 3.057584524154663 Valid loss: 3.062182918190956 Duration: 670.5534772396088 minutes\n",
      "Epoch: 363 Train loss: 3.0537917613983154 Valid loss: 3.05894835293293 Duration: 672.3981742501259 minutes\n",
      ", saved best model.\n",
      "Epoch: 364 Train loss: 3.0763962268829346 Valid loss: 3.0633739680051804 Duration: 674.241381307443 minutes\n",
      "Epoch: 365 Train loss: 3.0607447624206543 Valid loss: 3.0589185655117035 Duration: 676.0844494740169 minutes\n",
      ", saved best model.\n",
      "Epoch: 366 Train loss: 3.044665813446045 Valid loss: 3.0537969768047333 Duration: 677.9277835607529 minutes\n",
      ", saved best model.\n",
      "Epoch: 367 Train loss: 3.0533671379089355 Valid loss: 3.0569347143173218 Duration: 679.7727349003156 minutes\n",
      "Epoch: 368 Train loss: 3.086629629135132 Valid loss: 3.0575280487537384 Duration: 681.616407251358 minutes\n",
      "Epoch: 369 Train loss: 3.0558528900146484 Valid loss: 3.060729056596756 Duration: 683.4604876796404 minutes\n",
      "Epoch: 370 Train loss: 3.066716194152832 Valid loss: 3.0560291707515717 Duration: 685.300578769048 minutes\n",
      "Epoch: 371 Train loss: 3.047320604324341 Valid loss: 3.060994505882263 Duration: 687.1459939757983 minutes\n",
      "Epoch: 372 Train loss: 3.041957378387451 Valid loss: 3.050131633877754 Duration: 688.9889626979827 minutes\n",
      ", saved best model.\n",
      "Epoch: 373 Train loss: 3.0545263290405273 Valid loss: 3.056986317038536 Duration: 690.8333325544993 minutes\n",
      "Epoch: 374 Train loss: 3.0537636280059814 Valid loss: 3.0587522983551025 Duration: 692.6779347936313 minutes\n",
      "Epoch: 375 Train loss: 3.0548694133758545 Valid loss: 3.054074212908745 Duration: 694.5225823362669 minutes\n",
      "Epoch: 376 Train loss: 3.0480074882507324 Valid loss: 3.0576502829790115 Duration: 696.3628404696782 minutes\n",
      "Epoch: 377 Train loss: 3.0573394298553467 Valid loss: 3.0542755872011185 Duration: 698.2045132756233 minutes\n",
      "Epoch: 378 Train loss: 3.0667402744293213 Valid loss: 3.052754834294319 Duration: 700.0503639101983 minutes\n",
      "Epoch: 379 Train loss: 3.055962085723877 Valid loss: 3.0533215403556824 Duration: 701.9019787788391 minutes\n",
      "Epoch: 380 Train loss: 3.0292446613311768 Valid loss: 3.0541899353265762 Duration: 703.7540313720704 minutes\n",
      "Epoch: 381 Train loss: 3.0504612922668457 Valid loss: 3.0481570214033127 Duration: 705.6076234857242 minutes\n",
      ", saved best model.\n",
      "Epoch: 382 Train loss: 3.0591557025909424 Valid loss: 3.04899999499321 Duration: 707.457773454984 minutes\n",
      "Epoch: 383 Train loss: 3.0891621112823486 Valid loss: 3.049327865242958 Duration: 709.3091260472934 minutes\n",
      "Epoch: 384 Train loss: 3.0455405712127686 Valid loss: 3.04462830722332 Duration: 711.1597477118174 minutes\n",
      ", saved best model.\n",
      "Epoch: 385 Train loss: 3.055925130844116 Valid loss: 3.0503597110509872 Duration: 713.0173834284146 minutes\n",
      "Epoch: 386 Train loss: 3.0425546169281006 Valid loss: 3.053236871957779 Duration: 714.8605893969536 minutes\n",
      "Epoch: 387 Train loss: 3.039842128753662 Valid loss: 3.0470415502786636 Duration: 716.7081692655881 minutes\n",
      "Epoch: 388 Train loss: 3.062851905822754 Valid loss: 3.0541494637727737 Duration: 718.5515748659769 minutes\n",
      "Epoch: 389 Train loss: 3.04693865776062 Valid loss: 3.048580914735794 Duration: 720.3978265285492 minutes\n",
      "Epoch: 390 Train loss: 3.0623409748077393 Valid loss: 3.0420804619789124 Duration: 722.2428444107373 minutes\n",
      ", saved best model.\n",
      "Epoch: 391 Train loss: 3.042405843734741 Valid loss: 3.0472294837236404 Duration: 724.0881571968397 minutes\n",
      "Epoch: 392 Train loss: 3.0382771492004395 Valid loss: 3.0405613780021667 Duration: 725.933119503657 minutes\n",
      ", saved best model.\n",
      "Epoch: 393 Train loss: 3.029906988143921 Valid loss: 3.044771268963814 Duration: 727.776471654574 minutes\n",
      "Epoch: 394 Train loss: 3.0349981784820557 Valid loss: 3.0490822345018387 Duration: 729.6207557717959 minutes\n",
      "Epoch: 395 Train loss: 3.035486936569214 Valid loss: 3.044192537665367 Duration: 731.4659858584404 minutes\n",
      "Epoch: 396 Train loss: 3.033125638961792 Valid loss: 3.0438235849142075 Duration: 733.3104152997334 minutes\n",
      "Epoch: 397 Train loss: 3.0298256874084473 Valid loss: 3.0379358530044556 Duration: 735.1550470630328 minutes\n",
      ", saved best model.\n",
      "Epoch: 398 Train loss: 3.0430498123168945 Valid loss: 3.0478049367666245 Duration: 737.0000425974528 minutes\n",
      "Epoch: 399 Train loss: 3.0489494800567627 Valid loss: 3.0379610508680344 Duration: 738.8433764417966 minutes\n",
      "Epoch: 400 Train loss: 3.0172791481018066 Valid loss: 3.0404161512851715 Duration: 740.6877982219061 minutes\n",
      "Epoch: 401 Train loss: 3.033787488937378 Valid loss: 3.0371508598327637 Duration: 742.5322482983271 minutes\n",
      ", saved best model.\n",
      "Epoch: 402 Train loss: 3.0505287647247314 Valid loss: 3.0390972644090652 Duration: 744.3757119456927 minutes\n",
      "Epoch: 403 Train loss: 3.0283102989196777 Valid loss: 3.035929962992668 Duration: 746.219915831089 minutes\n",
      ", saved best model.\n",
      "Epoch: 404 Train loss: 3.0315001010894775 Valid loss: 3.038976773619652 Duration: 748.0660818974177 minutes\n",
      "Epoch: 405 Train loss: 3.0499606132507324 Valid loss: 3.036067396402359 Duration: 749.9090714653333 minutes\n",
      "Epoch: 406 Train loss: 3.068671226501465 Valid loss: 3.042789801955223 Duration: 751.7523525675138 minutes\n",
      "Epoch: 407 Train loss: 3.040240526199341 Valid loss: 3.0417082607746124 Duration: 753.5965235710144 minutes\n",
      "Epoch: 408 Train loss: 3.012540102005005 Valid loss: 3.0372981131076813 Duration: 755.4418483893077 minutes\n",
      "Epoch: 409 Train loss: 3.048661231994629 Valid loss: 3.036376252770424 Duration: 757.285373878479 minutes\n",
      "Epoch: 410 Train loss: 3.036778211593628 Valid loss: 3.034244492650032 Duration: 759.1297012845675 minutes\n",
      ", saved best model.\n",
      "Epoch: 411 Train loss: 3.0341527462005615 Valid loss: 3.0338294506073 Duration: 760.9736341953278 minutes\n",
      ", saved best model.\n",
      "Epoch: 412 Train loss: 3.0321927070617676 Valid loss: 3.040790170431137 Duration: 762.8146086176237 minutes\n",
      "Epoch: 413 Train loss: 3.0276453495025635 Valid loss: 3.040048822760582 Duration: 764.6587655742964 minutes\n",
      "Epoch: 414 Train loss: 3.034398317337036 Valid loss: 3.038677364587784 Duration: 766.5050533930461 minutes\n",
      "Epoch: 415 Train loss: 3.036508798599243 Valid loss: 3.037019222974777 Duration: 768.3499891241391 minutes\n",
      "Epoch: 416 Train loss: 3.044965982437134 Valid loss: 3.0298993289470673 Duration: 770.195389564832 minutes\n",
      ", saved best model.\n",
      "Epoch: 417 Train loss: 3.0333902835845947 Valid loss: 3.031837582588196 Duration: 772.0427170236906 minutes\n",
      "Epoch: 418 Train loss: 3.044790267944336 Valid loss: 3.033204436302185 Duration: 773.8881071885427 minutes\n",
      "Epoch: 419 Train loss: 3.008507490158081 Valid loss: 3.029533714056015 Duration: 775.7347899476687 minutes\n",
      ", saved best model.\n",
      "Epoch: 420 Train loss: 3.0543253421783447 Valid loss: 3.0357319712638855 Duration: 777.5790135820706 minutes\n",
      "Epoch: 421 Train loss: 3.049562692642212 Valid loss: 3.023405596613884 Duration: 779.4246686697006 minutes\n",
      ", saved best model.\n",
      "Epoch: 422 Train loss: 3.0043814182281494 Valid loss: 3.0367711037397385 Duration: 781.266942636172 minutes\n",
      "Epoch: 423 Train loss: 3.0294651985168457 Valid loss: 3.025327295064926 Duration: 783.1101800521214 minutes\n",
      "Epoch: 424 Train loss: 2.9951553344726562 Valid loss: 3.029534950852394 Duration: 784.9523225347201 minutes\n",
      "Epoch: 425 Train loss: 3.009577751159668 Valid loss: 3.0265699476003647 Duration: 786.7942475159963 minutes\n",
      "Epoch: 426 Train loss: 3.0129873752593994 Valid loss: 3.0246897637844086 Duration: 788.6439820766449 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 427 Train loss: 3.0468621253967285 Valid loss: 3.0303279012441635 Duration: 790.4880872050921 minutes\n",
      "Epoch: 428 Train loss: 3.0537588596343994 Valid loss: 3.0204939395189285 Duration: 792.3331376949947 minutes\n",
      ", saved best model.\n",
      "Epoch: 429 Train loss: 3.0227878093719482 Valid loss: 3.027157962322235 Duration: 794.1774938583374 minutes\n",
      "Epoch: 430 Train loss: 3.038226842880249 Valid loss: 3.023071527481079 Duration: 796.0228035767873 minutes\n",
      "Epoch: 431 Train loss: 2.9999332427978516 Valid loss: 3.023884281516075 Duration: 797.8690736611684 minutes\n",
      "Epoch: 432 Train loss: 3.0019357204437256 Valid loss: 3.024831086397171 Duration: 799.7151622494061 minutes\n",
      "Epoch: 433 Train loss: 3.027595043182373 Valid loss: 3.0183248221874237 Duration: 801.5632421135903 minutes\n",
      ", saved best model.\n",
      "Epoch: 434 Train loss: 2.9944024085998535 Valid loss: 3.0212394297122955 Duration: 803.4087690830231 minutes\n",
      "Epoch: 435 Train loss: 3.0289394855499268 Valid loss: 3.0190470069646835 Duration: 805.2561062892278 minutes\n",
      "Epoch: 436 Train loss: 3.0323452949523926 Valid loss: 3.016886606812477 Duration: 807.1040554006894 minutes\n",
      ", saved best model.\n",
      "Epoch: 437 Train loss: 3.00939679145813 Valid loss: 3.0182370394468307 Duration: 808.9526825706164 minutes\n",
      "Epoch: 438 Train loss: 3.0423288345336914 Valid loss: 3.0175367295742035 Duration: 810.7989572842915 minutes\n",
      "Epoch: 439 Train loss: 3.0471386909484863 Valid loss: 3.0220267325639725 Duration: 812.6458032051722 minutes\n",
      "Epoch: 440 Train loss: 3.0400960445404053 Valid loss: 3.0171453803777695 Duration: 814.4925279696782 minutes\n",
      "Epoch: 441 Train loss: 3.002531051635742 Valid loss: 3.0202403515577316 Duration: 816.3392387906711 minutes\n",
      "Epoch: 442 Train loss: 3.017670154571533 Valid loss: 3.0216224640607834 Duration: 818.1857459584872 minutes\n",
      "Epoch: 443 Train loss: 3.037224054336548 Valid loss: 3.015981748700142 Duration: 820.0299698551496 minutes\n",
      ", saved best model.\n",
      "Epoch: 444 Train loss: 3.014346122741699 Valid loss: 3.027373030781746 Duration: 821.8742148121198 minutes\n",
      "Epoch: 445 Train loss: 3.040898561477661 Valid loss: 3.020176574587822 Duration: 823.7187375267347 minutes\n",
      "Epoch: 446 Train loss: 3.0300791263580322 Valid loss: 3.011566534638405 Duration: 825.5625158230464 minutes\n",
      ", saved best model.\n",
      "Epoch: 447 Train loss: 3.049128770828247 Valid loss: 3.019115686416626 Duration: 827.4073168635368 minutes\n",
      "Epoch: 448 Train loss: 3.026785373687744 Valid loss: 3.0113874673843384 Duration: 829.2536241054535 minutes\n",
      ", saved best model.\n",
      "Epoch: 449 Train loss: 2.976884126663208 Valid loss: 3.0163082033395767 Duration: 831.0979057192802 minutes\n",
      "Epoch: 450 Train loss: 3.02599835395813 Valid loss: 3.014037922024727 Duration: 832.9429235855738 minutes\n",
      "Epoch: 451 Train loss: 3.0114245414733887 Valid loss: 3.0086745470762253 Duration: 834.7916621685029 minutes\n",
      ", saved best model.\n",
      "Epoch: 452 Train loss: 3.0342655181884766 Valid loss: 3.0175297409296036 Duration: 836.6399803638458 minutes\n",
      "Epoch: 453 Train loss: 3.010974168777466 Valid loss: 3.011131137609482 Duration: 838.4885257959365 minutes\n",
      "Epoch: 454 Train loss: 2.983928680419922 Valid loss: 3.0133543759584427 Duration: 840.33375467062 minutes\n",
      "Epoch: 455 Train loss: 3.0401804447174072 Valid loss: 3.015619233250618 Duration: 842.1783140063286 minutes\n",
      "Epoch: 456 Train loss: 3.0160164833068848 Valid loss: 3.0130661875009537 Duration: 844.0243376612664 minutes\n",
      "Epoch: 457 Train loss: 3.0403940677642822 Valid loss: 3.001075580716133 Duration: 845.8708432396253 minutes\n",
      ", saved best model.\n",
      "Epoch: 458 Train loss: 3.0091400146484375 Valid loss: 3.018448978662491 Duration: 847.7193599661191 minutes\n",
      "Epoch: 459 Train loss: 3.0146381855010986 Valid loss: 3.0056701749563217 Duration: 849.5674757877986 minutes\n",
      "Epoch: 460 Train loss: 2.995513916015625 Valid loss: 3.0125468224287033 Duration: 851.4159496744473 minutes\n",
      "Epoch: 461 Train loss: 2.997988700866699 Valid loss: 3.017376869916916 Duration: 853.2624053875605 minutes\n",
      "Epoch: 462 Train loss: 2.992504596710205 Valid loss: 3.0121570229530334 Duration: 855.1088791251183 minutes\n",
      "Epoch: 463 Train loss: 3.0136258602142334 Valid loss: 3.008390635251999 Duration: 856.9572316924731 minutes\n",
      "Epoch: 464 Train loss: 3.0133917331695557 Valid loss: 3.002612382173538 Duration: 858.8040984988213 minutes\n",
      "Epoch: 465 Train loss: 2.985142469406128 Valid loss: 3.0000779777765274 Duration: 860.6504740158717 minutes\n",
      ", saved best model.\n",
      "Epoch: 466 Train loss: 3.007513999938965 Valid loss: 3.0008644312620163 Duration: 862.4952871441841 minutes\n",
      "Epoch: 467 Train loss: 2.981428861618042 Valid loss: 3.0012863278388977 Duration: 864.340268735091 minutes\n",
      "Epoch: 468 Train loss: 3.010043144226074 Valid loss: 3.002252534031868 Duration: 866.1845085859298 minutes\n",
      "Epoch: 469 Train loss: 3.0059025287628174 Valid loss: 3.0111629217863083 Duration: 868.0310082912445 minutes\n",
      "Epoch: 470 Train loss: 2.9816811084747314 Valid loss: 2.998247444629669 Duration: 869.8743297696113 minutes\n",
      ", saved best model.\n",
      "Epoch: 471 Train loss: 3.019040584564209 Valid loss: 3.006880208849907 Duration: 871.718349826336 minutes\n",
      "Epoch: 472 Train loss: 2.9905192852020264 Valid loss: 3.0048393309116364 Duration: 873.5637052456538 minutes\n",
      "Epoch: 473 Train loss: 3.006025791168213 Valid loss: 2.999217242002487 Duration: 875.4091318011284 minutes\n",
      "Epoch: 474 Train loss: 3.0074384212493896 Valid loss: 3.0038824528455734 Duration: 877.2548336227735 minutes\n",
      "Epoch: 475 Train loss: 3.0146305561065674 Valid loss: 3.0114604979753494 Duration: 879.1001149376233 minutes\n",
      "Epoch: 476 Train loss: 3.0058977603912354 Valid loss: 3.00094173848629 Duration: 880.9445200125376 minutes\n",
      "Epoch: 477 Train loss: 3.005190372467041 Valid loss: 2.996299162507057 Duration: 882.7915766676267 minutes\n",
      ", saved best model.\n",
      "Epoch: 478 Train loss: 3.0122275352478027 Valid loss: 3.0014897137880325 Duration: 884.6356186072031 minutes\n",
      "Epoch: 479 Train loss: 3.007646083831787 Valid loss: 3.001053273677826 Duration: 886.4793322881063 minutes\n",
      "Epoch: 480 Train loss: 2.9982638359069824 Valid loss: 2.9946421086788177 Duration: 888.32375775973 minutes\n",
      ", saved best model.\n",
      "Epoch: 481 Train loss: 2.9915852546691895 Valid loss: 2.9989495873451233 Duration: 890.1707403977712 minutes\n",
      "Epoch: 482 Train loss: 2.973151683807373 Valid loss: 2.9940159022808075 Duration: 892.0173618475596 minutes\n",
      ", saved best model.\n",
      "Epoch: 483 Train loss: 2.9921867847442627 Valid loss: 2.9872253090143204 Duration: 893.8614742954572 minutes\n",
      ", saved best model.\n",
      "Epoch: 484 Train loss: 2.982088804244995 Valid loss: 2.9935972541570663 Duration: 895.7076650778453 minutes\n",
      "Epoch: 485 Train loss: 3.0069806575775146 Valid loss: 2.9940158128738403 Duration: 897.5571876764297 minutes\n",
      "Epoch: 486 Train loss: 2.9901082515716553 Valid loss: 2.9916737228631973 Duration: 899.4049162387848 minutes\n",
      "Epoch: 487 Train loss: 3.008770704269409 Valid loss: 2.995010867714882 Duration: 901.2527483383815 minutes\n",
      "Epoch: 488 Train loss: 2.9793503284454346 Valid loss: 2.9941451102495193 Duration: 903.1019749561946 minutes\n",
      "Epoch: 489 Train loss: 3.0314996242523193 Valid loss: 2.98820136487484 Duration: 904.9521605610847 minutes\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "patience = 200\n",
    "patience_controler = Patience(patience)\n",
    "start_time = time.time()\n",
    "\n",
    "for num_epoch in range(500000):\n",
    "    train_loss = train()        \n",
    "    valid_loss = validation()\n",
    "    \n",
    "    writer.add_scalar('Loss/train', train_loss, num_epoch)\n",
    "    writer.add_scalar('Loss/validation', valid_loss, num_epoch)\n",
    "    \n",
    "    print(f'Epoch: {num_epoch} Train loss: {train_loss} Valid loss: {valid_loss} Duration: {(time.time() - start_time)/60} minutes',)\n",
    "\n",
    "    if not patience_controler.more_patience(valid_loss):\n",
    "        print(\"Se acabó la paciencia\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model.load_state_dict(torch.load('CNN_model_30000_words_TF_PAD_noise.pt'))\n",
    "CNN_model.eval()\n",
    "\n",
    "Encoder_model.load_state_dict(torch.load('Encoder_model_30000_words_TF_PAD_noise.pt'))\n",
    "Encoder_model.eval()\n",
    "\n",
    "Decoder_model.load_state_dict(torch.load('Decoder_model_30000_words_TF_PAD_noise.pt'))\n",
    "Decoder_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test():\n",
    "    with torch.no_grad():\n",
    "        for num_batch, (image_test, label_test) in enumerate(test_loader):\n",
    "            num_batch += 1\n",
    "            encoder_hidden_test = Encoder_model.initHidden(batch_size = batch_size)\n",
    "            image_cnn_test = image_test.view(-1, color_channels, patch_height, patch_width).cuda(0)\n",
    "            encoder_input_test = CNN_model(image_cnn_test)\n",
    "            encoder_output, encoder_hidden_test = Encoder_model(encoder_input_test, encoder_hidden_test, batch = test_batch, seq_len = n_patches)\n",
    "\n",
    "            #decoder_hidden_test = (encoder_hidden_test[0][0, :, :].view(1, batch_size, hidden_size), # We take the last hidden state of the Encoder \n",
    "            #                       encoder_hidden_test[1][0, :, :].view(1, batch_size, hidden_size)) # for each image/word (j) within the batch \n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                decoder_input_test = mapeo['START'].cuda(0) # We initialize the first Decoder input as the START token\n",
    "                decoder_hidden_test = (encoder_hidden_test[0][0, j, :].view(1, 1, hidden_size), # We take the last hidden state of the Encoder \n",
    "                                       encoder_hidden_test[1][0, j, :].view(1, 1, hidden_size)) # for each image/word (j) within the batch \n",
    "                \n",
    "                for d in range(MAX_LENGTH + 2):\n",
    "                    decoder_output_test, decoder_hidden_test = Decoder_model(decoder_input_test, decoder_hidden_test, batch = 1, seq_len = 1)\n",
    "\n",
    "                    output_letter = one_hot_conversion(decoder_output_test, output_size = output_size)\n",
    "                    decoder_input_test = output_letter\n",
    "                    \n",
    "                    if d == 0:\n",
    "                        output_word = output_letter\n",
    "                    else:\n",
    "                        output_word = torch.cat((output_word, output_letter), dim = 1).cuda(0)\n",
    "                    \n",
    "                    if torch.equal(output_letter, letter_to_vector('END').cuda(0)):\n",
    "                        break\n",
    "                output_word = torch.argmax(output_word, dim=2)\n",
    "                output_word = output_word.view(output_word.numel()) # view as a rank-1 tensor\n",
    "\n",
    "                model_word = []\n",
    "                for item in output_word:\n",
    "                    model_word.append(letters[item])\n",
    "\n",
    "                model_word = ''.join(model_word[:-1])\n",
    "                print(model_word)\n",
    "            print(test_set) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7b08819e076e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-c209519ca05f>\u001b[0m in \u001b[0;36mTest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnum_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mnum_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mencoder_hidden_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_estoril",
   "language": "python",
   "name": "pytorch_estoril"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
