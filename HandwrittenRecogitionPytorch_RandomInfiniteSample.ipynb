{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66539e30",
   "metadata": {},
   "source": [
    "# Antes de empezar\n",
    "\n",
    "conda activate python3.6_cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b10b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "import cv2\n",
    "import skimage\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Ignore harmless warnings:\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea7ff72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n",
      "3.6.10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Quadro RTX 5000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "print(torch.__version__)\n",
    "print(platform.python_version())\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86dd2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['START', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "          'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'END', 'PAD']\n",
    "\n",
    "mapeo = {}\n",
    "\n",
    "cont = 0\n",
    "for key in letters:\n",
    "    vector = torch.zeros(1, 1, len(letters))\n",
    "    vector[0,0,cont] = 1.0\n",
    "    mapeo[key] = vector\n",
    "    cont += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e63141c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapeo['START']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "453a76db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'patch_generator' from '/home/jvelez/Documents/Handwritten_CNN_seq2seq_with_attention/patch_generator.py'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import patch_generator\n",
    "from patch_generator import patch_gen\n",
    "import importlib\n",
    "importlib.reload(patch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4e351a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 48\n",
    "width = 192\n",
    "patch_height = 48\n",
    "patch_width = 10\n",
    "stepsize = 2\n",
    "color_channels = 1\n",
    "batch_size=512\n",
    "MAX_LENGTH = 15\n",
    "n_patches = int((width - patch_width)/stepsize + 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3690b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_set(word_set):    \n",
    "    patches_tensor = torch.empty(batch_size, n_patches, color_channels, patch_height, patch_width)\n",
    "    words = []\n",
    "    cont = 0\n",
    "    for word in word_set:\n",
    "        patch_gen(cont, patches_tensor, word, n_patches, color_channels, patch_height, patch_width, stepsize), \n",
    "        words.append(word)\n",
    "        cont += 1\n",
    "        \n",
    "    #patches_tensor = patches_tensor.cuda(0)        \n",
    "    return patches_tensor,words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c708c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_target(labels, seq_len, output_size,batch_size):    \n",
    "    one_hot_target = torch.empty(batch_size, seq_len, output_size) \n",
    "\n",
    "    for j,word in enumerate(labels):\n",
    "        length = len(word)\n",
    "        one_hot_target[j, 0, :] = mapeo['START']\n",
    "\n",
    "        for k,letter in enumerate(word):\n",
    "            one_hot_target[j, k + 1, :] = mapeo[letter]\n",
    "\n",
    "        one_hot_target[j, length + 1, :] = mapeo['END']\n",
    "        one_hot_target[j, length + 2: seq_len, :] = mapeo['PAD']\n",
    "        \n",
    "    return one_hot_target        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3b2183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_conversion(decoder_output, output_size):\n",
    "    \n",
    "    one_hot_output_letter = torch.zeros(1, 1, output_size).cuda(0)\n",
    "    index = torch.argmax(decoder_output, dim = 2).item()\n",
    "    one_hot_output_letter[0, 0, index] = 1.\n",
    "    \n",
    "    return one_hot_output_letter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6bed8c",
   "metadata": {},
   "source": [
    "# Definiendo la arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54625c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTROS_EN_CNN_1 = 4\n",
    "NEURONS_IN_DENSE_LAYER = 1024\n",
    "PATCH_HEIGHT_AFTER_POOLING = patch_height//2\n",
    "PATCH_WIDTH_AFTER_POOLING = patch_width//2\n",
    "kernel_size = 1\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=FILTROS_EN_CNN_1, kernel_size=kernel_size, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(PATCH_HEIGHT_AFTER_POOLING*PATCH_WIDTH_AFTER_POOLING*FILTROS_EN_CNN_1, NEURONS_IN_DENSE_LAYER)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = F.relu((self.conv1(X)))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, PATCH_HEIGHT_AFTER_POOLING*PATCH_WIDTH_AFTER_POOLING*FILTROS_EN_CNN_1)\n",
    "        X = self.fc1(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a7997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, input, hidden, batch_size, seq_len):        \n",
    "        output = input.view(batch_size, seq_len, self.input_size)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aeaeda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, batch_first = True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 2)\n",
    "        # dim = 2 porque esta última dimensión es la correspondiente a output_size, que es sobre\n",
    "        # la que queremos hacer el softmax\n",
    "\n",
    "    def forward(self, input, hidden, batch_size, seq_len):        \n",
    "        output = input.view(batch_size, seq_len, self.output_size)\n",
    "        #output = F.relu(output) # la relu se metía aquí porque en el\n",
    "        #caso NLP del ejemplo de PyTorch previamente había una capa de embedding\n",
    "        #No nos hace falta porque nuestro tensor de inputs ya es one-hot\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "               torch.zeros(1, batch_size, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84e93a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "encoder_input_size = 1024\n",
    "hidden_size = 256\n",
    "output_size = len(letters)\n",
    "\n",
    "CNN_model = ConvolutionalNetwork().cuda(0)\n",
    "CNN_optimizer = torch.optim.Adam(CNN_model.parameters())\n",
    "\n",
    "Encoder_model = EncoderRNN(input_size = encoder_input_size, hidden_size = hidden_size).cuda(0)\n",
    "Encoder_optimizer = optim.SGD(Encoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "Decoder_model = DecoderRNN(hidden_size = hidden_size, output_size = output_size).cuda(0)\n",
    "Decoder_optimizer = optim.SGD(Decoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46937e74",
   "metadata": {},
   "source": [
    "## Entrenando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e09123b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0527201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(decoder_output, ground_truth):\n",
    "    loss = 0\n",
    "        \n",
    "    for j in range(batch_size):\n",
    "        loss += criterion(decoder_output[j], ground_truth[j])               \n",
    "    \n",
    "    loss = loss/batch_size\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6782bd3-1f3a-44ad-b116-694357416995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genera_palabras_al_azar():\n",
    "    list_random_strings = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        list_random_strings.append(''.join(random.choice(string.ascii_lowercase) for _ in range(random.randint(1,MAX_LENGTH))))\n",
    "    return list_random_strings\n",
    "\n",
    "train_losses = []\n",
    "list_random_strings = genera_palabras_al_azar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a291fb31-a234-4a0b-bd1c-0b034f96f6ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'height' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0a36504c8335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplete_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_random_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tiempo:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-02cc10b667c0>\u001b[0m in \u001b[0;36mcomplete_set\u001b[0;34m(word_set)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mpatch_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatches_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_patches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcont\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Handwritten_CNN_seq2seq_with_attention/patch_generator.py\u001b[0m in \u001b[0;36mpatch_gen\u001b[0;34m(num_batch, patches_tensor, word, n_patches, color_channels, patch_height, patch_width, stepsize)\u001b[0m\n\u001b[1;32m      7\u001b[0m         thickness = 1, lineType = cv2.LINE_AA)    \n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpatch_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatches_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_patches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrear_imagen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Handwritten_CNN_seq2seq_with_attention/patch_generator.py\u001b[0m in \u001b[0;36mcrear_imagen\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcrear_imagen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     image = cv2.putText(image, text = word, org = (5, 30),\n\u001b[1;32m      6\u001b[0m         \u001b[0mfontFace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontScale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.62\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'height' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "train_set = complete_set(list_random_strings)\n",
    "print(\"Tiempo:\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f79be58-3321-436c-923a-9d5ceede7402",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size = batch_size)\n",
    "for b, (image, labels) in enumerate(train_loader):\n",
    "    print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd1cbc-91bf-4f95-85df-143b3bdd2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b675fc48-73a0-4013-bbc9-7cff838c4dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    train_losses = []\n",
    "    \n",
    "    for num_batch in range(0, 8):    \n",
    "        list_random_strings = genera_palabras_al_azar()\n",
    "        \n",
    "        image,labels = complete_set(list_random_strings)\n",
    "\n",
    "        encoder_hidden = Encoder_model.initHidden(batch_size)\n",
    "\n",
    "        image_cnn = image.view(-1, color_channels, patch_height, patch_width).cuda(0)\n",
    "        encoder_input = CNN_model(image_cnn)\n",
    "        encoder_output, encoder_hidden = Encoder_model(encoder_input, encoder_hidden, batch_size = batch_size, seq_len = n_patches)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = get_one_hot_target(labels=labels, batch_size = batch_size, seq_len = MAX_LENGTH + 2,output_size = output_size).cuda(0)\n",
    "        decoder_output, decoder_hidden = Decoder_model(decoder_input, decoder_hidden, batch_size = batch_size, seq_len = MAX_LENGTH + 2)\n",
    "\n",
    "        output_indices = torch.tensor(list(range(0, MAX_LENGTH + 2 -1))).cuda(0) # removing last token from the output\n",
    "        decoder_output = torch.index_select(decoder_output, dim = 1, index = output_indices)\n",
    "\n",
    "        ground_truth = torch.argmax(decoder_input, dim = 2)\n",
    "        target_indices = torch.tensor(list(range(1, MAX_LENGTH + 2))).cuda(0) # remove SOS token from the input\n",
    "        ground_truth = torch.index_select(ground_truth, dim = 1, index = target_indices)\n",
    "\n",
    "        loss = calculate_loss(decoder_output,ground_truth)\n",
    "\n",
    "        CNN_optimizer.zero_grad()\n",
    "        Encoder_optimizer.zero_grad()\n",
    "        Decoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        CNN_optimizer.step()\n",
    "        Encoder_optimizer.step()\n",
    "        Decoder_optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        print(num_batch,)\n",
    "            \n",
    "    return np.mean(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d30ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    \n",
    "    valid_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for num_batch in range(0, 2):\n",
    "            list_random_strings = genera_palabras_al_azar()        \n",
    "\n",
    "            image_val,label_val = complete_set(list_random_strings)\n",
    "\n",
    "            encoder_hidden_val = Encoder_model.initHidden(batch_size = batch_size)\n",
    "            image_cnn_val = image_val.view(-1, color_channels, patch_height, patch_width).cuda(0)\n",
    "            encoder_input_val = CNN_model(image_cnn_val)\n",
    "            encoder_output_val, encoder_hidden_val = Encoder_model(encoder_input_val, encoder_hidden_val, batch_size = batch_size, seq_len = n_patches)\n",
    "\n",
    "            decoder_hidden_val = encoder_hidden_val\n",
    "            decoder_input_val = get_one_hot_target(labels=label_val, batch_size = batch_size, seq_len = MAX_LENGTH + 2,output_size = output_size).cuda(0)\n",
    "            decoder_output_val, decoder_hidden_val = Decoder_model(decoder_input_val, decoder_hidden_val,batch_size = batch_size, seq_len = MAX_LENGTH + 2)\n",
    "\n",
    "            output_indices_val = torch.tensor(list(range(0, MAX_LENGTH + 1))).cuda(0) # remove last token from the output\n",
    "            decoder_output_val = torch.index_select(decoder_output_val, dim = 1, index = output_indices_val)\n",
    "\n",
    "            ground_truth_val = torch.argmax(decoder_input_val, dim = 2)\n",
    "            target_indices_val = torch.tensor(list(range(1, MAX_LENGTH + 2))).cuda(0) # remove START token from the input\n",
    "            ground_truth_val = torch.index_select(ground_truth_val, dim = 1, index = target_indices_val)\n",
    "\n",
    "            loss = calculate_loss(decoder_output_val,ground_truth_val)\n",
    "            valid_losses.append(loss.item())\n",
    "            print(num_batch,)\n",
    "                \n",
    "        return np.mean(valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52e4461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patience():\n",
    "    \n",
    "    def __init__(self, patience):\n",
    "        self.patience = patience\n",
    "        self.current_patience = patience\n",
    "        self.min_loss_val = float('inf')\n",
    "\n",
    "    def more_patience(self,loss_val):\n",
    "        self.current_patience -= 1\n",
    "        if self.current_patience == 0:\n",
    "            return False\n",
    "\n",
    "        if loss_val < self.min_loss_val:\n",
    "            self.min_loss_val = loss_val\n",
    "            self.current_patience = patience\n",
    "\n",
    "            model_name = f\"{height}x{width}_by{patch_width}_jump{stepsize}_batch{batch_size} NN_{FILTROS_EN_CNN_1}_{NEURONS_IN_DENSE_LAYER}_{kernel_size}_{hidden_size}_INFINITE_RANDOM_SAMPLE\"\n",
    "            print(\", saved best model.\")\n",
    "            \n",
    "            torch.save(CNN_model.state_dict(), 'CNN_'+model_name)\n",
    "            torch.save(Encoder_model.state_dict(), 'Encoder_'+model_name)\n",
    "            torch.save(Decoder_model.state_dict(), 'Decoder_'+model_name)\n",
    "    \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5886fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "patience = 200\n",
    "patience_controler = Patience(patience)\n",
    "start_time = time.time()\n",
    "\n",
    "for num_epoch in range(500000):\n",
    "    train_loss = train()        \n",
    "    valid_loss = validation()\n",
    "    \n",
    "    writer.add_scalar('Loss/train', train_loss, num_epoch)\n",
    "    writer.add_scalar('Loss/validation', valid_loss, num_epoch)\n",
    "    \n",
    "    print(f'Epoch: {num_epoch} Train loss: {train_loss} Valid loss: {valid_loss} Duration: {(time.time() - start_time)/60} minutes',)\n",
    "\n",
    "    if not patience_controler.more_patience(valid_loss):\n",
    "        print(\"Se acabó la paciencia\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf79c2",
   "metadata": {},
   "source": [
    "## Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model.load_state_dict(torch.load('CNN_model_30000_words_TF_PAD_noise.pt'))\n",
    "CNN_model.eval()\n",
    "\n",
    "Encoder_model.load_state_dict(torch.load('Encoder_model_30000_words_TF_PAD_noise.pt'))\n",
    "Encoder_model.eval()\n",
    "\n",
    "Decoder_model.load_state_dict(torch.load('Decoder_model_30000_words_TF_PAD_noise.pt'))\n",
    "Decoder_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9270ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test():\n",
    "    with torch.no_grad():\n",
    "        for num_batch, (image_test, label_test) in enumerate(test_loader):\n",
    "            num_batch += 1\n",
    "            encoder_hidden_test = Encoder_model.initHidden(batch_size = batch_size)\n",
    "            image_cnn_test = image_test.view(-1, color_channels, patch_height, patch_width).cuda(0)\n",
    "            encoder_input_test = CNN_model(image_cnn_test)\n",
    "            encoder_output, encoder_hidden_test = Encoder_model(encoder_input_test, encoder_hidden_test, batch = test_batch, seq_len = n_patches)\n",
    "\n",
    "            #decoder_hidden_test = (encoder_hidden_test[0][0, :, :].view(1, batch_size, hidden_size), # We take the last hidden state of the Encoder \n",
    "            #                       encoder_hidden_test[1][0, :, :].view(1, batch_size, hidden_size)) # for each image/word (j) within the batch \n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                decoder_input_test = mapeo['START'].cuda(0) # We initialize the first Decoder input as the START token\n",
    "                decoder_hidden_test = (encoder_hidden_test[0][0, j, :].view(1, 1, hidden_size), # We take the last hidden state of the Encoder \n",
    "                                       encoder_hidden_test[1][0, j, :].view(1, 1, hidden_size)) # for each image/word (j) within the batch \n",
    "                \n",
    "                for d in range(MAX_LENGTH + 2):\n",
    "                    decoder_output_test, decoder_hidden_test = Decoder_model(decoder_input_test, decoder_hidden_test, batch = 1, seq_len = 1)\n",
    "\n",
    "                    output_letter = one_hot_conversion(decoder_output_test, output_size = output_size)\n",
    "                    decoder_input_test = output_letter\n",
    "                    \n",
    "                    if d == 0:\n",
    "                        output_word = output_letter\n",
    "                    else:\n",
    "                        output_word = torch.cat((output_word, output_letter), dim = 1).cuda(0)\n",
    "                    \n",
    "                    if torch.equal(output_letter, letter_to_vector('END').cuda(0)):\n",
    "                        break\n",
    "                output_word = torch.argmax(output_word, dim=2)\n",
    "                output_word = output_word.view(output_word.numel()) # view as a rank-1 tensor\n",
    "\n",
    "                model_word = []\n",
    "                for item in output_word:\n",
    "                    model_word.append(letters[item])\n",
    "\n",
    "                model_word = ''.join(model_word[:-1])\n",
    "                print(model_word)\n",
    "            print(test_set) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d215e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40956edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6_cv2",
   "language": "python",
   "name": "python3.6_cv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
