{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1621871458734,
     "user": {
      "displayName": "Álvaro Barreiro Garrido",
      "photoUrl": "https://lh6.googleusercontent.com/-MyFwdJKljQM/AAAAAAAAAAI/AAAAAAAAAx8/dul-WxxfCro/s64/photo.jpg",
      "userId": "06098121704066317706"
     },
     "user_tz": -120
    },
    "id": "JGgw9kK1YxZh",
    "outputId": "3575aba7-b22e-41fe-b6a0-c5098a8f7858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n",
      "3.6.10\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "import cv2\n",
    "import skimage\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Ignore harmless warnings:\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import platform\n",
    "print(torch.__version__)\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1621871461335,
     "user": {
      "displayName": "Álvaro Barreiro Garrido",
      "photoUrl": "https://lh6.googleusercontent.com/-MyFwdJKljQM/AAAAAAAAAAI/AAAAAAAAAx8/dul-WxxfCro/s64/photo.jpg",
      "userId": "06098121704066317706"
     },
     "user_tz": -120
    },
    "id": "9Mv9x8Rg7Nw3",
    "outputId": "12376843-63d4-4a13-9b63-b587d3378218"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TITAN X (Pascal)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1621871463042,
     "user": {
      "displayName": "Álvaro Barreiro Garrido",
      "photoUrl": "https://lh6.googleusercontent.com/-MyFwdJKljQM/AAAAAAAAAAI/AAAAAAAAAx8/dul-WxxfCro/s64/photo.jpg",
      "userId": "06098121704066317706"
     },
     "user_tz": -120
    },
    "id": "mGCNWlstYxZp"
   },
   "outputs": [],
   "source": [
    "train_set = ['hola', 'urjc', 'gavab', 'estoril', 'alvaro', 'victoria'] * 512\n",
    "import random\n",
    "random.seed(123)\n",
    "random.shuffle(train_set)\n",
    "\n",
    "val_set = ['hola', 'urjc', 'gavab', 'estoril', 'alvaro', 'victoria'] * 32\n",
    "\n",
    "len_train = len(train_set)\n",
    "len_val = len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1621871465507,
     "user": {
      "displayName": "Álvaro Barreiro Garrido",
      "photoUrl": "https://lh6.googleusercontent.com/-MyFwdJKljQM/AAAAAAAAAAI/AAAAAAAAAx8/dul-WxxfCro/s64/photo.jpg",
      "userId": "06098121704066317706"
     },
     "user_tz": -120
    },
    "id": "LHJugRyW6tSY",
    "outputId": "1a3f41ba-2e8b-4d1d-8683-de4e5ce0f547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# Function to convert letters (and therefore words) into PyTorch tensors:\n",
    "\n",
    "letters = ['SOS_token', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "          'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'EOS_token']\n",
    "\n",
    "def letter_to_vector(letter):\n",
    "    vector = torch.zeros(1, 1, len(letters))\n",
    "    for i in range(len(letters)):\n",
    "        if letters[i] == letter:\n",
    "            vector[0, 0, i] = 1.\n",
    "    return(vector)\n",
    "\n",
    "print(letter_to_vector('SOS_token'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1621871467497,
     "user": {
      "displayName": "Álvaro Barreiro Garrido",
      "photoUrl": "https://lh6.googleusercontent.com/-MyFwdJKljQM/AAAAAAAAAAI/AAAAAAAAAx8/dul-WxxfCro/s64/photo.jpg",
      "userId": "06098121704066317706"
     },
     "user_tz": -120
    },
    "id": "Wbo4YClf6wnf"
   },
   "outputs": [],
   "source": [
    "# Defining model and architecture:\n",
    "\n",
    "# CONVOLUTIONAL NEURAL NETWORK:\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1, 2) # padding???\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1, 2)\n",
    "        self.fc1 = nn.Linear(12*2*50, 1024)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, 12*2*50) # -1 para no tener que determinar aquí el tamaño del batch (se ajusta, podemos variarlo)\n",
    "        X = F.relu(self.fc1(X))\n",
    "\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1621871470282,
     "user": {
      "displayName": "Álvaro Barreiro Garrido",
      "photoUrl": "https://lh6.googleusercontent.com/-MyFwdJKljQM/AAAAAAAAAAI/AAAAAAAAAx8/dul-WxxfCro/s64/photo.jpg",
      "userId": "06098121704066317706"
     },
     "user_tz": -120
    },
    "id": "hgyZb4xH60aC"
   },
   "outputs": [],
   "source": [
    "# ENCODER:\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = input.view(1,1,-1)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1621871472376,
     "user": {
      "displayName": "Álvaro Barreiro Garrido",
      "photoUrl": "https://lh6.googleusercontent.com/-MyFwdJKljQM/AAAAAAAAAAI/AAAAAAAAAx8/dul-WxxfCro/s64/photo.jpg",
      "userId": "06098121704066317706"
     },
     "user_tz": -120
    },
    "id": "diLfAJEx68zA"
   },
   "outputs": [],
   "source": [
    "# DECODER:\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(output_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = input.view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1621871475224,
     "user": {
      "displayName": "Álvaro Barreiro Garrido",
      "photoUrl": "https://lh6.googleusercontent.com/-MyFwdJKljQM/AAAAAAAAAAI/AAAAAAAAAx8/dul-WxxfCro/s64/photo.jpg",
      "userId": "06098121704066317706"
     },
     "user_tz": -120
    },
    "id": "Wr8q3l1y6_s0"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model, define loss and optimization functions:\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "input_size = 1024\n",
    "hidden_size = 256\n",
    "output_size = 28\n",
    "\n",
    "CNN_model = ConvolutionalNetwork().cuda()\n",
    "CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr = 0.001)\n",
    "\n",
    "Encoder_model = EncoderRNN(input_size = input_size, hidden_size = hidden_size).cuda()\n",
    "Encoder_optimizer = optim.SGD(Encoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "Decoder_model = DecoderRNN(hidden_size = hidden_size, output_size = output_size).cuda()\n",
    "Decoder_optimizer = optim.SGD(Decoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2104101,
     "status": "ok",
     "timestamp": 1621873635464,
     "user": {
      "displayName": "Álvaro Barreiro Garrido",
      "photoUrl": "https://lh6.googleusercontent.com/-MyFwdJKljQM/AAAAAAAAAAI/AAAAAAAAAx8/dul-WxxfCro/s64/photo.jpg",
      "userId": "06098121704066317706"
     },
     "user_tz": -120
    },
    "id": "lZvf1_kc7-1Q",
    "outputId": "213ecac6-ccde-41b0-e3c6-a5f4f9f9f758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "[tensor(1.4109, device='cuda:0', grad_fn=<NllLossBackward>), tensor(1.2808, device='cuda:0', grad_fn=<NllLossBackward>), tensor(1.1014, device='cuda:0', grad_fn=<NllLossBackward>), tensor(1.0337, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.8392, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.6461, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.4395, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.4004, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)]\n",
      "[tensor(1.6229, device='cuda:0'), tensor(1.4912, device='cuda:0'), tensor(1.2559, device='cuda:0'), tensor(1.4414, device='cuda:0'), tensor(1.1153, device='cuda:0'), tensor(0.9347, device='cuda:0'), tensor(0.9579, device='cuda:0'), tensor(1.0567, device='cuda:0'), tensor(0.7460, device='cuda:0'), tensor(0.7820, device='cuda:0'), tensor(0.5298, device='cuda:0'), tensor(0.3874, device='cuda:0'), tensor(0.2931, device='cuda:0'), tensor(0.4108, device='cuda:0'), tensor(0.2896, device='cuda:0'), tensor(0.1773, device='cuda:0'), tensor(0.1415, device='cuda:0'), tensor(0.1149, device='cuda:0'), tensor(0.0982, device='cuda:0'), tensor(0.0860, device='cuda:0'), tensor(0.0762, device='cuda:0'), tensor(0.0681, device='cuda:0'), tensor(0.0615, device='cuda:0'), tensor(0.0559, device='cuda:0'), tensor(0.0511, device='cuda:0'), tensor(0.0470, device='cuda:0'), tensor(0.0435, device='cuda:0'), tensor(0.0404, device='cuda:0'), tensor(0.0377, device='cuda:0'), tensor(0.0352, device='cuda:0')]\n",
      "Duration: 281.39748638073604 minutes\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE MODEL:\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Setting the image parameters:\n",
    "\n",
    "height = 48\n",
    "width = 192\n",
    "\n",
    "# Setting the sliding window parameters:\n",
    "\n",
    "patch_height = 48\n",
    "patch_width = 10\n",
    "stepsize = 2\n",
    "n_patches = int((width - patch_width)/stepsize + 1)\n",
    "\n",
    "# Setting the sliding window parameters:\n",
    "\n",
    "torch.manual_seed(123)\n",
    "epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for j in range(len_train): # we chose individual words/images as batches:\n",
    "        \n",
    "        CNN_optimizer.zero_grad()\n",
    "        Encoder_optimizer.zero_grad()\n",
    "        Decoder_optimizer.zero_grad()\n",
    "        \n",
    "        image = 255 * np.ones(shape = [height, width], dtype = np.uint8)\n",
    "        image = cv2.putText(image, text = train_set[j], org = (5, 30),\n",
    "        fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.7, color = (0, 0, 0),\n",
    "        thickness = 2, lineType = cv2.LINE_AA)\n",
    "        \n",
    "        image = transforms.ToPILImage()(image) # np.ndarray to PIL.Image.Image\n",
    "        \n",
    "        input_word = list(train_set[j])\n",
    "        input_word_length = len(input_word) # number of letters\n",
    "        encoder_hidden = Encoder_model.initHidden()\n",
    "        \n",
    "        \n",
    "        for p in range(n_patches):\n",
    "            \n",
    "            patch = transforms.functional.crop(image, 0, 0 + p * stepsize, patch_height, patch_width) # cropping of the image into patches\n",
    "            patch = transforms.ToTensor()(patch) # torch.Tensor of the patch (normalized)\n",
    "            #patch = skimage.util.random_noise(patch, mode='gaussian') # we set some random noise to the image\n",
    "            #patch = torch.from_numpy(patch) # conversion to pytorch tensor again\n",
    "            patch = 1. - patch # it will work better if we have white text over black background\n",
    "            patch = patch.view(1, 1, 48, 10) # CNN_model expects a 4-dimensional tensor (1 dimension for batch)\n",
    "            patch = patch.type(torch.FloatTensor) # conversion to float\n",
    "            patch = patch.cuda() # set to cuda\n",
    "\n",
    "            encoder_input = CNN_model(patch).cuda() # 1024-length vector associated to patch p (i.e. CNN output, Encoder input)\n",
    "            \n",
    "            _, encoder_hidden = Encoder_model(encoder_input, encoder_hidden)\n",
    "            \n",
    "        decoder_input = letter_to_vector('SOS_token').cuda()\n",
    "        decoder_hidden = encoder_hidden\n",
    "                \n",
    "        \n",
    "        for d in range(input_word_length):\n",
    "            \n",
    "            decoder_output, decoder_hidden = Decoder_model(decoder_input, decoder_hidden)\n",
    "            #decoder_input = letter_to_vector(input_word[d]) # teacher forcing\n",
    "            \n",
    "            one_hot_decoder_output = torch.zeros(1, 1, output_size).cuda()\n",
    "            one_hot_decoder_output[0][0][torch.argmax(decoder_output)] = 1.\n",
    "            decoder_input = one_hot_decoder_output.cuda()\n",
    "            \n",
    "            if torch.equal(one_hot_decoder_output, letter_to_vector('EOS_token').cuda()) == True:\n",
    "                break\n",
    "            \n",
    "            if d == 0:\n",
    "                \n",
    "                output_word = decoder_output \n",
    "                one_hot_input_letter = letter_to_vector(input_word[d]).type(torch.LongTensor)\n",
    "                one_hot_input_word = one_hot_input_letter\n",
    "                index = torch.argmax(one_hot_input_letter.view(output_size))\n",
    "                ground_letter = torch.tensor([index], dtype = torch.long)\n",
    "                ground_word = ground_letter\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                output_word = torch.cat((output_word, decoder_output), dim = 0) # we concatenate the remaining output letters\n",
    "                one_hot_input_letter = letter_to_vector(input_word[d]).type(torch.LongTensor)\n",
    "                one_hot_input_word = torch.cat((one_hot_input_word, one_hot_input_letter), dim = 0)\n",
    "                index = torch.argmax(one_hot_input_letter.view(output_size))\n",
    "                ground_letter = torch.tensor([index], dtype = torch.long)\n",
    "                ground_word = torch.cat((ground_word, ground_letter), dim = 0)\n",
    "\n",
    "                \n",
    "        one_hot_input_word = one_hot_input_word.view(-1, output_size)\n",
    "        output_word = output_word.view(-1, output_size) \n",
    "        ground_word = ground_word.cuda()\n",
    "        loss = criterion(output_word, ground_word)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        CNN_optimizer.step()\n",
    "        Encoder_optimizer.step()\n",
    "        Decoder_optimizer.step()\n",
    "        \n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    # VALIDATION SET:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for j in range(len_val): # we chose individual words/images as batches:\n",
    "        \n",
    "            image_val = 255 * np.ones(shape = [height, width], dtype = np.uint8)\n",
    "            image_val = cv2.putText(image_val, text = val_set[j], org = (5, 30),\n",
    "            fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.7, color = (0, 0, 0),\n",
    "            thickness = 2, lineType = cv2.LINE_AA)\n",
    "\n",
    "            image_val = transforms.ToPILImage()(image_val) # np.ndarray to PIL.Image.Image\n",
    "\n",
    "            input_word_val = list(val_set[j])\n",
    "            input_word_val_length = len(input_word_val) # number of letters\n",
    "\n",
    "            encoder_hidden_val = Encoder_model.initHidden()\n",
    "        \n",
    "        \n",
    "            for p in range(n_patches):\n",
    "\n",
    "                patch_val = transforms.functional.crop(image_val, 0, 0 + p * stepsize, patch_height, patch_width) # cropping of the image into patches\n",
    "                patch_val = transforms.ToTensor()(patch_val) # torch.Tensor of the patch (normalized)\n",
    "                #patch_val = skimage.util.random_noise(patch_val, mode='gaussian') # we set some random noise to the image\n",
    "                #patch_val = torch.from_numpy(patch_val) # conversion to pytorch tensor again\n",
    "                patch_val = 1. - patch_val # it will work better if we have white text over black background\n",
    "                patch_val = patch_val.view(1, 1, 48, 10) # CNN_model expects a 4-dimensional tensor (1 dimension for batch)\n",
    "                patch_val = patch_val.type(torch.FloatTensor) # conversion to float\n",
    "                patch_val = patch_val.cuda() # set to cuda\n",
    "\n",
    "                encoder_input_val = CNN_model(patch_val).cuda() # 1024-length vector associated to patch p (i.e. CNN output, Encoder input)\n",
    "\n",
    "                _, encoder_hidden_val = Encoder_model(encoder_input_val, encoder_hidden_val)\n",
    "\n",
    "            decoder_input_val = letter_to_vector('SOS_token').cuda()\n",
    "            decoder_hidden_val = encoder_hidden_val\n",
    "\n",
    "\n",
    "            for d in range(input_word_val_length):\n",
    "\n",
    "                decoder_output_val, decoder_hidden_val = Decoder_model(decoder_input_val, decoder_hidden_val)\n",
    "                #decoder_input = letter_to_vector(input_word[d])\n",
    "\n",
    "                one_hot_decoder_output_val = torch.zeros(1, 1, output_size).cuda()\n",
    "                one_hot_decoder_output_val[0][0][torch.argmax(decoder_output_val)] = 1.\n",
    "                decoder_input_val = one_hot_decoder_output_val.cuda()\n",
    "\n",
    "                if torch.equal(one_hot_decoder_output_val, letter_to_vector('EOS_token').cuda()) == True:\n",
    "                    break\n",
    "\n",
    "                if d == 0:\n",
    "\n",
    "                    output_word_val = decoder_output_val\n",
    "                    one_hot_input_letter_val = letter_to_vector(input_word_val[d]).type(torch.LongTensor)\n",
    "                    one_hot_input_word_val = one_hot_input_letter_val\n",
    "                    index_val = torch.argmax(one_hot_input_letter_val.view(output_size))\n",
    "                    ground_letter_val = torch.tensor([index_val], dtype = torch.long)\n",
    "                    ground_word_val = ground_letter_val\n",
    "\n",
    "                else:\n",
    "\n",
    "                    output_word_val = torch.cat((output_word_val, decoder_output_val), dim = 0) # we concatenate the remaining output letters\n",
    "                    one_hot_input_letter_val = letter_to_vector(input_word_val[d]).type(torch.LongTensor)\n",
    "                    one_hot_input_word_val = torch.cat((one_hot_input_word_val, one_hot_input_letter_val), dim = 0)\n",
    "                    index_val = torch.argmax(one_hot_input_letter_val.view(output_size))\n",
    "                    ground_letter_val = torch.tensor([index_val], dtype = torch.long)\n",
    "                    ground_word_val = torch.cat((ground_word_val, ground_letter_val), dim = 0)\n",
    "\n",
    "            one_hot_input_word_val = one_hot_input_word_val.view(-1, output_size)\n",
    "            output_word_val = output_word_val.view(-1, output_size) \n",
    "            ground_word_val = ground_word_val.cuda()\n",
    "            loss_val = criterion(output_word_val, ground_word_val)\n",
    "\n",
    "        val_losses.append(loss_val)\n",
    "    \n",
    "    print(i)\n",
    "print(train_losses)\n",
    "print(val_losses)\n",
    "print(f'Duration: {(time.time() - start_time)/60} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1621873726919,
     "user": {
      "displayName": "Álvaro Barreiro Garrido",
      "photoUrl": "https://lh6.googleusercontent.com/-MyFwdJKljQM/AAAAAAAAAAI/AAAAAAAAAx8/dul-WxxfCro/s64/photo.jpg",
      "userId": "06098121704066317706"
     },
     "user_tz": -120
    },
    "id": "ljL7hrKCQVTc",
    "outputId": "7c01b5d4-7a44-46bb-b61d-2ab9c4255fbb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.yscale(\"log\")\n",
    "plt.plot(train_losses, label='training loss')\n",
    "plt.plot(val_losses, label='validation loss')\n",
    "plt.title('Losses per epoch CNN-S2S 6 words')\n",
    "plt.legend();\n",
    "plt.savefig('6 words losses log lr 0,001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1621873802188,
     "user": {
      "displayName": "Álvaro Barreiro Garrido",
      "photoUrl": "https://lh6.googleusercontent.com/-MyFwdJKljQM/AAAAAAAAAAI/AAAAAAAAAx8/dul-WxxfCro/s64/photo.jpg",
      "userId": "06098121704066317706"
     },
     "user_tz": -120
    },
    "id": "SIYn5gtT8Ee-",
    "outputId": "23497762-8ef7-4652-fe13-e322ad3c0d8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "victoria\n",
      "Duration: 0.0017043749491373699 minutes\n"
     ]
    }
   ],
   "source": [
    "# EVALUATING THE MODEL:\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Setting the image parameters:\n",
    "\n",
    "height = 48\n",
    "width = 192\n",
    "\n",
    "# Setting the sliding window parameters:\n",
    "\n",
    "patch_height = 48\n",
    "patch_width = 10 \n",
    "stepsize = 2\n",
    "n_patches = int((width - patch_width)/2 + 1)\n",
    "\n",
    "input_word_test = 'victoria'\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "image_test = 255 * np.ones(shape=[height, width], dtype=np.uint8)\n",
    "image_test = cv2.putText(image_test, text = input_word_test, org=(5, 30),\n",
    "fontFace= cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, color=(0,0,0),\n",
    "thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "image_test = transforms.ToPILImage()(image_test) # np.ndarray to PIL.Image.Image\n",
    "\n",
    "input_word_test = list(input_word_test)\n",
    "input_word_test_length = len(input_word_test) # number of letters\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    encoder_hidden_test = Encoder_model.initHidden()\n",
    "\n",
    "    for p in range(n_patches):\n",
    "\n",
    "        patch_test = transforms.functional.crop(image_test, 0, 0 + p * stepsize, patch_height, patch_width) # cropping of the image into patches\n",
    "        patch_test = transforms.ToTensor()(patch_test) # torch.Tensor of the patch (normalized)\n",
    "        #patch_test = skimage.util.random_noise(patch_test, mode='gaussian')\n",
    "        #patch_test = torch.from_numpy(patch_test)\n",
    "        patch_test = 1. - patch_test\n",
    "        patch_test = patch_test.view(1, 1, 48, 10) # CNN_model expects a 4-dimensional tensor (1 dimension for batch)\n",
    "        patch_test = patch_test.type(torch.FloatTensor)\n",
    "        patch_test = patch_test.cuda()\n",
    "\n",
    "        encoder_input_test = CNN_model(patch_test).cuda() # 1024-length vector associated to patch p (i.e. CNN output, Encoder input)\n",
    "\n",
    "        _, encoder_hidden_test = Encoder_model(encoder_input_test, encoder_hidden_test)\n",
    "\n",
    "    decoder_input_test = letter_to_vector('SOS_token').cuda()\n",
    "    decoder_hidden_test = encoder_hidden_test\n",
    "\n",
    "\n",
    "    for d in range(input_word_test_length):\n",
    "\n",
    "        decoder_output_test, decoder_hidden_test = Decoder_model(decoder_input_test, decoder_hidden_test)\n",
    "        #decoder_input = letter_to_vector(input_word[d])\n",
    "\n",
    "        one_hot_decoder_output_test = torch.zeros(1, 1, output_size).cuda()\n",
    "        one_hot_decoder_output_test[0][0][torch.argmax(decoder_output_test)] = 1.\n",
    "        decoder_input_test = one_hot_decoder_output_test.cuda()\n",
    "\n",
    "        if torch.equal(one_hot_decoder_output_test, letter_to_vector('EOS_token').cuda())==True:\n",
    "            break\n",
    "\n",
    "        if d == 0:\n",
    "\n",
    "            output_word_test = decoder_output_test\n",
    "            one_hot_input_letter_test = letter_to_vector(input_word_test[d]).type(torch.LongTensor)\n",
    "            one_hot_input_word_test = one_hot_input_letter_test\n",
    "            index_test = torch.argmax(one_hot_input_letter_test.view(output_size))\n",
    "            ground_letter_test = torch.tensor([index_test], dtype = torch.long)\n",
    "            ground_word_test = ground_letter_test\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            output_word_test = torch.cat((output_word_test, decoder_output_test), dim = 0) # we concatenate the remaining output letters\n",
    "            one_hot_input_letter_test = letter_to_vector(input_word_test[d]).type(torch.LongTensor)\n",
    "            one_hot_input_word_test = torch.cat((one_hot_input_word_test, one_hot_input_letter_test), dim = 0)\n",
    "            index_test = torch.argmax(one_hot_input_letter_test.view(output_size))\n",
    "            ground_letter_test = torch.tensor([index_test], dtype = torch.long)\n",
    "            ground_word_test = torch.cat((ground_word_test, ground_letter_test), dim = 0)\n",
    "\n",
    "\n",
    "    one_hot_input_word_test = one_hot_input_word_test.view(-1, output_size)\n",
    "    output_word_test = output_word_test.view(-1, output_size) \n",
    "    ground_word_test = ground_word_test.cuda()\n",
    "\n",
    "\n",
    "model_word = []\n",
    "\n",
    "indices = torch.argmax(output_word_test, dim = 1)\n",
    "for i in range(indices.numel()):\n",
    "    model_word.append(letters[indices[i]])\n",
    "\n",
    "model_word = ''.join(model_word)    \n",
    "print(model_word)\n",
    "\n",
    "        \n",
    "print(f'Duration: {(time.time() - start_time)/60} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "QEsjkoWm7H-7"
   },
   "outputs": [],
   "source": [
    "torch.save(CNN_model.state_dict(), 'CNN_model_6_words.pt')\n",
    "torch.save(Encoder_model.state_dict(), 'Encoder_model_6_words.pt')\n",
    "torch.save(Decoder_model.state_dict(), 'Decoder_model_6_words.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_estoril",
   "language": "python",
   "name": "pytorch_estoril"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
