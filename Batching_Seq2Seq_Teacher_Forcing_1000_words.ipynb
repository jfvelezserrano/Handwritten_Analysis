{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "3.6.10\n"
     ]
    }
   ],
   "source": [
    "# IMPORTING THE REQUIRED LIBRARIES:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "import cv2\n",
    "import skimage\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Ignore harmless warnings:\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import platform\n",
    "print(torch.__version__)\n",
    "print(platform.python_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TITAN X (Pascal)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA: 1000 MOST COMMON ENGLISH WORDS\n",
    "\n",
    "top_1000_words = pd.read_csv('top_1000.txt', delimiter = '\\t', header = None)\n",
    "\n",
    "words = []\n",
    "sample_length = len(top_1000_words)\n",
    "\n",
    "for i in range(sample_length):\n",
    "    words.append(top_1000_words.loc[i][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "['oxygen', 'sugar', 'death', 'pretty', 'skill', 'women', 'season', 'solution', 'magnet', 'silver', 'thank', 'branch', 'match', 'suffix', 'especially', 'fig', 'afraid', 'huge', 'sister', 'steel', 'discuss', 'forward', 'similar', 'guide', 'experience', 'score', 'apple', 'bought', 'led', 'pitch', 'coat', 'mass', 'card', 'band', 'rope', 'slip', 'win', 'dream', 'evening', 'condition', 'feed', 'tool', 'total', 'basic', 'smell', 'valley', 'nor', 'double', 'seat', 'arrive', 'master', 'track', 'parent', 'shore', 'division', 'sheet', 'substance', 'favor', 'connect', 'post', 'spend', 'chord', 'fat', 'glad', 'original', 'share', 'station', 'dad', 'bread', 'charge', 'proper', 'bar', 'offer', 'segment', 'slave', 'duck', 'instant', 'market', 'degree', 'populate', 'chick', 'dear', 'enemy', 'reply', 'drink', 'occur', 'support', 'speech', 'nature', 'range', 'steam', 'motion', 'path', 'liquid', 'log', 'meant', 'quotient', 'teeth', 'shell', 'neck']\n"
     ]
    }
   ],
   "source": [
    "# DEFINING TRAINING, VALIDATION AND TEST SETS\n",
    "\n",
    "train_set = words[0:800] * 32\n",
    "val_set = words[800:900] \n",
    "test_set = words[900:1000] \n",
    "\n",
    "MAX_LENGTH = max(len(list(word)) for word in train_set) # length of the longest word within our sample\n",
    "\n",
    "print(MAX_LENGTH)\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0.]]])\n",
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# ONE HOT ENCODING OF THE ALPHABET (+ START, END & PAD)\n",
    "\n",
    "letters = ['START', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "          'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'END']\n",
    "\n",
    "PAD = torch.zeros(1, 1, 28)\n",
    "\n",
    "def letter_to_vector(letter):\n",
    "    vector = torch.zeros(1, 1, len(letters))\n",
    "    for i in range(len(letters)):\n",
    "        if letters[i] == letter:\n",
    "            vector[0, 0, i] = 1.\n",
    "    return(vector)\n",
    "\n",
    "print(PAD)\n",
    "print(letter_to_vector('START'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING PATCH TENSOR FOR EACH WORD (PATCH, COLOR CHANNEL, HEIGHT, WIDTH)\n",
    "\n",
    "def patch_gen(word, n_patches, patch_height, patch_width, stepsize):\n",
    "    \n",
    "    image = 255 * np.ones(shape = [height, width], dtype = np.uint8)\n",
    "    image = cv2.putText(image, text = word, org = (5, 30),\n",
    "        fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.7, color = (0, 0, 0),\n",
    "        thickness = 1, lineType = cv2.LINE_AA)\n",
    "    image = transforms.ToPILImage()(image) # np.ndarray to PIL.Image.Image\n",
    "    patches_tensor = torch.empty(n_patches, color_channels, patch_height, patch_width)\n",
    "    \n",
    "    for p in range(n_patches):\n",
    "        \n",
    "        patch = transforms.functional.crop(image, 0, 0 + p * stepsize, patch_height, patch_width) # cropping of the image into patches\n",
    "        patch = transforms.ToTensor()(patch) # torch.Tensor of the patch (normalized)\n",
    "        #patch = skimage.util.random_noise(patch, mode='s&p') # we set some random noise to the image\n",
    "        #patch = torch.from_numpy(patch) # conversion to pytorch tensor again\n",
    "        patch = 1. - patch # it will work better if we have white text over black background\n",
    "        patch = patch.view(1, 1, patch_height, patch_width) # CNN_model expects a 4-dimensional tensor (1 dimension for batch)\n",
    "        patch = patch.type(torch.FloatTensor) # conversion to float\n",
    "        patch = patch.cuda(1) # set to cuda\n",
    "        patches_tensor[p, 0, :, :] = patch\n",
    "        patches_tensor = patches_tensor.cuda(1)\n",
    "        \n",
    "    return patches_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING IMAGE AND SLIDING WINDOW (PATCH) PARAMETERS\n",
    "\n",
    "height = 48\n",
    "width = 192\n",
    "patch_height = 48\n",
    "patch_width = 10\n",
    "stepsize = 2\n",
    "color_channels = 1\n",
    "n_patches = int((width - patch_width)/stepsize + 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING TUPLE (TENSOR WITH PATCHES, LABEL) FOR EACH WORD OF A GIVEN SET:\n",
    "\n",
    "def complete_set(which_set):\n",
    "    \n",
    "    complete_set = []\n",
    "    \n",
    "    for word in which_set:\n",
    "        \n",
    "        complete_set.append((patch_gen(word, n_patches, patch_height, patch_width, stepsize), word))\n",
    "        \n",
    "    return complete_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUPLE OF (RANK-4 TENSOR [PATCH, CHANNEL, HEIGHT, WIDTH], LABEL) FOR EVERY WORD IN EVERY SET\n",
    "\n",
    "comp_train_set = complete_set(which_set = train_set)\n",
    "comp_val_set = complete_set(which_set = val_set)\n",
    "comp_test_set = complete_set(which_set = test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 92, 1, 48, 10])\n",
      "torch.Size([1472, 1, 48, 10])\n",
      "('war', 'sleep', 'stick', 'paper', 'page', 'floor', 'not', 'special', 'hunt', 'food', 'measure', 'equal', 'soon', 'ride', 'tall', 'stone')\n"
     ]
    }
   ],
   "source": [
    "# LOADING DATA AND SETS IN BATCHES\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(comp_train_set, batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(comp_val_set, batch_size = batch_size, shuffle = False)\n",
    "test_loader = DataLoader(comp_test_set, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "\n",
    "for image, label in train_loader:\n",
    "    break\n",
    "    \n",
    "image_cnn = image.view(-1, color_channels, patch_height, patch_width)   \n",
    "print(image.shape)\n",
    "print(image_cnn.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING ONE HOT TENSOR OF THE TARGET WORD\n",
    "\n",
    "    # one hot tensor of the input batch of words for the Decoder:\n",
    "    # 1 dim for position within batch, 1 dim for position of the letter in the fixed sequence (MAX_LENGTH + START + END),\n",
    "    # 1 final dim specifying letter (output_size = 26 + START and END tokens)\n",
    "    # torch.zeros() ensures that we always have PAD token vectors in case our target word is not MAX_LENGTH long\n",
    "    # WARNING: only works if label contains 2 words or more (it is OK as long as we are doing batching)\n",
    "    \n",
    "def get_one_hot_target(label, batch, seq_len, output_size):\n",
    "    \n",
    "    one_hot_target = torch.zeros(batch, seq_len, output_size) \n",
    "\n",
    "    for j in range(batch): # for each word of the batch\n",
    "\n",
    "        length = len(list(label[j])) # we compute the number of letters\n",
    "\n",
    "        one_hot_target[j, 0, :] = letter_to_vector('START') # the first letter of every word will always be the START\n",
    "\n",
    "        for k in range(0, length): # now for each letter of the target word\n",
    "\n",
    "            target_letter = list(label[j])[k] # picks the 'k' target letter \n",
    "            one_hot_target[j, k + 1, :] = letter_to_vector(target_letter) # adds that one hot target letter to our global tensor\n",
    "\n",
    "        one_hot_target[j, length + 1, :] = letter_to_vector('END') # we put END after the last letter of the word\n",
    "\n",
    "    return one_hot_target     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EARLY STOPPING FUNCTION\n",
    "\n",
    "# If our validation loss has not improved for a certain patience 'n', we stop the training\n",
    "\n",
    "def Early_Stopping(n, losses):\n",
    "    previous_losses = torch.tensor(losses[-(n + 1):-1]).cuda(1) # n previous values of the validation loss\n",
    "    last_loss = losses[-1] # last value of the validation loss\n",
    "    comparison = last_loss > previous_losses # n-length boolean vector\n",
    "    comparison = all(comparison) # = True if all previous values of the validation loss are smaller than the last one\n",
    "    \n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING MODEL AND ARCHITECTURE\n",
    "\n",
    "# CONVOLUTIONAL NEURAL NETWORK:\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1, 2) # padding???\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1, 2)\n",
    "        self.fc1 = nn.Linear(12*2*50, 1024)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, 12*2*50) # -1 para no tener que determinar aquí el tamaño del batch (se ajusta, podemos variarlo)\n",
    "        X = F.relu(self.fc1(X))\n",
    "\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER:\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, input, hidden, batch, seq_len):\n",
    "        \n",
    "        output = input.view(batch, seq_len, input_size)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch):\n",
    "        return (torch.zeros(1, batch, self.hidden_size, device=device),\n",
    "                torch.zeros(1, batch, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECODER:\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, batch_first = True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 2) \n",
    "        # dim = 2 porque esta última dimensión es la correspondiente a output_size, que es sobre\n",
    "        # la que queremos hacer el softmax\n",
    "\n",
    "    def forward(self, input, hidden, batch, seq_len):\n",
    "        \n",
    "        output = input.view(batch, seq_len, output_size)\n",
    "        #output = F.relu(output) # la relu se metía aquí porque en el\n",
    "        #caso NLP del ejemplo de PyTorch previamente había una capa de embedding\n",
    "        #No nos hace falta porque nuestro tensor de inputs ya es one-hot\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch):\n",
    "        return (torch.zeros(1, batch, self.hidden_size, device=device),\n",
    "               torch.zeros(1, batch, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_size = 1024\n",
    "hidden_size = 256\n",
    "output_size = 28\n",
    "\n",
    "CNN_model = ConvolutionalNetwork().cuda(1)\n",
    "CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr = 0.001)\n",
    "\n",
    "Encoder_model = EncoderRNN(input_size = input_size, hidden_size = hidden_size).cuda(1)\n",
    "Encoder_optimizer = optim.SGD(Encoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "Decoder_model = DecoderRNN(hidden_size = hidden_size, output_size = output_size).cuda(1)\n",
    "Decoder_optimizer = optim.SGD(Decoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Epoch: 101\n",
      "Epoch: 102\n",
      "Epoch: 103\n",
      "Epoch: 104\n",
      "Epoch: 105\n",
      "Epoch: 106\n",
      "Epoch: 107\n",
      "Epoch: 108\n",
      "Epoch: 109\n",
      "Epoch: 110\n",
      "Epoch: 111\n",
      "Epoch: 112\n",
      "Epoch: 113\n",
      "Epoch: 114\n",
      "Epoch: 115\n",
      "Epoch: 116\n",
      "Epoch: 117\n",
      "Epoch: 118\n",
      "Epoch: 119\n",
      "Epoch: 120\n",
      "Epoch: 121\n",
      "Epoch: 122\n",
      "Epoch: 123\n",
      "Epoch: 124\n",
      "Epoch: 125\n",
      "Epoch: 126\n",
      "Epoch: 127\n",
      "Epoch: 128\n",
      "Epoch: 129\n",
      "Epoch: 130\n",
      "Epoch: 131\n",
      "Epoch: 132\n",
      "Epoch: 133\n",
      "Epoch: 134\n",
      "Epoch: 135\n",
      "Epoch: 136\n",
      "Epoch: 137\n",
      "Epoch: 138\n",
      "Epoch: 139\n",
      "Epoch: 140\n",
      "Epoch: 141\n",
      "Epoch: 142\n",
      "Epoch: 143\n",
      "Epoch: 144\n",
      "Epoch: 145\n",
      "Epoch: 146\n",
      "Epoch: 147\n",
      "Epoch: 148\n",
      "Epoch: 149\n",
      "Epoch: 150\n",
      "Epoch: 151\n",
      "Epoch: 152\n",
      "Epoch: 153\n",
      "Epoch: 154\n",
      "Epoch: 155\n",
      "Epoch: 156\n",
      "Epoch: 157\n",
      "Epoch: 158\n",
      "Epoch: 159\n",
      "Epoch: 160\n",
      "Epoch: 161\n",
      "Epoch: 162\n",
      "Epoch: 163\n",
      "Epoch: 164\n",
      "Epoch: 165\n",
      "Epoch: 166\n",
      "Epoch: 167\n",
      "Epoch: 168\n",
      "Epoch: 169\n",
      "Epoch: 170\n",
      "Epoch: 171\n",
      "Epoch: 172\n",
      "Epoch: 173\n",
      "Epoch: 174\n",
      "Epoch: 175\n",
      "Epoch: 176\n",
      "Epoch: 177\n",
      "Epoch: 178\n",
      "Epoch: 179\n",
      "Epoch: 180\n",
      "Epoch: 181\n",
      "Epoch: 182\n",
      "Epoch: 183\n",
      "Epoch: 184\n",
      "Epoch: 185\n",
      "Epoch: 186\n",
      "Epoch: 187\n",
      "Epoch: 188\n",
      "Epoch: 189\n",
      "Epoch: 190\n",
      "Epoch: 191\n",
      "Epoch: 192\n",
      "Duration: 132.13068429231643 minutes\n",
      "[tensor(1.7510, device='cuda:1', grad_fn=<DivBackward0>), tensor(1.5511, device='cuda:1', grad_fn=<DivBackward0>), tensor(1.3989, device='cuda:1', grad_fn=<DivBackward0>), tensor(1.3275, device='cuda:1', grad_fn=<DivBackward0>), tensor(1.3457, device='cuda:1', grad_fn=<DivBackward0>), tensor(1.2969, device='cuda:1', grad_fn=<DivBackward0>), tensor(1.1978, device='cuda:1', grad_fn=<DivBackward0>), tensor(1.1447, device='cuda:1', grad_fn=<DivBackward0>), tensor(1.1100, device='cuda:1', grad_fn=<DivBackward0>), tensor(1.3014, device='cuda:1', grad_fn=<DivBackward0>), tensor(1.2690, device='cuda:1', grad_fn=<DivBackward0>), tensor(1.2896, device='cuda:1', grad_fn=<DivBackward0>), tensor(1.0291, device='cuda:1', grad_fn=<DivBackward0>), tensor(1.1340, device='cuda:1', grad_fn=<DivBackward0>), tensor(1.0614, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.9771, device='cuda:1', grad_fn=<DivBackward0>), tensor(1.0001, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.9938, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.8810, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.8849, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.9116, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.7753, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.8832, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.9086, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.7910, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.8636, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.7012, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.8215, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.8680, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.8887, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.8989, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.7271, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.6480, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.7954, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.6396, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.6973, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.8580, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.7012, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.7382, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.7895, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.6171, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.6362, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.6125, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.7022, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.6602, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.7342, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.6463, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.6394, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.7054, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4285, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.7349, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.6428, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.6035, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.5724, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.7033, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.5698, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.6114, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4280, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.7004, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.5483, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4635, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.5016, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.6006, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4473, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4929, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4363, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.5079, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4870, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.5133, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4472, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4633, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4084, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.6382, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.5025, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4416, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.5005, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3975, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4943, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2999, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4216, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4466, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3153, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.5356, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4732, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3867, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4292, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3647, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4104, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3173, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3865, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3573, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.5868, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4042, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4592, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4698, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4363, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3296, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.5149, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3377, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2920, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3830, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.5338, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3404, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2920, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3326, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3181, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3710, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2954, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3089, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2982, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2950, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3058, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3119, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2589, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2569, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2231, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3866, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2513, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3353, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3355, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3128, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.4096, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2934, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3027, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2449, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2930, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2320, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2000, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3027, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2880, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2436, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3247, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2805, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2506, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2995, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2743, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3757, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2674, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2510, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2881, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2148, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3130, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2041, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1868, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2868, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3804, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1841, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2325, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2510, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2123, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2483, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2722, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2567, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2557, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2137, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1792, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3173, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2220, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1971, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2642, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2515, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2096, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2733, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2324, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1935, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1137, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2465, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2445, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2088, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2101, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1877, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2659, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1446, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3032, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1586, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1812, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1412, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1713, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2198, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1817, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1732, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2208, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1756, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2167, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1615, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1149, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.2091, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1220, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1738, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1978, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1665, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.3260, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1269, device='cuda:1', grad_fn=<DivBackward0>), tensor(0.1556, device='cuda:1', grad_fn=<DivBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(2.1420, device='cuda:1'), tensor(2.0116, device='cuda:1'), tensor(1.9238, device='cuda:1'), tensor(1.8392, device='cuda:1'), tensor(1.7457, device='cuda:1'), tensor(1.6855, device='cuda:1'), tensor(1.6418, device='cuda:1'), tensor(1.5993, device='cuda:1'), tensor(1.5755, device='cuda:1'), tensor(1.5641, device='cuda:1'), tensor(1.5046, device='cuda:1'), tensor(1.4807, device='cuda:1'), tensor(1.4609, device='cuda:1'), tensor(1.4697, device='cuda:1'), tensor(1.4100, device='cuda:1'), tensor(1.3811, device='cuda:1'), tensor(1.3608, device='cuda:1'), tensor(1.3397, device='cuda:1'), tensor(1.3136, device='cuda:1'), tensor(1.2972, device='cuda:1'), tensor(1.2801, device='cuda:1'), tensor(1.2611, device='cuda:1'), tensor(1.2468, device='cuda:1'), tensor(1.2320, device='cuda:1'), tensor(1.2171, device='cuda:1'), tensor(1.2127, device='cuda:1'), tensor(1.2024, device='cuda:1'), tensor(1.1848, device='cuda:1'), tensor(1.1885, device='cuda:1'), tensor(1.1790, device='cuda:1'), tensor(1.1529, device='cuda:1'), tensor(1.1592, device='cuda:1'), tensor(1.1504, device='cuda:1'), tensor(1.1335, device='cuda:1'), tensor(1.1145, device='cuda:1'), tensor(1.1251, device='cuda:1'), tensor(1.1206, device='cuda:1'), tensor(1.1053, device='cuda:1'), tensor(1.0985, device='cuda:1'), tensor(1.1030, device='cuda:1'), tensor(1.1007, device='cuda:1'), tensor(1.0934, device='cuda:1'), tensor(1.0976, device='cuda:1'), tensor(1.1041, device='cuda:1'), tensor(1.1110, device='cuda:1'), tensor(1.0748, device='cuda:1'), tensor(1.0522, device='cuda:1'), tensor(1.1170, device='cuda:1'), tensor(1.0690, device='cuda:1'), tensor(1.0770, device='cuda:1'), tensor(1.0332, device='cuda:1'), tensor(1.0706, device='cuda:1'), tensor(1.0557, device='cuda:1'), tensor(1.0623, device='cuda:1'), tensor(1.0376, device='cuda:1'), tensor(1.0702, device='cuda:1'), tensor(1.0753, device='cuda:1'), tensor(1.0569, device='cuda:1'), tensor(1.0584, device='cuda:1'), tensor(1.0517, device='cuda:1'), tensor(1.0650, device='cuda:1'), tensor(1.0823, device='cuda:1'), tensor(1.0501, device='cuda:1'), tensor(1.0490, device='cuda:1'), tensor(1.0446, device='cuda:1'), tensor(1.0277, device='cuda:1'), tensor(1.0521, device='cuda:1'), tensor(1.0409, device='cuda:1'), tensor(1.0334, device='cuda:1'), tensor(1.0327, device='cuda:1'), tensor(1.0270, device='cuda:1'), tensor(1.0495, device='cuda:1'), tensor(1.0191, device='cuda:1'), tensor(1.0185, device='cuda:1'), tensor(1.0071, device='cuda:1'), tensor(1.0267, device='cuda:1'), tensor(1.0250, device='cuda:1'), tensor(1.0157, device='cuda:1'), tensor(1.0166, device='cuda:1'), tensor(0.9959, device='cuda:1'), tensor(1.0220, device='cuda:1'), tensor(1.0002, device='cuda:1'), tensor(1.0288, device='cuda:1'), tensor(1.0180, device='cuda:1'), tensor(1.0155, device='cuda:1'), tensor(1.0360, device='cuda:1'), tensor(1.0469, device='cuda:1'), tensor(0.9917, device='cuda:1'), tensor(1.0258, device='cuda:1'), tensor(0.9937, device='cuda:1'), tensor(1.0162, device='cuda:1'), tensor(1.0077, device='cuda:1'), tensor(1.0046, device='cuda:1'), tensor(0.9871, device='cuda:1'), tensor(0.9605, device='cuda:1'), tensor(0.9870, device='cuda:1'), tensor(0.9809, device='cuda:1'), tensor(0.9776, device='cuda:1'), tensor(0.9486, device='cuda:1'), tensor(0.9890, device='cuda:1'), tensor(0.9471, device='cuda:1'), tensor(0.9444, device='cuda:1'), tensor(0.9763, device='cuda:1'), tensor(0.9593, device='cuda:1'), tensor(0.9897, device='cuda:1'), tensor(0.9471, device='cuda:1'), tensor(0.9739, device='cuda:1'), tensor(0.9505, device='cuda:1'), tensor(0.9515, device='cuda:1'), tensor(0.9524, device='cuda:1'), tensor(0.9502, device='cuda:1'), tensor(0.9043, device='cuda:1'), tensor(0.9854, device='cuda:1'), tensor(0.9184, device='cuda:1'), tensor(0.9121, device='cuda:1'), tensor(0.9411, device='cuda:1'), tensor(0.9524, device='cuda:1'), tensor(0.9427, device='cuda:1'), tensor(0.9735, device='cuda:1'), tensor(0.9690, device='cuda:1'), tensor(0.9427, device='cuda:1'), tensor(0.9125, device='cuda:1'), tensor(0.9624, device='cuda:1'), tensor(0.9527, device='cuda:1'), tensor(0.9179, device='cuda:1'), tensor(0.9645, device='cuda:1'), tensor(0.9618, device='cuda:1'), tensor(0.9162, device='cuda:1'), tensor(0.9147, device='cuda:1'), tensor(0.9638, device='cuda:1'), tensor(0.9622, device='cuda:1'), tensor(0.9335, device='cuda:1'), tensor(0.9632, device='cuda:1'), tensor(0.9439, device='cuda:1'), tensor(0.9474, device='cuda:1'), tensor(0.9469, device='cuda:1'), tensor(0.9504, device='cuda:1'), tensor(0.9655, device='cuda:1'), tensor(0.9279, device='cuda:1'), tensor(0.9540, device='cuda:1'), tensor(0.9374, device='cuda:1'), tensor(0.9285, device='cuda:1'), tensor(0.9764, device='cuda:1'), tensor(0.9755, device='cuda:1'), tensor(0.9873, device='cuda:1'), tensor(0.9579, device='cuda:1'), tensor(0.9576, device='cuda:1'), tensor(0.9228, device='cuda:1'), tensor(0.9317, device='cuda:1'), tensor(0.9110, device='cuda:1'), tensor(0.9541, device='cuda:1'), tensor(0.9482, device='cuda:1'), tensor(0.9564, device='cuda:1'), tensor(0.9849, device='cuda:1'), tensor(0.9227, device='cuda:1'), tensor(0.9947, device='cuda:1'), tensor(0.9569, device='cuda:1'), tensor(0.9363, device='cuda:1'), tensor(0.9011, device='cuda:1'), tensor(0.9639, device='cuda:1'), tensor(0.9100, device='cuda:1'), tensor(0.9107, device='cuda:1'), tensor(0.8817, device='cuda:1'), tensor(0.9342, device='cuda:1'), tensor(0.8983, device='cuda:1'), tensor(0.9043, device='cuda:1'), tensor(0.8939, device='cuda:1'), tensor(0.9186, device='cuda:1'), tensor(0.9123, device='cuda:1'), tensor(0.9091, device='cuda:1'), tensor(0.9250, device='cuda:1'), tensor(0.9176, device='cuda:1'), tensor(0.9281, device='cuda:1'), tensor(0.8663, device='cuda:1'), tensor(0.9249, device='cuda:1'), tensor(0.9879, device='cuda:1'), tensor(0.9483, device='cuda:1'), tensor(0.9027, device='cuda:1'), tensor(0.9405, device='cuda:1'), tensor(0.8823, device='cuda:1'), tensor(0.8917, device='cuda:1'), tensor(0.9411, device='cuda:1'), tensor(0.9220, device='cuda:1'), tensor(0.9046, device='cuda:1'), tensor(0.9384, device='cuda:1'), tensor(0.9499, device='cuda:1'), tensor(0.9916, device='cuda:1'), tensor(1.0102, device='cuda:1'), tensor(0.9484, device='cuda:1'), tensor(0.8847, device='cuda:1'), tensor(0.9156, device='cuda:1'), tensor(0.9282, device='cuda:1'), tensor(0.9261, device='cuda:1'), tensor(1.0291, device='cuda:1')]\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE MODEL:\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Tensorboard real time visualisation:\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "epochs = 5000\n",
    "\n",
    "patience = 100\n",
    "min_loss_val = 5 # huge initial value for the minimum validation loss\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for b, (image, label) in enumerate(train_loader):\n",
    "        \n",
    "        b += 1\n",
    "        train_batch = len(label) # last batch may be of smaller, different size\n",
    "\n",
    "        encoder_hidden = Encoder_model.initHidden(batch = train_batch)\n",
    "\n",
    "        image_cnn = image.view(-1, color_channels, patch_height, patch_width).cuda(1)\n",
    "        encoder_input = CNN_model(image_cnn)\n",
    "        _, encoder_hidden = Encoder_model(encoder_input, encoder_hidden, batch = train_batch, seq_len = n_patches)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = get_one_hot_target(label=label, batch = train_batch, seq_len = MAX_LENGTH + 2,\n",
    "                                          output_size = output_size).cuda(1)\n",
    "        decoder_output, decoder_hidden = Decoder_model(decoder_input, decoder_hidden,\n",
    "                                                       batch = train_batch, seq_len = MAX_LENGTH + 2)\n",
    "\n",
    "        output_indices = torch.tensor(list(range(0, MAX_LENGTH + 1))).cuda(1) # removing last token from the output\n",
    "        decoder_output = torch.index_select(decoder_output, dim = 1, index = output_indices)\n",
    "\n",
    "        ground_truth = torch.argmax(decoder_input, dim = 2)\n",
    "        target_indices = torch.tensor(list(range(1, MAX_LENGTH + 2))).cuda(1) # remove SOS_token from the input\n",
    "        ground_truth = torch.index_select(ground_truth, dim = 1, index = target_indices)\n",
    "\n",
    "        loss = 0\n",
    "        \n",
    "        for j in range(train_batch):\n",
    "            \n",
    "            loss += criterion(decoder_output[j], ground_truth[j])   \n",
    "            \n",
    "        loss = loss/train_batch\n",
    "        \n",
    "        \n",
    "        CNN_optimizer.zero_grad()\n",
    "        Encoder_optimizer.zero_grad()\n",
    "        Decoder_optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        CNN_optimizer.step()\n",
    "        Encoder_optimizer.step()\n",
    "        Decoder_optimizer.step()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for v, (image_val, label_val) in enumerate(val_loader):\n",
    "        \n",
    "            v += 1\n",
    "            val_batch = len(label_val)\n",
    "            \n",
    "            encoder_hidden_val = Encoder_model.initHidden(batch = val_batch)\n",
    "\n",
    "            image_cnn_val = image_val.view(-1, color_channels, patch_height, patch_width).cuda(1)\n",
    "            encoder_input_val = CNN_model(image_cnn_val)\n",
    "            _, encoder_hidden_val = Encoder_model(encoder_input_val, encoder_hidden_val, batch = val_batch, seq_len = n_patches)\n",
    "\n",
    "            decoder_hidden_val = encoder_hidden_val\n",
    "            decoder_input_val = get_one_hot_target(label=label_val, batch = val_batch, seq_len = MAX_LENGTH + 2,\n",
    "                                                  output_size = output_size).cuda(1)\n",
    "            decoder_output_val, decoder_hidden_val = Decoder_model(decoder_input_val, decoder_hidden_val,\n",
    "                                                                  batch = val_batch, seq_len = MAX_LENGTH + 2)\n",
    "            \n",
    "            output_indices_val = torch.tensor(list(range(0, MAX_LENGTH + 1))).cuda(1) # remove EOS_token from the output\n",
    "            decoder_output_val = torch.index_select(decoder_output_val, dim = 1, index = output_indices_val)\n",
    "\n",
    "            ground_truth_val = torch.argmax(decoder_input_val, dim = 2)\n",
    "            target_indices_val = torch.tensor(list(range(1, MAX_LENGTH + 2))).cuda(1) # remove SOS_token from the input\n",
    "            ground_truth_val = torch.index_select(ground_truth_val, dim = 1, index = target_indices_val)\n",
    "            \n",
    "            loss_val = 0\n",
    "            \n",
    "            for j in range(val_batch):\n",
    "                \n",
    "                loss_val += criterion(decoder_output_val[j], ground_truth_val[j])\n",
    "            \n",
    "            loss_val = loss_val/val_batch\n",
    "        \n",
    "        \n",
    "    train_losses.append(loss)\n",
    "    val_losses.append(loss_val)\n",
    "    \n",
    "    # Sending losses to Tensorboard: \n",
    "    writer.add_scalar('Loss/train', loss, i)\n",
    "    writer.add_scalar('Loss/validation', loss_val, i)\n",
    "    \n",
    "    patience -= 1 # updating patience\n",
    "    \n",
    "    if patience == 0: # Early Stopping\n",
    "        \n",
    "        break\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        if loss_val < min_loss_val: # condition to save the best model\n",
    "\n",
    "            min_loss_val = loss_val\n",
    "\n",
    "            patience = 100\n",
    "\n",
    "            # Saving best model:\n",
    "\n",
    "            torch.save(CNN_model.state_dict(), 'CNN_model_1000_words_TF.pt')\n",
    "            torch.save(Encoder_model.state_dict(), 'Encoder_model_1000_words_TF.pt')\n",
    "            torch.save(Decoder_model.state_dict(), 'Decoder_model_1000_words_TF.pt')\n",
    "\n",
    "    \n",
    "    # if (i > patience) & (Early_Stopping(patience, val_losses) == True): # Early Stopping\n",
    "        \n",
    "    #    break\n",
    "\n",
    "        \n",
    "    print(f'Epoch: {i}')\n",
    "    \n",
    "print(f'Duration: {(time.time() - start_time)/60} minutes')    \n",
    "print(train_losses)\n",
    "print(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting results\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(train_losses, label='training loss')\n",
    "plt.plot(val_losses, label='validation loss')\n",
    "plt.title('Losses per epoch CNN-S2S 1000 words')\n",
    "plt.legend();\n",
    "#plt.savefig('8 words training losses lr=0,001, bs=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (lstm): LSTM(28, 256, batch_first=True)\n",
       "  (out): Linear(in_features=256, out_features=28, bias=True)\n",
       "  (softmax): LogSoftmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_model.load_state_dict(torch.load('CNN_model_1000_words_TF.pt'))\n",
    "CNN_model.eval()\n",
    "\n",
    "Encoder_model.load_state_dict(torch.load('Encoder_model_1000_words_TF.pt'))\n",
    "Encoder_model.eval()\n",
    "\n",
    "Decoder_model.load_state_dict(torch.load('Decoder_model_1000_words_TF.pt'))\n",
    "Decoder_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  5, 14, 14, 14, 27]], device='cuda:1')\n",
      "tensor([[19, 18,  1, 18, 18, 27]], device='cuda:1')\n",
      "tensor([[ 1,  1, 20, 20,  8, 27]], device='cuda:1')\n",
      "tensor([[ 5,  5, 20, 20, 20, 18, 27]], device='cuda:1')\n",
      "tensor([[12, 12, 12, 12, 12, 27]], device='cuda:1')\n",
      "tensor([[13, 15,  5, 13, 14, 27]], device='cuda:1')\n",
      "tensor([[19,  1, 19, 19, 19, 19, 27]], device='cuda:1')\n",
      "tensor([[19, 15, 21, 21, 21,  9,  9, 12, 27]], device='cuda:1')\n",
      "tensor([[13,  5,  5,  5, 14, 20, 27]], device='cuda:1')\n",
      "tensor([[19,  9, 12, 12,  5,  5, 27]], device='cuda:1')\n",
      "tensor([[20,  8,  1,  1, 11, 27]], device='cuda:1')\n",
      "tensor([[ 2, 18,  1,  1,  1,  8, 27]], device='cuda:1')\n",
      "tensor([[20,  1,  1,  3,  8, 27]], device='cuda:1')\n",
      "tensor([[19, 21,  9,  9, 21, 21, 27]], device='cuda:1')\n",
      "tensor([[16,  5, 16, 16, 16, 16,  5, 12, 12, 27]], device='cuda:1')\n",
      "tensor([[ 7,  9,  7, 27]], device='cuda:1')\n",
      "tensor([[ 6, 18,  1,  9,  9,  4, 27]], device='cuda:1')\n",
      "tensor([[ 8, 21,  7,  5, 27]], device='cuda:1')\n",
      "tensor([[19,  9, 19, 19, 19, 20, 27]], device='cuda:1')\n",
      "tensor([[19,  5,  5,  5, 20, 27]], device='cuda:1')\n",
      "tensor([[ 3, 15, 19, 19, 19, 19, 19, 27]], device='cuda:1')\n",
      "tensor([[ 6, 15, 15, 15, 18, 18, 18, 27]], device='cuda:1')\n",
      "tensor([[19,  9,  9,  9,  9,  9, 18, 27]], device='cuda:1')\n",
      "tensor([[ 7,  9,  9,  9,  5, 27]], device='cuda:1')\n",
      "tensor([[ 5,  5,  5,  5,  5,  5,  5,  5,  5, 27]], device='cuda:1')\n",
      "tensor([[19, 15, 15, 18,  5, 27]], device='cuda:1')\n",
      "tensor([[16, 16, 16, 16,  5, 27]], device='cuda:1')\n",
      "tensor([[19, 21, 21, 21,  8,  8, 27]], device='cuda:1')\n",
      "tensor([[12, 12,  4, 27]], device='cuda:1')\n",
      "tensor([[ 1,  9,  9,  8,  8, 27]], device='cuda:1')\n",
      "tensor([[ 3, 15,  3, 20, 27]], device='cuda:1')\n",
      "tensor([[19,  1, 19, 19, 27]], device='cuda:1')\n",
      "tensor([[ 3,  1,  1,  4, 27]], device='cuda:1')\n",
      "tensor([[ 2,  1, 14,  4, 27]], device='cuda:1')\n",
      "tensor([[18, 15, 15,  5, 27]], device='cuda:1')\n",
      "tensor([[19,  9,  9, 16, 27]], device='cuda:1')\n",
      "tensor([[23,  9, 14, 27]], device='cuda:1')\n",
      "tensor([[13,  1,  1,  5, 18, 27]], device='cuda:1')\n",
      "tensor([[12, 12, 14, 14, 14, 14, 14, 27]], device='cuda:1')\n",
      "tensor([[16, 16, 19,  9,  9,  9,  9, 15, 14, 27]], device='cuda:1')\n",
      "tensor([[ 6,  5,  5,  4, 27]], device='cuda:1')\n",
      "tensor([[20, 15, 15, 12, 27]], device='cuda:1')\n",
      "tensor([[20, 20,  1, 20, 20, 27]], device='cuda:1')\n",
      "tensor([[16,  1,  9,  9, 19, 27]], device='cuda:1')\n",
      "tensor([[19, 13,  5, 12, 12, 27]], device='cuda:1')\n",
      "tensor([[12, 12, 12, 12, 12, 25, 27]], device='cuda:1')\n",
      "tensor([[15, 15, 18, 27]], device='cuda:1')\n",
      "tensor([[16, 15, 15, 12, 12,  5, 27]], device='cuda:1')\n",
      "tensor([[19,  5,  1, 20, 27]], device='cuda:1')\n",
      "tensor([[18, 18, 18, 18,  5,  5, 27]], device='cuda:1')\n",
      "tensor([[13,  1,  1, 20, 20, 20, 27]], device='cuda:1')\n",
      "tensor([[ 3, 18,  1,  3, 11, 27]], device='cuda:1')\n",
      "tensor([[16,  1,  5,  5, 20, 20, 27]], device='cuda:1')\n",
      "tensor([[19, 15, 15, 15,  5, 27]], device='cuda:1')\n",
      "tensor([[19,  9,  9,  9,  9,  9,  9,  4, 27]], device='cuda:1')\n",
      "tensor([[19,  5,  5, 20, 20, 27]], device='cuda:1')\n",
      "tensor([[ 3, 21, 21, 21, 21, 20, 20, 20,  5, 27]], device='cuda:1')\n",
      "tensor([[ 6, 15, 15, 18, 18, 27]], device='cuda:1')\n",
      "tensor([[ 3, 15,  3,  3,  3,  3, 20, 27]], device='cuda:1')\n",
      "tensor([[16, 15, 19, 20, 27]], device='cuda:1')\n",
      "tensor([[19,  5,  5, 14,  4, 27]], device='cuda:1')\n",
      "tensor([[18, 18, 15, 15,  4, 27]], device='cuda:1')\n",
      "tensor([[ 6,  1, 20, 27]], device='cuda:1')\n",
      "tensor([[12, 12,  1,  4, 27]], device='cuda:1')\n",
      "tensor([[18,  9,  9,  9,  9,  9, 14, 12, 27]], device='cuda:1')\n",
      "tensor([[19,  1,  1, 18,  5, 27]], device='cuda:1')\n",
      "tensor([[19, 20, 20, 20, 20, 20, 20, 27]], device='cuda:1')\n",
      "tensor([[ 4,  4,  4, 27]], device='cuda:1')\n",
      "tensor([[ 2, 18,  1,  4,  4, 27]], device='cuda:1')\n",
      "tensor([[ 3,  1,  1,  7,  7,  5, 27]], device='cuda:1')\n",
      "tensor([[16, 16, 16, 16, 18, 18, 27]], device='cuda:1')\n",
      "tensor([[ 2,  1, 18, 27]], device='cuda:1')\n",
      "tensor([[ 6,  6,  6,  5, 18, 27]], device='cuda:1')\n",
      "tensor([[19,  5,  5,  5,  5,  5, 14, 27]], device='cuda:1')\n",
      "tensor([[19, 12,  1, 22,  5, 27]], device='cuda:1')\n",
      "tensor([[ 4, 21, 11, 11, 27]], device='cuda:1')\n",
      "tensor([[14,  1, 20, 20, 14, 14, 20, 27]], device='cuda:1')\n",
      "tensor([[13,  1, 18, 20,  5, 20, 27]], device='cuda:1')\n",
      "tensor([[ 4,  5,  5,  5,  5,  5, 27]], device='cuda:1')\n",
      "tensor([[16, 15, 16, 16, 21, 21, 21,  5, 27]], device='cuda:1')\n",
      "tensor([[ 3,  3,  3,  3, 11, 27]], device='cuda:1')\n",
      "tensor([[ 4,  5,  1, 18, 27]], device='cuda:1')\n",
      "tensor([[13,  5, 14, 14, 14, 27]], device='cuda:1')\n",
      "tensor([[16,  5, 16, 16, 25, 27]], device='cuda:1')\n",
      "tensor([[18,  9,  9, 11, 11, 27]], device='cuda:1')\n",
      "tensor([[ 3, 15,  3,  3, 18, 27]], device='cuda:1')\n",
      "tensor([[16, 15, 15, 15, 16, 16, 20, 27]], device='cuda:1')\n",
      "tensor([[19,  5,  5,  5,  5,  3, 27]], device='cuda:1')\n",
      "tensor([[20, 21, 18, 18, 18,  5, 27]], device='cuda:1')\n",
      "tensor([[ 7,  1,  1, 14,  5, 27]], device='cuda:1')\n",
      "tensor([[19, 20,  1,  1, 13, 27]], device='cuda:1')\n",
      "tensor([[13, 15, 15, 15, 14, 14, 27]], device='cuda:1')\n",
      "tensor([[16,  1, 20, 20, 27]], device='cuda:1')\n",
      "tensor([[ 4,  9,  9,  9,  9,  4, 27]], device='cuda:1')\n",
      "tensor([[ 2, 15,  7, 27]], device='cuda:1')\n",
      "tensor([[13,  1,  1, 14, 20, 27]], device='cuda:1')\n",
      "tensor([[21, 15, 21, 20, 20, 20, 20, 20, 27]], device='cuda:1')\n",
      "tensor([[20,  5, 20, 20, 20, 27]], device='cuda:1')\n",
      "tensor([[19,  5,  5, 12, 12, 27]], device='cuda:1')\n",
      "tensor([[ 3,  3, 11, 11, 27]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        \n",
    "    for t, (image_test, label_test) in enumerate(test_loader):\n",
    "\n",
    "        t += 1\n",
    "        test_batch = len(label_test)\n",
    "        \n",
    "        encoder_hidden_test = Encoder_model.initHidden(batch = test_batch)\n",
    "        \n",
    "        image_cnn_test = image_test.view(-1, color_channels, patch_height, patch_width).cuda(1)\n",
    "        encoder_input_test = CNN_model(image_cnn_test)\n",
    "        _, encoder_hidden_test = Encoder_model(encoder_input_test, encoder_hidden_test, batch=test_batch,\n",
    "                                                                 seq_len=n_patches)\n",
    "        \n",
    "        for j in range(test_batch):\n",
    "           \n",
    "            decoder_input_test = letter_to_vector('SOS_token').cuda(1) # We initialize the first Decoder input as the SOS token\n",
    "            \n",
    "            decoder_hidden_test = (encoder_hidden_test[0][0, j, :].view(1, 1, hidden_size), # We take the last hidden state of the Encoder \n",
    "                                   encoder_hidden_test[1][0, j, :].view(1, 1, hidden_size)) # for each image/word (j) within the patch \n",
    "            # This would be the first hidden state of the Decoder for image/word (j)\n",
    "\n",
    "            for d in range(MAX_LENGTH):\n",
    "                \n",
    "                decoder_output_test, decoder_hidden_test = Decoder_model(decoder_input_test, decoder_hidden_test,\n",
    "                                                                        batch=1, seq_len=1)\n",
    "\n",
    "                if d == 0:\n",
    "                    \n",
    "                    output_word = decoder_output_test\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    output_word = torch.cat((output_word, decoder_output_test), dim = 1).cuda(1)\n",
    "                \n",
    "                \n",
    "                if  torch.equal(torch.argmax(decoder_output_test, dim = 2), \n",
    "                                 torch.argmax(letter_to_vector('END').cuda(1), dim = 2)):\n",
    "                    break\n",
    "                \n",
    "            output_word = torch.argmax(output_word, dim = 2)\n",
    "            print(output_word)   \n",
    "           # if j == 0:\n",
    "                \n",
    "            #    total_output_word = output_word\n",
    "                \n",
    "            #else:\n",
    "                \n",
    "             #   total_output_word = torch.cat((total_output_word, output_word), dim = 1)\n",
    "           \n",
    "    #print(total_output_word)\n",
    "\n",
    "\n",
    "#for j in range(batch_size):\n",
    "    \n",
    " #   model_word = []\n",
    "    \n",
    "  #  for i in range(total_output_word[j].numel()):\n",
    "        \n",
    "   #     if letters[total_output_word[j][i]] == 'SOS_token':\n",
    "      #      break\n",
    "            \n",
    "    #    model_word.append(letters[total_output_word[j][i]])\n",
    "        \n",
    "        \n",
    "    #model_word = ''.join(model_word)    \n",
    "    #print(model_word)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.7393, -1.1640, -0.6692],\n",
      "         [-0.6714, -3.1625, -0.8059]]])\n",
      "tensor([[[-1.3905, -0.8152, -0.3204],\n",
      "         [ 0.7377, -1.7534,  0.6033]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "c = nn.LogSoftmax(dim=2)\n",
    "a = torch.randn(1,2,3)\n",
    "print(c(a))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7392961570497802"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log((np.exp(-1.3905))/(np.exp(-1.3905) + np.exp(-0.8152) + np.exp(-0.3204)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.240681356437162"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log((np.exp(-1.3905))/(np.exp(-1.3905) + np.exp(0.7377)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:1')\n",
      "tensor(0, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(28).cuda(1)\n",
    "c = torch.argmax(a, dim = 0)\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-23-214c4d391130>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-214c4d391130>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    tensorboard --logdir=runs\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for n_iter in range(100):\n",
    "    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "x = range(100)\n",
    "for i in x:\n",
    "    writer.add_scalar('y=2x', i * 2, i)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([0.5])\n",
    "c = b<a\n",
    "x = all(c)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([0.3553, 0.2992, 0.3441, 0.3689], device='cuda:1')\n",
      "tensor(0.5201, device='cuda:1', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(train_losses[-5:-1]).cuda(1)\n",
    "b = train_losses[-1]\n",
    "c = b<a\n",
    "x = all(c)\n",
    "print(x)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Early_Stopping(5, train_losses, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activar pytorch_estoril (environment) en la terminal y ejecutar tensorboard --host 0.0.0.0 --logdir ./runs\n",
    "# Tensorboard se ejecutará en un cierto puerto y nos dará el enlace. Habrá que sustituir la IP 0.0.0.0 por la del equipo\n",
    "# en remoto en la que esté corriendo en el caso de Estoril 212.128.3.86:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28])\n",
      "tensor([[0]])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(1,1,28)\n",
    "print(a.shape)\n",
    "a_2 = torch.argmax(a, 2)\n",
    "print(a_2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "tensor([[28]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[28]])\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(letter_to_vector('EOS_token').view(1,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2961, 0.5166, 0.2517, 0.6886],\n",
      "        [0.0740, 0.8665, 0.1366, 0.1025],\n",
      "        [0.1841, 0.7264, 0.3153, 0.6871],\n",
      "        [0.0756, 0.1966, 0.3164, 0.4017],\n",
      "        [0.1186, 0.8274, 0.3821, 0.6605],\n",
      "        [0.8536, 0.5932, 0.6367, 0.9826]])\n",
      "tensor([[[0.2961, 0.5166, 0.2517, 0.6886],\n",
      "         [0.0740, 0.8665, 0.1366, 0.1025],\n",
      "         [0.1841, 0.7264, 0.3153, 0.6871]],\n",
      "\n",
      "        [[0.0756, 0.1966, 0.3164, 0.4017],\n",
      "         [0.1186, 0.8274, 0.3821, 0.6605],\n",
      "         [0.8536, 0.5932, 0.6367, 0.9826]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "t = torch.rand(6, 4)\n",
    "print(t)\n",
    "t = t.view(2,3,4)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(torch.argmax(letter_to_vector('END'), dim = 2), torch.argmax(letter_to_vector('START'), dim = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_estoril",
   "language": "python",
   "name": "pytorch_estoril"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
