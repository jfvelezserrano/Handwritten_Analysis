{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6ca991-4cb9-44e1-8ee8-70ff520cc885",
   "metadata": {},
   "source": [
    "# Antes de empezar\n",
    "\n",
    "conda activate python3.6_cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb7184b-e35a-4e67-9aef-865fe8dbc1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "import cv2\n",
    "import skimage\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Ignore harmless warnings:\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3320ac7-a579-4ced-a885-d17400ab99c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n",
      "3.6.10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Quadro RTX 5000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "print(torch.__version__)\n",
    "print(platform.python_version())\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb78a6ea-117f-4794-82d6-6f9883f91c78",
   "metadata": {},
   "source": [
    "## Cargando datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b527dc-df83-4727-ad05-78760cb9bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWords(file_name):\n",
    "    top_30000_words = pd.read_csv(file_name, delimiter = '\\t', header = None, keep_default_na=False)\n",
    "    src_words = []\n",
    "    sample_length = len(top_30000_words)\n",
    "\n",
    "    for i in range(sample_length):\n",
    "        src_words.append(top_30000_words.loc[i][0])\n",
    "\n",
    "    total_length = len(src_words)\n",
    "    return src_words,total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8669b9a-2b45-48a7-ac35-c2e1a86c16ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras:  30000\n",
      "Las primeras:  ['the', 'of', 'and', 'to', 'a', 'in', 'for', 'is', 'on', 'that']\n"
     ]
    }
   ],
   "source": [
    "src_words,total_length = loadWords('30k.txt') \n",
    "print(\"Palabras: \", total_length)\n",
    "print(\"Las primeras: \", src_words[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6325ef-9a4d-4a6e-8093-b83bdd61b77e",
   "metadata": {},
   "source": [
    "## Eliminar palabras de tamaño grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c38aa84-5638-4a26-9367-335155faae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteBigWords(max_len,words):\n",
    "    output = words.copy()\n",
    "\n",
    "    huge_words = 0\n",
    "\n",
    "    for i in range(total_length):\n",
    "        if len(output[i]) > max_len:\n",
    "            output[i] = output[i-1]\n",
    "            huge_words += 1\n",
    "\n",
    "    print(\"Palabras largas cambiadas: \", huge_words)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78b70f10-ce1e-4419-a5af-b4a658c243dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras largas cambiadas:  35\n"
     ]
    }
   ],
   "source": [
    "words = deleteBigWords(15,src_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439a4d4-0a30-4dc8-b003-726ef80c3e72",
   "metadata": {},
   "source": [
    "## Definiendo conjuntos de train, test y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e101312f-284d-42a2-b0eb-af221bd6d442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "train_set = words[0:28000]\n",
    "val_set = words[28000:29000] \n",
    "test_set = words[29000:30000]\n",
    "test_set_2 = words[20000:21000] # (seen during training, the network shoud have memorised it)\n",
    "\n",
    "MAX_LENGTH = max(len(word) for word in words) # length of the longest word within our sample\n",
    "\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b50dc67f-fd3e-4389-b49b-71f38c8189a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['START', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "          'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'END', 'PAD']\n",
    "\n",
    "mapeo = {}\n",
    "\n",
    "cont = 0\n",
    "for key in letters:\n",
    "    vector = torch.zeros(1, 1, len(letters))\n",
    "    vector[0,0,cont] = 1.0\n",
    "    mapeo[key] = vector\n",
    "    cont += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4aa1154c-7000-4102-b2ea-8c5d318a258c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7509e984-5790-45ee-a5e3-bec216bdeab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_gen(word, n_patches, patch_height, patch_width, stepsize, color_channels):\n",
    "    \n",
    "    image = 255 * np.ones(shape = (height, width), dtype = np.uint8)\n",
    "    image = cv2.putText(image, text = word, org = (5, 30),\n",
    "        fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.62, color = (0, 0, 0),\n",
    "        thickness = 1, lineType = cv2.LINE_AA)\n",
    "    #image = skimage.util.random_noise(image, mode='s&p')\n",
    "    image = transforms.ToPILImage()(image) # np.ndarray to PIL.Image.Image\n",
    "    patches_tensor = torch.empty(n_patches, color_channels, patch_height, patch_width)\n",
    "    patches_tensor = patches_tensor.cuda(0)\n",
    "    \n",
    "    for p in range(n_patches):\n",
    "        \n",
    "        patch = transforms.functional.crop(image, 0, 0 + p * stepsize, patch_height, patch_width) # cropping of the image into patches\n",
    "        patch = transforms.ToTensor()(patch) # torch.Tensor of the patch (normalized)\n",
    "        #patch = torch.from_numpy(patch) # conversion to pytorch tensor again\n",
    "        patch = 1. - patch # it will work better if we have white text over black background\n",
    "        patch = patch.view(1, 1, patch_height, patch_width) # CNN_model expects a 4-dimensional tensor (1 dimension for batch)\n",
    "        #patch = patch.type(torch.FloatTensor) # conversion to float\n",
    "        patch = patch.cuda(0) # set to cuda\n",
    "        patches_tensor[p, 0, :, :] = patch\n",
    "                \n",
    "    return patches_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d704837-87fd-4ba8-976a-3ccf75b606bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 48\n",
    "width = 192\n",
    "patch_height = 48\n",
    "patch_width = 10\n",
    "stepsize = 2\n",
    "color_channels = 1\n",
    "\n",
    "n_patches = int((width - patch_width)/stepsize + 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "048ed70b-0e9d-4247-b36c-d54179ff0510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_set(word_set):    \n",
    "    complete_set = []\n",
    "    \n",
    "    for word in word_set:        \n",
    "        complete_set.append((patch_gen(word, n_patches, patch_height, patch_width, stepsize, color_channels), word))\n",
    "        \n",
    "    return complete_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0061e4a-6356-48a4-878c-2f1adb4d7ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_train_set = complete_set(train_set)\n",
    "comp_val_set = complete_set(val_set)\n",
    "comp_test_set = complete_set(test_set)\n",
    "comp_test_set_2 = complete_set(test_set_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "669b3fee-0d89-4635-8b17-b4c8f5e9928c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 92, 1, 48, 10])\n",
      "torch.Size([47104, 1, 48, 10])\n",
      "('itv', 'beside', 'reminiscent', 'embarrassed', 'brought', 'endangered', 'countryside', 'gardner', 'fredericksburg', 'titty', 'clemente', 'lesson', 'stanford', 'elevation', 'faint', 'synchronization', 'striving', 'sponsorship', 'cancun', 'development', 'latina', 'severe', 'ry', 'albion', 'bets', 'superannuation', 'rhythmic', 'inter', 'difference', 'scandinavia', 'figuring', 'spouse', 'ghetto', 'penelope', 'tuscan', 'intermittent', 'casper', 'umass', 'authority', 'talk', 'ar', 'ellison', 'athletics', 'delicious', 'awesome', 'subscriptions', 'shaun', 'listprice', 'laredo', 'fnal', 'goodwill', 'carmen', 'endeavors', 'proliferation', 'line', 'clips', 'ang', 'vax', 'lesbo', 'notify', 'patsy', 'mobil', 'professionalism', 'diagonal', 'subs', 'grimes', 'god', 'bedrooms', 'linguistics', 'butalbital', 'destroys', 'plate', 'unprecedented', 'milano', 'acronyms', 'singers', 'pure', 'monitor', 'celebrates', 'ixus', 'amature', 'cfm', 'snoop', 'messageslog', 'cipher', 'franz', 'absence', 'clashes', 'subscribing', 'api', 'deli', 'ramsey', 'satisfied', 'elena', 'ended', 'orbitz', 'validator', 'tra', 'sorority', 'crosby', 'ranking', 'fools', 'too', 'dial', 'browsing', 'serial', 'tomas', 'dwight', 'sis', 'took', 'develop', 'braided', 'spool', 'falls', 'thinkcentre', 'showcase', 'keychains', 'communion', 'cathy', 'wifi', 'dining', 'scoreboard', 'ser', 'neighbouring', 'surveillance', 'theron', 'trenches', 'technician', 'cigars', 'together', 'hitter', 'kuala', 'kidd', 'pee', 'stationary', 'passports', 'radcliffe', 'peterborough', 'requirement', 'resisted', 'craigslist', 'breakdown', 'parkinson', 'wretch', 'weed', 'silvia', 'stunt', 'nichols', 'hunter', 'meetings', 'tartan', 'olympic', 'bog', 'allegro', 'remarkable', 'icp', 'secured', 'of', 'improper', 'salsa', 'sock', 'premise', 'jerseys', 'mikhail', 'dependents', 'intra', 'purchasing', 'retarded', 'walkthroughs', 'exempted', 'juniper', 'promise', 'vicky', 'add', 'rang', 'edible', 'deterministic', 'biases', 'winter', 'limitation', 'plastic', 'pinned', 'telnet', 'digging', 'midas', 'notification', 'journey', 'quantification', 'appraised', 'wipes', 'evangelism', 'kerberos', 'sixteenth', 'yelled', 'doses', 'iss', 'posed', 'equals', 'paging', 'subjective', 'coefficients', 'probability', 'aiming', 'employed', 'meds', 'tei', 'snes', 'filtered', 'encyclopedia', 'nicole', 'emails', 'climatic', 'complying', 'photons', 'customise', 'adrenaline', 'everywhere', 'hughes', 'starsmerchant', 'visit', 'pleasures', 'because', 'bethlehem', 'logs', 'pfc', 'calm', 'molten', 'hispanic', 'ability', 'dane', 'scientists', 'lungs', 'detroit', 'samples', 'storytelling', 'drudge', 'caught', 'liens', 'cytoplasmic', 'vocals', 'accolades', 'happier', 'mennonite', 'xvi', 'makeover', 'indicating', 'tigers', 'hazard', 'innsbruck', 'hannover', 'zanussi', 'hepa', 'tod', 'ans', 'appearances', 'fleur', 'scrolling', 'dunlop', 'primitives', 'petersburg', 'sakura', 'shines', 'treadmill', 'hardcover', 'monarchy', 'vv', 'receivers', 'wap', 'jenn', 'inconvenience', 'landmark', 'fostering', 'fastener', 'anc', 'auditory', 'distributors', 'bernhard', 'walking', 'interpretive', 'documentation', 'jane', 'bachelor', 'enforcing', 'comparing', 'booty', 'persuasive', 'fortunes', 'hash', 'untreated', 'noir', 'predicting', 'marsh', 'hawkes', 'clarity', 'vests', 'renewed', 'project', 'doe', 'mormon', 'assessor', 'mailboxes', 'exalted', 'ares', 'widows', 'provided', 'ativan', 'labor', 'cosponsors', 'loretta', 'implementing', 'clare', 'poverty', 'pudding', 'ati', 'stabilize', 'far', 'joyce', 'outage', 'assistants', 'count', 'ionization', 'flood', 'vaio', 'kelsey', 'gd', 'momma', 'carpets', 'majors', 'hibernate', 'prestigious', 'strive', 'herd', 'nautica', 'demon', 'br', 'chests', 'buffalo', 'switching', 'highlighting', 'kwazulu', 'ima', 'filed', 'xb', 'poles', 'recital', 'nicholas', 'czk', 'diggs', 'intracellular', 'ubuntu', 'contemplate', 'mild', 'pants', 'actors', 'recherche', 'norwich', 'idc', 'accidents', 'lan', 'contrast', 'furniture', 'love', 'categorised', 'depeche', 'srv', 'epson', 'institute', 'marketers', 'link', 'cardbus', 'drag', 'slightest', 'nonlinear', 'alumni', 'settlements', 'dependencies', 'flip', 'rockville', 'tru', 'diner', 'magenta', 'fiberglass', 'delights', 'financials', 'francesca', 'aga', 'colors', 'earthquakes', 'cleaned', 'episode', 'incremental', 'abrupt', 'cheshire', 'hodgson', 'experts', 'seam', 'exclaimed', 'wednesday', 'brookline', 'kerry', 'dollar', 'doll', 'tended', 'bread', 'individual', 'specialised', 'clocks', 'cascading', 'dentists', 'marietta', 'tourists', 'reading', 'desert', 'printing', 'coats', 'gonzo', 'weary', 'treasures', 'oper', 'lecturer', 'choi', 'midday', 'clam', 'openoffice', 'becky', 'soul', 'ami', 'proactive', 'excluded', 'bouvet', 'mds', 'enom', 'theories', 'medications', 'truman', 'oj', 'rpms', 'claire', 'art', 'minors', 'tailoring', 'proved', 'maintaining', 'tamworth', 'voluntary', 'oscillation', 'warehouse', 'governors', 'stones', 'connections', 'xe', 'hemingway', 'posture', 'denim', 'perspective', 'susan', 'banners', 'rethink', 'careers', 'profit', 'lizards', 'cyan', 'memory', 'symbolic', 'transports', 'acronym', 'begun', 'valance', 'investigators', 'passionate', 'interacting', 'eid', 'incubation', 'residents', 'biologists', 'metadata', 'concept', 'characteristics', 'subroutine', 'sta', 'rooms', 'scouts', 'daphne', 'profound', 'evaluating', 'zombies', 'fine', 'taxis', 'prayed', 'layer', 'uma', 'reviewing', 'sparkling', 'frames', 'darren', 'pcbs', 'tale', 'break', 'heavens', 'ezine', 'jesuit', 'simulations', 'keen', 'uconn', 'loudly', 'monique', 'documenting', 'manga', 'ltr', 'kerr', 'ny', 'competitors')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "train_loader = DataLoader(comp_train_set, batch_size = batch_size, shuffle = True, drop_last= True)\n",
    "val_loader = DataLoader(comp_val_set, batch_size = batch_size, shuffle = False, drop_last= True)\n",
    "test_loader = DataLoader(comp_test_set, batch_size = batch_size, shuffle = False, drop_last= True)\n",
    "test_2_loader = DataLoader(comp_test_set_2, batch_size = batch_size, shuffle = False, drop_last= True)\n",
    "\n",
    "for image, label in train_loader:\n",
    "    break\n",
    "    \n",
    "image_cnn = image.view(-1, color_channels, patch_height, patch_width)   \n",
    "print(image.shape)\n",
    "print(image_cnn.shape)\n",
    "print(label)\n",
    "\n",
    "for n_patch in range(0,3):\n",
    "    patch = image[3,n_patch,0,:,:].cpu()\n",
    "    plt.imshow(patch)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "903b2e50-7b52-4f1d-a30a-483bb6d15cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_target(labels, seq_len, output_size,batch_size):    \n",
    "    one_hot_target = torch.empty(batch_size, seq_len, output_size) \n",
    "\n",
    "    for j,word in enumerate(labels):\n",
    "        length = len(word)\n",
    "        one_hot_target[j, 0, :] = mapeo['START']\n",
    "\n",
    "        for k,letter in enumerate(word):\n",
    "            one_hot_target[j, k + 1, :] = mapeo[letter]\n",
    "\n",
    "        one_hot_target[j, length + 1, :] = mapeo['END']\n",
    "        one_hot_target[j, length + 2: seq_len, :] = mapeo['PAD']\n",
    "        \n",
    "    return one_hot_target        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "367aa05f-9a8e-4021-a9a5-fae21118dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_conversion(decoder_output, output_size):\n",
    "    \n",
    "    one_hot_output_letter = torch.zeros(1, 1, output_size).cuda(0)\n",
    "    index = torch.argmax(decoder_output, dim = 2).item()\n",
    "    one_hot_output_letter[0, 0, index] = 1.\n",
    "    \n",
    "    return one_hot_output_letter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e5757-79df-4499-a9aa-73e41953e8e3",
   "metadata": {},
   "source": [
    "# Definiendo la arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9667a512-0bbc-4b57-9741-d0d80f0301fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTROS_EN_CNN_1 = 4\n",
    "NEURONS_IN_DENSE_LAYER = 1024\n",
    "PATCH_HEIGHT_AFTER_POOLING = patch_height//2\n",
    "PATCH_WIDTH_AFTER_POOLING = patch_width//2\n",
    "kernel_size = 3\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=FILTROS_EN_CNN_1, kernel_size=kernel_size, stride=1, padding=1, device=device)\n",
    "        self.fc1 = nn.Linear(PATCH_HEIGHT_AFTER_POOLING*PATCH_WIDTH_AFTER_POOLING*FILTROS_EN_CNN_1, NEURONS_IN_DENSE_LAYER)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = F.relu((self.conv1(X)))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, PATCH_HEIGHT_AFTER_POOLING*PATCH_WIDTH_AFTER_POOLING*FILTROS_EN_CNN_1)\n",
    "        X = self.fc1(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "4e7d5571-d6cf-4416-98cb-44ebe0630d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, input, hidden, batch_size, seq_len):        \n",
    "        output = input.view(batch_size, seq_len, self.input_size)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "e89a3cef-0243-4758-89f2-7d3248a7ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, batch_first = True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 2)\n",
    "        # dim = 2 porque esta última dimensión es la correspondiente a output_size, que es sobre\n",
    "        # la que queremos hacer el softmax\n",
    "\n",
    "    def forward(self, input, hidden, batch_size, seq_len):        \n",
    "        output = input.view(batch_size, seq_len, self.output_size)\n",
    "        #output = F.relu(output) # la relu se metía aquí porque en el\n",
    "        #caso NLP del ejemplo de PyTorch previamente había una capa de embedding\n",
    "        #No nos hace falta porque nuestro tensor de inputs ya es one-hot\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "               torch.zeros(1, batch_size, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "0b511a13-8946-4211-b383-2e19337ee86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "encoder_input_size = 1024\n",
    "hidden_size = 256\n",
    "output_size = len(letters)\n",
    "\n",
    "CNN_model = ConvolutionalNetwork().cuda(0)\n",
    "CNN_optimizer = torch.optim.Adam(CNN_model.parameters())\n",
    "\n",
    "Encoder_model = EncoderRNN(input_size = encoder_input_size, hidden_size = hidden_size).cuda(0)\n",
    "Encoder_optimizer = optim.SGD(Encoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "Decoder_model = DecoderRNN(hidden_size = hidden_size, output_size = output_size).cuda(0)\n",
    "Decoder_optimizer = optim.SGD(Decoder_model.parameters(), lr = 0.001)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d57b509-adfc-42ef-8380-315192084600",
   "metadata": {},
   "source": [
    "## Entrenando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "982e3025-d3f6-40eb-9621-61797ae8b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b3061ff9-9632-407b-a7fd-ca0462b54175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4afc442d50>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5000\n",
    "patience = 100\n",
    "min_loss_val = 10 # huge initial value for the minimum validation loss\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "86021409-901b-43fa-b145-0fb89bbe6614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(decoder_output, ground_truth):\n",
    "    loss = 0\n",
    "        \n",
    "    for j in range(batch_size):\n",
    "        loss += criterion(decoder_output[j], ground_truth[j])               \n",
    "    \n",
    "    loss = loss/batch_size\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b998e9a1-e5fb-482e-9718-3cab8836c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_losses = []\n",
    "    for num_batch, (image, labels) in enumerate(train_loader):        \n",
    "        num_batch += 1\n",
    "        encoder_hidden = Encoder_model.initHidden(batch_size)\n",
    "\n",
    "        image_cnn = image.view(-1, color_channels, patch_height, patch_width).cuda(0)\n",
    "        encoder_input = CNN_model(image_cnn)\n",
    "        encoder_output, encoder_hidden = Encoder_model(encoder_input, encoder_hidden, batch_size = batch_size, seq_len = n_patches)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = get_one_hot_target(labels=labels, batch_size = batch_size, seq_len = MAX_LENGTH + 2,output_size = output_size).cuda(0)\n",
    "        decoder_output, decoder_hidden = Decoder_model(decoder_input, decoder_hidden, batch_size = batch_size, seq_len = MAX_LENGTH + 2)\n",
    "\n",
    "        output_indices = torch.tensor(list(range(0, MAX_LENGTH + 2 -1))).cuda(0) # removing last token from the output\n",
    "        decoder_output = torch.index_select(decoder_output, dim = 1, index = output_indices)\n",
    "\n",
    "        ground_truth = torch.argmax(decoder_input, dim = 2)\n",
    "        target_indices = torch.tensor(list(range(1, MAX_LENGTH + 2))).cuda(0) # remove SOS token from the input\n",
    "        ground_truth = torch.index_select(ground_truth, dim = 1, index = target_indices)\n",
    "\n",
    "        loss = calculate_loss(decoder_output,ground_truth)\n",
    "\n",
    "        CNN_optimizer.zero_grad()\n",
    "        Encoder_optimizer.zero_grad()\n",
    "        Decoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        CNN_optimizer.step()\n",
    "        Encoder_optimizer.step()\n",
    "        Decoder_optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    return np.mean(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "fe9f1ec6-0936-4945-bb1a-096cb49f6845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    valid_losses = []\n",
    "\n",
    "    with torch.no_grad():       \n",
    "        for num_batch, (image_val, label_val) in enumerate(val_loader):        \n",
    "            num_batch += 1\n",
    "            encoder_hidden_val = Encoder_model.initHidden(batch_size = batch_size)\n",
    "            image_cnn_val = image_val.view(-1, color_channels, patch_height, patch_width).cuda(0)\n",
    "            encoder_input_val = CNN_model(image_cnn_val)\n",
    "            encoder_output_val, encoder_hidden_val = Encoder_model(encoder_input_val, encoder_hidden_val, batch_size = batch_size, seq_len = n_patches)\n",
    "\n",
    "            decoder_hidden_val = encoder_hidden_val\n",
    "            decoder_input_val = get_one_hot_target(labels=label_val, batch_size = batch_size, seq_len = MAX_LENGTH + 2,output_size = output_size).cuda(0)\n",
    "            decoder_output_val, decoder_hidden_val = Decoder_model(decoder_input_val, decoder_hidden_val,batch_size = batch_size, seq_len = MAX_LENGTH + 2)\n",
    "\n",
    "            output_indices_val = torch.tensor(list(range(0, MAX_LENGTH + 1))).cuda(0) # remove last token from the output\n",
    "            decoder_output_val = torch.index_select(decoder_output_val, dim = 1, index = output_indices_val)\n",
    "\n",
    "            ground_truth_val = torch.argmax(decoder_input_val, dim = 2)\n",
    "            target_indices_val = torch.tensor(list(range(1, MAX_LENGTH + 2))).cuda(0) # remove START token from the input\n",
    "            ground_truth_val = torch.index_select(ground_truth_val, dim = 1, index = target_indices_val)\n",
    "            \n",
    "            loss = calculate_loss(decoder_output_val,ground_truth_val)\n",
    "            valid_losses.append(loss.item())\n",
    "    return np.mean(valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "b4e17c3f-cca4-45cb-b498-fa1f2ebb0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patience():\n",
    "    \n",
    "    def __init__(self, patience):\n",
    "        self.patience = patience\n",
    "        self.current_patience = patience\n",
    "        self.min_loss_val = float('inf')\n",
    "\n",
    "    def more_patience(self,loss_val):\n",
    "        self.current_patience -= 1\n",
    "        if self.current_patience == 0:\n",
    "            return False\n",
    "\n",
    "        if loss_val < self.min_loss_val:\n",
    "            self.min_loss_val = loss_val\n",
    "            self.current_patience = patience\n",
    "\n",
    "            model_name = f\"{height}x{width}_by{patch_width}_jump{stepsize}_batch{batch_size} NN_{FILTROS_EN_CNN_1}_{NEURONS_IN_DENSE_LAYER}_{kernel_size}_{hidden_size} pats:{total_length}\"\n",
    "            print(\", saved best model.\")\n",
    "            \n",
    "            torch.save(CNN_model.state_dict(), 'CNN_'+model_name)\n",
    "            torch.save(Encoder_model.state_dict(), 'Encoder_'+model_name)\n",
    "            torch.save(Decoder_model.state_dict(), 'Decoder_'+model_name)\n",
    "    \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6e19b6-8cd6-4672-8e25-c2f0f9dfa84d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Patience' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dac4f0edb800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpatience_controler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPatience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Patience' is not defined"
     ]
    }
   ],
   "source": [
    "patience_controler = Patience(patience)\n",
    "start_time = time.time()\n",
    "\n",
    "for num_epoch in range(epochs):\n",
    "    train_loss = train()        \n",
    "    valid_loss = validation()\n",
    "    \n",
    "    writer.add_scalar('Loss/train', train_loss, num_epoch)\n",
    "    writer.add_scalar('Loss/validation', valid_loss, num_epoch)\n",
    "    \n",
    "    print(f'Epoch: {num_epoch} Train loss: {train_loss} Valid loss: {valid_loss} Duration: {(time.time() - start_time)/60} minutes',)\n",
    "\n",
    "    if not patience_controler.more_patience(valid_loss):\n",
    "        print(\"Se acabó la paciencia\")\n",
    "        break\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b9a50-e6c7-4762-9252-28669e269ecd",
   "metadata": {},
   "source": [
    "## Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66612c-0185-46f5-9e70-ce08270bd5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model.load_state_dict(torch.load('CNN_model_30000_words_TF_PAD_noise.pt'))\n",
    "CNN_model.eval()\n",
    "\n",
    "Encoder_model.load_state_dict(torch.load('Encoder_model_30000_words_TF_PAD_noise.pt'))\n",
    "Encoder_model.eval()\n",
    "\n",
    "Decoder_model.load_state_dict(torch.load('Decoder_model_30000_words_TF_PAD_noise.pt'))\n",
    "Decoder_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eadc45de-8e86-436c-bf22-a9f71ced7135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test():\n",
    "    with torch.no_grad():\n",
    "        for num_batch, (image_test, label_test) in enumerate(test_loader):\n",
    "            num_batch += 1\n",
    "            encoder_hidden_test = Encoder_model.initHidden(batch_size = batch_size)\n",
    "            image_cnn_test = image_test.view(-1, color_channels, patch_height, patch_width).cuda(0)\n",
    "            encoder_input_test = CNN_model(image_cnn_test)\n",
    "            encoder_output, encoder_hidden_test = Encoder_model(encoder_input_test, encoder_hidden_test, batch = test_batch, seq_len = n_patches)\n",
    "\n",
    "            #decoder_hidden_test = (encoder_hidden_test[0][0, :, :].view(1, batch_size, hidden_size), # We take the last hidden state of the Encoder \n",
    "            #                       encoder_hidden_test[1][0, :, :].view(1, batch_size, hidden_size)) # for each image/word (j) within the batch \n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                decoder_input_test = mapeo['START'].cuda(0) # We initialize the first Decoder input as the START token\n",
    "                decoder_hidden_test = (encoder_hidden_test[0][0, j, :].view(1, 1, hidden_size), # We take the last hidden state of the Encoder \n",
    "                                       encoder_hidden_test[1][0, j, :].view(1, 1, hidden_size)) # for each image/word (j) within the batch \n",
    "                \n",
    "                for d in range(MAX_LENGTH + 2):\n",
    "                    decoder_output_test, decoder_hidden_test = Decoder_model(decoder_input_test, decoder_hidden_test, batch = 1, seq_len = 1)\n",
    "\n",
    "                    output_letter = one_hot_conversion(decoder_output_test, output_size = output_size)\n",
    "                    decoder_input_test = output_letter\n",
    "                    \n",
    "                    if d == 0:\n",
    "                        output_word = output_letter\n",
    "                    else:\n",
    "                        output_word = torch.cat((output_word, output_letter), dim = 1).cuda(0)\n",
    "                    \n",
    "                    if torch.equal(output_letter, letter_to_vector('END').cuda(0)):\n",
    "                        break\n",
    "                output_word = torch.argmax(output_word, dim=2)\n",
    "                output_word = output_word.view(output_word.numel()) # view as a rank-1 tensor\n",
    "\n",
    "                model_word = []\n",
    "                for item in output_word:\n",
    "                    model_word.append(letters[item])\n",
    "\n",
    "                model_word = ''.join(model_word[:-1])\n",
    "                print(model_word)\n",
    "            print(test_set) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc398edb-7bee-48ec-9028-4d1e885e94fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7b08819e076e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-c209519ca05f>\u001b[0m in \u001b[0;36mTest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnum_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mnum_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mencoder_hidden_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d27e1a-8d9d-4052-8ac0-9f99a888b082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a160db-836a-41b2-b692-1bad1c88e8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6_cv2",
   "language": "python",
   "name": "python3.6_cv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
